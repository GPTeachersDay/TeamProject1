{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 추출\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메서드 정의\n",
    "def main(URL = 'D:/project/Teamproject1/JeseongMoon/Dataset/binary_classification_data.csv'):\n",
    "    \n",
    "    # 랜덤 시드 고정\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    global df, LEARNING_RATE, EPOCH_COUNT, MB_SIZE, REPORT, TRAIN_RATIO, X, y, X_train, X_test, y_train, y_test, y_pred\n",
    "    \n",
    "    df = load_dataset(URL) # 데이터 로드\n",
    "    Regression_Model() # 회귀 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/project/TeamProject1/JeseongMoon/Dataset/binary_classification_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean of the integrated profile</th>\n",
       "      <th>Standard deviation of the integrated profile</th>\n",
       "      <th>Excess kurtosis of the integrated profile</th>\n",
       "      <th>Skewness of the integrated profile</th>\n",
       "      <th>Mean of the DM-SNR curve</th>\n",
       "      <th>Standard deviation of the DM-SNR curve</th>\n",
       "      <th>Excess kurtosis of the DM-SNR curve</th>\n",
       "      <th>Skewness of the DM-SNR curve</th>\n",
       "      <th>target_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140.562500</td>\n",
       "      <td>55.683782</td>\n",
       "      <td>-0.234571</td>\n",
       "      <td>-0.699648</td>\n",
       "      <td>3.199833</td>\n",
       "      <td>19.110426</td>\n",
       "      <td>7.975532</td>\n",
       "      <td>74.242225</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102.507812</td>\n",
       "      <td>58.882430</td>\n",
       "      <td>0.465318</td>\n",
       "      <td>-0.515088</td>\n",
       "      <td>1.677258</td>\n",
       "      <td>14.860146</td>\n",
       "      <td>10.576487</td>\n",
       "      <td>127.393580</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.015625</td>\n",
       "      <td>39.341649</td>\n",
       "      <td>0.323328</td>\n",
       "      <td>1.051164</td>\n",
       "      <td>3.121237</td>\n",
       "      <td>21.744669</td>\n",
       "      <td>7.735822</td>\n",
       "      <td>63.171909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>136.750000</td>\n",
       "      <td>57.178449</td>\n",
       "      <td>-0.068415</td>\n",
       "      <td>-0.636238</td>\n",
       "      <td>3.642977</td>\n",
       "      <td>20.959280</td>\n",
       "      <td>6.896499</td>\n",
       "      <td>53.593661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.726562</td>\n",
       "      <td>40.672225</td>\n",
       "      <td>0.600866</td>\n",
       "      <td>1.123492</td>\n",
       "      <td>1.178930</td>\n",
       "      <td>11.468720</td>\n",
       "      <td>14.269573</td>\n",
       "      <td>252.567306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Mean of the integrated profile  \\\n",
       "0                       140.562500   \n",
       "1                       102.507812   \n",
       "2                       103.015625   \n",
       "3                       136.750000   \n",
       "4                        88.726562   \n",
       "\n",
       "    Standard deviation of the integrated profile  \\\n",
       "0                                      55.683782   \n",
       "1                                      58.882430   \n",
       "2                                      39.341649   \n",
       "3                                      57.178449   \n",
       "4                                      40.672225   \n",
       "\n",
       "    Excess kurtosis of the integrated profile  \\\n",
       "0                                   -0.234571   \n",
       "1                                    0.465318   \n",
       "2                                    0.323328   \n",
       "3                                   -0.068415   \n",
       "4                                    0.600866   \n",
       "\n",
       "    Skewness of the integrated profile   Mean of the DM-SNR curve  \\\n",
       "0                            -0.699648                   3.199833   \n",
       "1                            -0.515088                   1.677258   \n",
       "2                             1.051164                   3.121237   \n",
       "3                            -0.636238                   3.642977   \n",
       "4                             1.123492                   1.178930   \n",
       "\n",
       "    Standard deviation of the DM-SNR curve  \\\n",
       "0                                19.110426   \n",
       "1                                14.860146   \n",
       "2                                21.744669   \n",
       "3                                20.959280   \n",
       "4                                11.468720   \n",
       "\n",
       "    Excess kurtosis of the DM-SNR curve   Skewness of the DM-SNR curve  \\\n",
       "0                              7.975532                      74.242225   \n",
       "1                             10.576487                     127.393580   \n",
       "2                              7.735822                      63.171909   \n",
       "3                              6.896499                      53.593661   \n",
       "4                             14.269573                     252.567306   \n",
       "\n",
       "   target_class  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14318, 8) (3580, 8) (14318,) (3580,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "TRAIN_RATIO = 0.8\n",
    "# 학습 데이터 분리\n",
    "X = df.drop('target_class', axis=1)\n",
    "y = df['target_class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=TRAIN_RATIO, random_state = 83)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean of the integrated profile</th>\n",
       "      <th>Standard deviation of the integrated profile</th>\n",
       "      <th>Excess kurtosis of the integrated profile</th>\n",
       "      <th>Skewness of the integrated profile</th>\n",
       "      <th>Mean of the DM-SNR curve</th>\n",
       "      <th>Standard deviation of the DM-SNR curve</th>\n",
       "      <th>Excess kurtosis of the DM-SNR curve</th>\n",
       "      <th>Skewness of the DM-SNR curve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13984</th>\n",
       "      <td>127.203125</td>\n",
       "      <td>52.269189</td>\n",
       "      <td>-0.222664</td>\n",
       "      <td>-0.101506</td>\n",
       "      <td>15.086120</td>\n",
       "      <td>54.658243</td>\n",
       "      <td>3.364636</td>\n",
       "      <td>9.392991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3126</th>\n",
       "      <td>52.968750</td>\n",
       "      <td>31.446367</td>\n",
       "      <td>3.410426</td>\n",
       "      <td>16.839254</td>\n",
       "      <td>41.168896</td>\n",
       "      <td>67.290303</td>\n",
       "      <td>1.648880</td>\n",
       "      <td>1.635625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13748</th>\n",
       "      <td>124.750000</td>\n",
       "      <td>35.676717</td>\n",
       "      <td>0.178535</td>\n",
       "      <td>1.837915</td>\n",
       "      <td>1.110368</td>\n",
       "      <td>11.973404</td>\n",
       "      <td>14.471259</td>\n",
       "      <td>246.172871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14864</th>\n",
       "      <td>100.164062</td>\n",
       "      <td>48.281689</td>\n",
       "      <td>0.561865</td>\n",
       "      <td>0.633153</td>\n",
       "      <td>1.925585</td>\n",
       "      <td>15.784724</td>\n",
       "      <td>9.617048</td>\n",
       "      <td>105.568196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17160</th>\n",
       "      <td>96.734375</td>\n",
       "      <td>46.709121</td>\n",
       "      <td>0.377105</td>\n",
       "      <td>0.152048</td>\n",
       "      <td>2.836120</td>\n",
       "      <td>18.707122</td>\n",
       "      <td>8.906146</td>\n",
       "      <td>92.142170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Mean of the integrated profile  \\\n",
       "13984                       127.203125   \n",
       "3126                         52.968750   \n",
       "13748                       124.750000   \n",
       "14864                       100.164062   \n",
       "17160                        96.734375   \n",
       "\n",
       "        Standard deviation of the integrated profile  \\\n",
       "13984                                      52.269189   \n",
       "3126                                       31.446367   \n",
       "13748                                      35.676717   \n",
       "14864                                      48.281689   \n",
       "17160                                      46.709121   \n",
       "\n",
       "        Excess kurtosis of the integrated profile  \\\n",
       "13984                                   -0.222664   \n",
       "3126                                     3.410426   \n",
       "13748                                    0.178535   \n",
       "14864                                    0.561865   \n",
       "17160                                    0.377105   \n",
       "\n",
       "        Skewness of the integrated profile   Mean of the DM-SNR curve  \\\n",
       "13984                            -0.101506                  15.086120   \n",
       "3126                             16.839254                  41.168896   \n",
       "13748                             1.837915                   1.110368   \n",
       "14864                             0.633153                   1.925585   \n",
       "17160                             0.152048                   2.836120   \n",
       "\n",
       "        Standard deviation of the DM-SNR curve  \\\n",
       "13984                                54.658243   \n",
       "3126                                 67.290303   \n",
       "13748                                11.973404   \n",
       "14864                                15.784724   \n",
       "17160                                18.707122   \n",
       "\n",
       "        Excess kurtosis of the DM-SNR curve   Skewness of the DM-SNR curve  \n",
       "13984                              3.364636                       9.392991  \n",
       "3126                               1.648880                       1.635625  \n",
       "13748                             14.471259                     246.172871  \n",
       "14864                              9.617048                     105.568196  \n",
       "17160                              8.906146                      92.142170  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean of the integrated profile</th>\n",
       "      <th>Standard deviation of the integrated profile</th>\n",
       "      <th>Excess kurtosis of the integrated profile</th>\n",
       "      <th>Skewness of the integrated profile</th>\n",
       "      <th>Mean of the DM-SNR curve</th>\n",
       "      <th>Standard deviation of the DM-SNR curve</th>\n",
       "      <th>Excess kurtosis of the DM-SNR curve</th>\n",
       "      <th>Skewness of the DM-SNR curve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13971</th>\n",
       "      <td>118.890625</td>\n",
       "      <td>44.510223</td>\n",
       "      <td>0.223548</td>\n",
       "      <td>0.057119</td>\n",
       "      <td>1.369565</td>\n",
       "      <td>13.526842</td>\n",
       "      <td>14.222397</td>\n",
       "      <td>234.325537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4322</th>\n",
       "      <td>113.562500</td>\n",
       "      <td>47.418804</td>\n",
       "      <td>0.168827</td>\n",
       "      <td>-0.035996</td>\n",
       "      <td>2.205686</td>\n",
       "      <td>13.998608</td>\n",
       "      <td>10.654285</td>\n",
       "      <td>146.149662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14042</th>\n",
       "      <td>103.226562</td>\n",
       "      <td>40.028547</td>\n",
       "      <td>0.469162</td>\n",
       "      <td>0.883947</td>\n",
       "      <td>3.876254</td>\n",
       "      <td>19.969912</td>\n",
       "      <td>6.818652</td>\n",
       "      <td>54.926998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7102</th>\n",
       "      <td>123.609375</td>\n",
       "      <td>51.892352</td>\n",
       "      <td>0.288274</td>\n",
       "      <td>-0.199181</td>\n",
       "      <td>29.039298</td>\n",
       "      <td>67.633561</td>\n",
       "      <td>1.915681</td>\n",
       "      <td>1.718845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>134.960938</td>\n",
       "      <td>58.112418</td>\n",
       "      <td>-0.016643</td>\n",
       "      <td>-0.415690</td>\n",
       "      <td>4.735786</td>\n",
       "      <td>24.653527</td>\n",
       "      <td>6.325710</td>\n",
       "      <td>43.993983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Mean of the integrated profile  \\\n",
       "13971                       118.890625   \n",
       "4322                        113.562500   \n",
       "14042                       103.226562   \n",
       "7102                        123.609375   \n",
       "1003                        134.960938   \n",
       "\n",
       "        Standard deviation of the integrated profile  \\\n",
       "13971                                      44.510223   \n",
       "4322                                       47.418804   \n",
       "14042                                      40.028547   \n",
       "7102                                       51.892352   \n",
       "1003                                       58.112418   \n",
       "\n",
       "        Excess kurtosis of the integrated profile  \\\n",
       "13971                                    0.223548   \n",
       "4322                                     0.168827   \n",
       "14042                                    0.469162   \n",
       "7102                                     0.288274   \n",
       "1003                                    -0.016643   \n",
       "\n",
       "        Skewness of the integrated profile   Mean of the DM-SNR curve  \\\n",
       "13971                             0.057119                   1.369565   \n",
       "4322                             -0.035996                   2.205686   \n",
       "14042                             0.883947                   3.876254   \n",
       "7102                             -0.199181                  29.039298   \n",
       "1003                             -0.415690                   4.735786   \n",
       "\n",
       "        Standard deviation of the DM-SNR curve  \\\n",
       "13971                                13.526842   \n",
       "4322                                 13.998608   \n",
       "14042                                19.969912   \n",
       "7102                                 67.633561   \n",
       "1003                                 24.653527   \n",
       "\n",
       "        Excess kurtosis of the DM-SNR curve   Skewness of the DM-SNR curve  \n",
       "13971                             14.222397                     234.325537  \n",
       "4322                              10.654285                     146.149662  \n",
       "14042                              6.818652                      54.926998  \n",
       "7102                               1.915681                       1.718845  \n",
       "1003                               6.325710                      43.993983  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean of the integrated profile</th>\n",
       "      <th>Standard deviation of the integrated profile</th>\n",
       "      <th>Excess kurtosis of the integrated profile</th>\n",
       "      <th>Skewness of the integrated profile</th>\n",
       "      <th>Mean of the DM-SNR curve</th>\n",
       "      <th>Standard deviation of the DM-SNR curve</th>\n",
       "      <th>Excess kurtosis of the DM-SNR curve</th>\n",
       "      <th>Skewness of the DM-SNR curve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14318.000000</td>\n",
       "      <td>14318.000000</td>\n",
       "      <td>14318.000000</td>\n",
       "      <td>14318.000000</td>\n",
       "      <td>14318.000000</td>\n",
       "      <td>14318.000000</td>\n",
       "      <td>14318.000000</td>\n",
       "      <td>14318.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>110.979922</td>\n",
       "      <td>46.524674</td>\n",
       "      <td>0.477455</td>\n",
       "      <td>1.748246</td>\n",
       "      <td>12.444476</td>\n",
       "      <td>26.217401</td>\n",
       "      <td>8.333270</td>\n",
       "      <td>105.312684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>25.526123</td>\n",
       "      <td>6.782110</td>\n",
       "      <td>1.052675</td>\n",
       "      <td>6.072202</td>\n",
       "      <td>29.200648</td>\n",
       "      <td>19.425697</td>\n",
       "      <td>4.491693</td>\n",
       "      <td>106.165624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.812500</td>\n",
       "      <td>24.772042</td>\n",
       "      <td>-1.876011</td>\n",
       "      <td>-1.755332</td>\n",
       "      <td>0.213211</td>\n",
       "      <td>7.370432</td>\n",
       "      <td>-2.812353</td>\n",
       "      <td>-1.976976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>100.710938</td>\n",
       "      <td>42.362011</td>\n",
       "      <td>0.028368</td>\n",
       "      <td>-0.186510</td>\n",
       "      <td>1.917224</td>\n",
       "      <td>14.416392</td>\n",
       "      <td>5.797204</td>\n",
       "      <td>35.364865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>114.914062</td>\n",
       "      <td>46.954915</td>\n",
       "      <td>0.223197</td>\n",
       "      <td>0.197768</td>\n",
       "      <td>2.788880</td>\n",
       "      <td>18.415923</td>\n",
       "      <td>8.460192</td>\n",
       "      <td>83.517364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>127.015625</td>\n",
       "      <td>51.001216</td>\n",
       "      <td>0.476759</td>\n",
       "      <td>0.937653</td>\n",
       "      <td>5.414716</td>\n",
       "      <td>28.217723</td>\n",
       "      <td>10.721939</td>\n",
       "      <td>140.055368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>192.617188</td>\n",
       "      <td>91.206475</td>\n",
       "      <td>8.069522</td>\n",
       "      <td>68.101622</td>\n",
       "      <td>211.948997</td>\n",
       "      <td>110.642211</td>\n",
       "      <td>34.539844</td>\n",
       "      <td>1191.000837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Mean of the integrated profile  \\\n",
       "count                     14318.000000   \n",
       "mean                        110.979922   \n",
       "std                          25.526123   \n",
       "min                           5.812500   \n",
       "25%                         100.710938   \n",
       "50%                         114.914062   \n",
       "75%                         127.015625   \n",
       "max                         192.617188   \n",
       "\n",
       "        Standard deviation of the integrated profile  \\\n",
       "count                                   14318.000000   \n",
       "mean                                       46.524674   \n",
       "std                                         6.782110   \n",
       "min                                        24.772042   \n",
       "25%                                        42.362011   \n",
       "50%                                        46.954915   \n",
       "75%                                        51.001216   \n",
       "max                                        91.206475   \n",
       "\n",
       "        Excess kurtosis of the integrated profile  \\\n",
       "count                                14318.000000   \n",
       "mean                                     0.477455   \n",
       "std                                      1.052675   \n",
       "min                                     -1.876011   \n",
       "25%                                      0.028368   \n",
       "50%                                      0.223197   \n",
       "75%                                      0.476759   \n",
       "max                                      8.069522   \n",
       "\n",
       "        Skewness of the integrated profile   Mean of the DM-SNR curve  \\\n",
       "count                         14318.000000               14318.000000   \n",
       "mean                              1.748246                  12.444476   \n",
       "std                               6.072202                  29.200648   \n",
       "min                              -1.755332                   0.213211   \n",
       "25%                              -0.186510                   1.917224   \n",
       "50%                               0.197768                   2.788880   \n",
       "75%                               0.937653                   5.414716   \n",
       "max                              68.101622                 211.948997   \n",
       "\n",
       "        Standard deviation of the DM-SNR curve  \\\n",
       "count                             14318.000000   \n",
       "mean                                 26.217401   \n",
       "std                                  19.425697   \n",
       "min                                   7.370432   \n",
       "25%                                  14.416392   \n",
       "50%                                  18.415923   \n",
       "75%                                  28.217723   \n",
       "max                                 110.642211   \n",
       "\n",
       "        Excess kurtosis of the DM-SNR curve   Skewness of the DM-SNR curve  \n",
       "count                          14318.000000                   14318.000000  \n",
       "mean                               8.333270                     105.312684  \n",
       "std                                4.491693                     106.165624  \n",
       "min                               -2.812353                      -1.976976  \n",
       "25%                                5.797204                      35.364865  \n",
       "50%                                8.460192                      83.517364  \n",
       "75%                               10.721939                     140.055368  \n",
       "max                               34.539844                    1191.000837  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "통합 프로필:\n",
    "<br>\n",
    "<br>평균: 평균 값은 대략 111으로 중앙값과 거의 비슷하며, 최소값과 최대값 사이에는 큰 차이가 있어 표준편차 값이 25.6으로 큽니다.\n",
    "<br>표준 편차: 평균 표준 편차는 46.6으로 중앙값에 가깝고, 최소값과 최대값을 보면 데이터 세트 전체에 정규 분포가 나타나는 것으로 보입니다.\n",
    "<br>초과 첨도: 데이터의 75% 이상이 0.5 아래에 있으며, 첨도가 낮은 경우 꼬리가 가벼우거나 이상치가 부족한 경향이 있습니다.\n",
    "<br>비대칭도: 데이터의 75% 이상이 1.0 아래에 있어 통합 프로필의 대부분은 약간만 비대칭인 것으로 나타납니다.\n",
    "<br>\n",
    "<br>DM-SNR 곡선:\n",
    "<br>\n",
    "<br>평균: 표준 편차가 매우 높은 편입니다. 29로, 값의 75%가 5.6 아래에 있고 최대 값은 극단적으로 높습니다. 대부분의 곡선의 평균 값은 낮은 편이 될 것으로 기대할 수 있습니다.\n",
    "<br>표준 편차: 이것도 최대 값이 매우 높아 보이며, 값의 75%가 28 아래에 있으며 중앙값은 19이고 평균은 26입니다. 평균의 높은 크기는 극단적으로 높은 값의 영향을 받은 것이지만, 해당 값들이 포함되지 않았다면 표준 편차는 중앙값 주변에 위치할 것으로 예상됩니다.\n",
    "<br>초과 첨도: 값들은 상당히 높으며, 평균 값은 8.23이고 최대 값은 34입니다. 값들은 정규 분포를 나타내는 것으로 보입니다.\n",
    "<br>비대칭도: 값들은 상당히 높으며, 평균 값은 102이고 최대 값은 1191으로 매우 높은 값을 가지고 있습니다. 이는 75분위 값과 비교했을 때 극도로 높은 값입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Train set\n",
      "Missing values in feature  Mean of the integrated profile : 0\n",
      "Missing values in feature  Standard deviation of the integrated profile : 0\n",
      "Missing values in feature  Excess kurtosis of the integrated profile : 0\n",
      "Missing values in feature  Skewness of the integrated profile : 0\n",
      "Missing values in feature  Mean of the DM-SNR curve : 0\n",
      "Missing values in feature  Standard deviation of the DM-SNR curve : 0\n",
      "Missing values in feature  Excess kurtosis of the DM-SNR curve : 0\n",
      "Missing values in feature  Skewness of the DM-SNR curve : 0\n",
      "\n",
      "\n",
      "For Test set\n",
      "Missing values in feature  Mean of the integrated profile : 0\n",
      "Missing values in feature  Standard deviation of the integrated profile : 0\n",
      "Missing values in feature  Excess kurtosis of the integrated profile : 0\n",
      "Missing values in feature  Skewness of the integrated profile : 0\n",
      "Missing values in feature  Mean of the DM-SNR curve : 0\n",
      "Missing values in feature  Standard deviation of the DM-SNR curve : 0\n",
      "Missing values in feature  Excess kurtosis of the DM-SNR curve : 0\n",
      "Missing values in feature  Skewness of the DM-SNR curve : 0\n"
     ]
    }
   ],
   "source": [
    "print('For Train set')\n",
    "for feature in X_train.columns:\n",
    "    print('Missing values in feature ' + str(feature) + ' : ' + str(len(X_train[X_train[feature].isnull() == True])))\n",
    "\n",
    "print('\\n')\n",
    "print('For Test set')\n",
    "for feature in X_test.columns:\n",
    "    print('Missing values in feature ' + str(feature) + ' : ' + str(len(X_test[X_test[feature].isnull() == True])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.909345\n",
       "1    0.090655\n",
       "Name: target_class, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    13020\n",
       "1     1298\n",
       "Name: target_class, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\82105\\anaconda3\\envs\\project1\\lib\\site-packages\\seaborn\\_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='target_class', ylabel='count'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>데이터셋은 매우 불균형합니다. \n",
    "<br>펄서가 1,153개이고 펄서가 아닌 것은 11,375개입니다. \n",
    "<br>모델 구축 과정 전에 데이터셋을 샘플링해야 할 수도 있습니다.\n",
    "<br>\n",
    "<br>분류 모델을 평가하기 위해 고려해야 할 가장 중요한 지표는 F1 점수와 재현율입니다. \n",
    "<br>이는 대상 클래스가 극도로 적기 때문에 중요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "sns.heatmap(X_train.corr(), annot=True, fmt='.2f', cmap='Blues',mask=np.triu(X_train.corr(),+1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>2개의 종속 변수 간의 상관 관계:\n",
    "<br>\n",
    "<br>높은 양의 상관 관계:\n",
    "<br>\n",
    "<br>편향된 통합 프로필의 왜도와 DM-SNR 곡선의 초과 첨도\n",
    "<br>DM-SNR 곡선의 왜도와 DM-SNR 곡선의 초과 첨도\n",
    "<br>DM-SNR 곡선의 평균과 표준 편차\n",
    "<br>---------------------------------------------------\n",
    "<br>높은 음의 상관 관계:\n",
    "<br>\n",
    "<br>통합 프로필의 평균과 통합 프로필의 초과 첨도\n",
    "<br>통합 프로필의 평균과 통합 프로필의 왜도\n",
    "<br>DM-SNR 곡선의 초과 첨도와 표준 편차\n",
    "<br>---------------------------------------------------\n",
    "<br>독립 변수와 종속 변수 간의 상관 관계:\n",
    "<br>\n",
    "<br>통합 프로필의 초과 첨도와 통합 프로필의 왜도는 Target_class와 높은 양의 상관 관계를 가지고 있습니다. \n",
    "<br>통합 프로필의 평균은 Target_class와 높은 음의 상관 관계를 가지고 있습니다.\n",
    "<br>\n",
    "<br>상관 관계는 데이터에서 다중공선성을 나타내며, \n",
    "<br>모델 구축 과정에서 바람직하지 않으므로 데이터 전처리 과정에서 처리되어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1298,) (13020,)\n"
     ]
    }
   ],
   "source": [
    "y_train_1 = y_train[y_train == 1] #creating a dataset for only true pulsars for EDA\n",
    "y_train_0 = y_train[y_train == 0] #creating a dataset for only non pulsars for EDA\n",
    "print(y_train_1.shape, y_train_0.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>통합 프로필:\n",
    "<br>\n",
    "<br>평균: 이상치가 없으며, 약간 오른쪽으로 치우친 분포입니다. 평균은 56.52이고 표준 편차는 29.81입니다.\n",
    "<br>표준 편차: 이상치가 있으며, 오른쪽으로 치우친 분포입니다. 평균은 38.7이고 표준 편차는 7.87입니다.\n",
    "<br>초과 첨도: 이상치가 없으며, 약간 오른쪽으로 치우친 분포입니다. 평균은 3.12이고 표준 편차는 1.87입니다.\n",
    "<br>왜도: 이상치가 있으며, 오른쪽으로 치우친 분포입니다. 평균은 15.56이고 표준 편차는 14.14입니다.\n",
    "<br>\n",
    "<br>일반적으로 정규 분포를 따른다고 가정한다면, 펄서 별의 통합 프로필은 평균 약 56, 높은 표준 편차 약 38, \n",
    "<br>평균 왜도 약 15로 오른쪽으로 치우친 경향을 가지며, 평균 초과 첨도 약 3.12으로 \n",
    "<br>매우 뾰족한 곡선과 두꺼운 꼬리를 가질 것으로 예상할 수 있습니다.\n",
    "\n",
    "<br>DM-SNR 곡선:\n",
    "<br>\n",
    "<br>평균: 이상치가 매우 적게 있으며, 오른쪽으로 치우친 분포입니다. 평균은 44.91이고 표준 편차는 45.13입니다.\n",
    "<br>표준 편차: 이상치가 없으며, 약간 오른쪽으로 치우친 분포입니다. 평균은 56.92이고 표준 편차는 19.73입니다.\n",
    "<br>초과 첨도: 이상치가 있으며, 극단적으로 오른쪽으로 치우친 분포입니다. 평균은 2.78이고 표준 편차는 3.20입니다.\n",
    "<br>왜도: 이상치가 있으며, 극단적으로 오른쪽으로 치우친 분포입니다. 평균은 17.93이고 표준 편차는 46.92입니다.\n",
    "<br>\n",
    "<br>일반적으로 정규 분포를 따른다고 가정한다면, 펄서 별의 DM-SNR 곡선은 평균 약 45, 높은 표준 편차 약 57, \n",
    "<br>평균 왜도 약 18로 오른쪽으로 치우친 경향을 가지며, 평균 초과 첨도 약 2.78으로 \n",
    "<br>매우 뾰족한 곡선과 두꺼운 꼬리를 가질 것으로 예상할 수 있습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Integrated Profile (통합 프로필):\n",
    "<br>\n",
    "<br>평균: 이상치가 있으며, 평균은 116.56이고 표준 편차는 17.43입니다. 정규 분포에 가까운 형태입니다.\n",
    "<br>표준 편차: 이상치가 있으며, 평균은 47.31이고 표준 편차는 6.15입니다. 거의 정규 분포에 가깝지만 오른쪽으로 꼬리가 있습니다.\n",
    "<br>초과 첨도: 이상치가 있으며, 평균은 0.20이고 표준 편차는 0.33입니다. 양쪽 꼬리를 가진 거의 정규 분포에 가깝습니다.\n",
    "<br>왜도: 이상치가 있으며, 오른쪽으로 치우친 분포입니다. 평균은 0.38이고 표준 편차는 0.98입니다.\n",
    "\n",
    "<br>일반적으로 정규 분포를 따른다고 가정한다면, 비펄서 별의 통합 프로필은 평균 약 117, 높은 표준 편차 약 47, \n",
    "<br>왜도가 거의 없는 분포 (평균 왜도 약 0.38)를 가질 것으로 예상할 수 있으며, 약간 뾰족한 곡선을 가질 것으로 예상됩니다 (평균 초과 첨도 약 0.38).\n",
    "<br>\n",
    "<br>DM-SNR Curve (DM-SNR 곡선):\n",
    "<br>\n",
    "<br>평균: 이상치가 몇 개 있으며, 극단적으로 오른쪽으로 치우친 분포입니다. 평균은 8.90이고 표준 편차는 24.59입니다.\n",
    "<br>표준 편차: 이상치가 몇 개 있으며, 오른쪽으로 치우친 분포입니다. 평균은 23.24이고 표준 편차는 16.71입니다.\n",
    "<br>초과 첨도: 이상치가 있으며, 약간 오른쪽으로 치우친 분포입니다. 평균은 8.89이고 표준 편차는 4.26입니다.\n",
    "<br>왜도: 이상치가 있으며, 오른쪽으로 치우친 분포입니다. 평균은 114.36이고 표준 편차는 107.81입니다.\n",
    "<br>\n",
    "<br>일반적으로 정규 분포를 따른다고 가정한다면, 비펄서 별의 DM-SNR 곡선은 평균 약 9, 높은 표준 편차 약 23, \n",
    "<br>오른쪽으로 치우친 분포 (평균 왜도 약 114)를 가질 것으로 예상되며, \n",
    "<br>약간 뾰족한 곡선과 두꺼운 꼬리를 가질 것으로 예상됩니다 (평균 초과 첨도 약 8.89)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_iris\n",
    "%matplotlib inline\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=StandardScaler() \n",
    "X_trains = ss.fit_transform(X_train)\n",
    "X_tests = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = svm.SVC(random_state=42).fit(X_trains, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_param_grid = {'C': [0.01,0.1, 1, 10],  \n",
    "              'gamma': [0.09, 0.1, 0.2, 0.001], \n",
    "              'kernel': ['rbf'],\n",
    "              'tol':[0.001,0.0001],\n",
    "              'degree':[2,3]}\n",
    "\n",
    "SVM_grid1 = GridSearchCV(SVM, param_grid = SVM_param_grid, cv = 5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(random_state=42), n_jobs=-1,\n",
       "             param_grid={'C': [0.01, 0.1, 1, 10], 'degree': [2, 3],\n",
       "                         'gamma': [0.09, 0.1, 0.2, 0.001], 'kernel': ['rbf'],\n",
       "                         'tol': [0.001, 0.0001]})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_grid1.fit(X_trains, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, degree=2, gamma=0.2, random_state=42)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_grid1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, degree=2, gamma=0.09, random_state=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_grid = svm.SVC(C=10,degree=2, gamma=0.09, kernel='rbf', random_state=1)\n",
    "SVM_grid.fit(X_trains, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report of the training data:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     13020\n",
      "           1       0.95      0.84      0.89      1298\n",
      "\n",
      "    accuracy                           0.98     14318\n",
      "   macro avg       0.97      0.92      0.94     14318\n",
      "weighted avg       0.98      0.98      0.98     14318\n",
      " \n",
      "\n",
      "Classification Report of the test data:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      3239\n",
      "           1       0.95      0.81      0.88       341\n",
      "\n",
      "    accuracy                           0.98      3580\n",
      "   macro avg       0.97      0.90      0.93      3580\n",
      "weighted avg       0.98      0.98      0.98      3580\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report of the training data:\\n\\n',metrics.classification_report(y_train, SVM_grid.predict(X_trains)),'\\n')\n",
    "print('Classification Report of the test data:\\n\\n',metrics.classification_report(y_test, SVM_grid.predict(X_tests)),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM best accuracy : 0.978\n"
     ]
    }
   ],
   "source": [
    "y_pred = SVM_grid.predict(X_tests)\n",
    "print( \"SVM best accuracy : \" + str(np.round(metrics.accuracy_score(y_test,y_pred),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score :  0.9782122905027933\n",
      "Recall Score :  0.9782122905027933\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision Score : \",precision_score(y_test, y_pred, pos_label='positive',average='micro'))\n",
    "print(\"Recall Score : \",recall_score(y_test, y_pred, pos_label='positive',average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_grid = np.linspace(0.0,1.0,100)\n",
    "TPR = []                                                 \n",
    "FPR = []    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPR, TPR, cutoffs = metrics.roc_curve(y_test,y_pred,pos_label=1)      # positive label = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 시각화\n",
    "plt.plot(FPR,TPR,linewidth=1.0)\n",
    "plt.xlabel('False Positive')\n",
    "plt.ylabel('True Positive')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\82105\\anaconda3\\envs\\project1\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in c:\\users\\82105\\anaconda3\\envs\\project1\\lib\\site-packages (from imbalanced-learn) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\82105\\anaconda3\\envs\\project1\\lib\\site-packages (from imbalanced-learn) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\82105\\anaconda3\\envs\\project1\\lib\\site-packages (from imbalanced-learn) (1.1.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\82105\\anaconda3\\envs\\project1\\lib\\site-packages (from imbalanced-learn) (1.5.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\82105\\anaconda3\\envs\\project1\\lib\\site-packages (from scikit-learn>=0.24->imbalanced-learn) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 배치정규화, 오버샘플링, elu, adamax, es, rlrp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# SMOTE를 사용하여 오버샘플링 수행\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_trains, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from sklearn.utils import class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "87/87 [==============================] - 1s 5ms/step - loss: 0.1800 - accuracy: 0.9369 - val_loss: 0.5444 - val_accuracy: 0.7165\n",
      "Epoch 2/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1484 - accuracy: 0.9488 - val_loss: 0.3549 - val_accuracy: 0.8631\n",
      "Epoch 3/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1438 - accuracy: 0.9510 - val_loss: 0.2341 - val_accuracy: 0.9277\n",
      "Epoch 4/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1421 - accuracy: 0.9500 - val_loss: 0.1944 - val_accuracy: 0.9408\n",
      "Epoch 5/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1393 - accuracy: 0.9502 - val_loss: 0.1436 - val_accuracy: 0.9561\n",
      "Epoch 6/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1392 - accuracy: 0.9508 - val_loss: 0.1533 - val_accuracy: 0.9536\n",
      "Epoch 7/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1344 - accuracy: 0.9526 - val_loss: 0.1279 - val_accuracy: 0.9617\n",
      "Epoch 8/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1362 - accuracy: 0.9510 - val_loss: 0.1282 - val_accuracy: 0.9620\n",
      "Epoch 9/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1338 - accuracy: 0.9521 - val_loss: 0.1255 - val_accuracy: 0.9626\n",
      "Epoch 10/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1339 - accuracy: 0.9518 - val_loss: 0.1121 - val_accuracy: 0.9659\n",
      "Epoch 11/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1330 - accuracy: 0.9518 - val_loss: 0.1400 - val_accuracy: 0.9575\n",
      "Epoch 12/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1322 - accuracy: 0.9520 - val_loss: 0.1375 - val_accuracy: 0.9587\n",
      "Epoch 13/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1313 - accuracy: 0.9525 - val_loss: 0.1192 - val_accuracy: 0.9665\n",
      "Epoch 14/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1292 - accuracy: 0.9524 - val_loss: 0.1517 - val_accuracy: 0.9517\n",
      "Epoch 15/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1303 - accuracy: 0.9523 - val_loss: 0.1106 - val_accuracy: 0.9679\n",
      "Epoch 16/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1292 - accuracy: 0.9531 - val_loss: 0.1238 - val_accuracy: 0.9637\n",
      "Epoch 17/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1293 - accuracy: 0.9524 - val_loss: 0.1198 - val_accuracy: 0.9642\n",
      "Epoch 18/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1296 - accuracy: 0.9520 - val_loss: 0.1117 - val_accuracy: 0.9668\n",
      "Epoch 19/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1294 - accuracy: 0.9525 - val_loss: 0.1419 - val_accuracy: 0.9517\n",
      "Epoch 20/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1290 - accuracy: 0.9530 - val_loss: 0.1218 - val_accuracy: 0.9642\n",
      "Epoch 21/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1254 - accuracy: 0.9529 - val_loss: 0.1292 - val_accuracy: 0.9601\n",
      "Epoch 22/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1282 - accuracy: 0.9519 - val_loss: 0.1185 - val_accuracy: 0.9642\n",
      "Epoch 23/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1255 - accuracy: 0.9534 - val_loss: 0.1083 - val_accuracy: 0.9701\n",
      "Epoch 24/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1247 - accuracy: 0.9533 - val_loss: 0.1082 - val_accuracy: 0.9682\n",
      "Epoch 25/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1250 - accuracy: 0.9527 - val_loss: 0.1163 - val_accuracy: 0.9659\n",
      "Epoch 26/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1247 - accuracy: 0.9537 - val_loss: 0.1316 - val_accuracy: 0.9623\n",
      "Epoch 27/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1246 - accuracy: 0.9532 - val_loss: 0.1020 - val_accuracy: 0.9723\n",
      "Epoch 28/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1245 - accuracy: 0.9534 - val_loss: 0.1194 - val_accuracy: 0.9662\n",
      "Epoch 29/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1229 - accuracy: 0.9546 - val_loss: 0.0961 - val_accuracy: 0.9760\n",
      "Epoch 30/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1233 - accuracy: 0.9536 - val_loss: 0.1169 - val_accuracy: 0.9637\n",
      "Epoch 31/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.1239 - accuracy: 0.9546 - val_loss: 0.1133 - val_accuracy: 0.9684\n",
      "Epoch 32/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.1233 - accuracy: 0.9536 - val_loss: 0.1162 - val_accuracy: 0.9676\n",
      "Epoch 33/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1202 - accuracy: 0.9556 - val_loss: 0.1107 - val_accuracy: 0.9701\n",
      "Epoch 34/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1208 - accuracy: 0.9547 - val_loss: 0.1097 - val_accuracy: 0.9682\n",
      "Epoch 35/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1228 - accuracy: 0.9537 - val_loss: 0.1025 - val_accuracy: 0.9715\n",
      "Epoch 36/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1200 - accuracy: 0.9545 - val_loss: 0.1202 - val_accuracy: 0.9631\n",
      "Epoch 37/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1180 - accuracy: 0.9557 - val_loss: 0.1201 - val_accuracy: 0.9651\n",
      "Epoch 38/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1206 - accuracy: 0.9546 - val_loss: 0.1108 - val_accuracy: 0.9701\n",
      "Epoch 39/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.1190 - accuracy: 0.9549 - val_loss: 0.1432 - val_accuracy: 0.9508\n",
      "Epoch 40/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1190 - accuracy: 0.9546 - val_loss: 0.1087 - val_accuracy: 0.9679\n",
      "Epoch 41/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1165 - accuracy: 0.9560 - val_loss: 0.0920 - val_accuracy: 0.9760\n",
      "Epoch 42/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1176 - accuracy: 0.9554 - val_loss: 0.1062 - val_accuracy: 0.9712\n",
      "Epoch 43/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1175 - accuracy: 0.9548 - val_loss: 0.1292 - val_accuracy: 0.9626\n",
      "Epoch 44/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1155 - accuracy: 0.9554 - val_loss: 0.1141 - val_accuracy: 0.9670\n",
      "Epoch 45/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1144 - accuracy: 0.9570 - val_loss: 0.1180 - val_accuracy: 0.9640\n",
      "Epoch 46/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1141 - accuracy: 0.9570 - val_loss: 0.1045 - val_accuracy: 0.9701\n",
      "Epoch 47/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.1151 - accuracy: 0.9558 - val_loss: 0.0942 - val_accuracy: 0.9751\n",
      "Epoch 48/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1148 - accuracy: 0.9557 - val_loss: 0.0959 - val_accuracy: 0.9757\n",
      "Epoch 49/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1134 - accuracy: 0.9568 - val_loss: 0.1159 - val_accuracy: 0.9676\n",
      "Epoch 50/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.1136 - accuracy: 0.9573 - val_loss: 0.1102 - val_accuracy: 0.9690\n",
      "Epoch 51/300\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.1154 - accuracy: 0.9565 - val_loss: 0.1002 - val_accuracy: 0.9715\n",
      "Epoch 52/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.1129 - accuracy: 0.9566 - val_loss: 0.0988 - val_accuracy: 0.9715\n",
      "Epoch 53/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1126 - accuracy: 0.9573 - val_loss: 0.1178 - val_accuracy: 0.9642\n",
      "Epoch 54/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1117 - accuracy: 0.9582 - val_loss: 0.1095 - val_accuracy: 0.9684\n",
      "Epoch 55/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1116 - accuracy: 0.9578 - val_loss: 0.0960 - val_accuracy: 0.9749\n",
      "Epoch 56/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1097 - accuracy: 0.9573 - val_loss: 0.1158 - val_accuracy: 0.9662\n",
      "Epoch 57/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1100 - accuracy: 0.9586 - val_loss: 0.1001 - val_accuracy: 0.9737\n",
      "Epoch 58/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1097 - accuracy: 0.9575 - val_loss: 0.1235 - val_accuracy: 0.9626\n",
      "Epoch 59/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1110 - accuracy: 0.9580 - val_loss: 0.1153 - val_accuracy: 0.9659\n",
      "Epoch 60/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1069 - accuracy: 0.9596 - val_loss: 0.1045 - val_accuracy: 0.9704\n",
      "Epoch 61/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1075 - accuracy: 0.9589 - val_loss: 0.1112 - val_accuracy: 0.9665\n",
      "Epoch 62/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1096 - accuracy: 0.9575 - val_loss: 0.1176 - val_accuracy: 0.9662\n",
      "Epoch 63/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1074 - accuracy: 0.9598 - val_loss: 0.1646 - val_accuracy: 0.9363\n",
      "Epoch 64/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1095 - accuracy: 0.9593 - val_loss: 0.1300 - val_accuracy: 0.9589\n",
      "Epoch 65/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.1057 - accuracy: 0.9590 - val_loss: 0.1097 - val_accuracy: 0.9704\n",
      "Epoch 66/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1068 - accuracy: 0.9584 - val_loss: 0.1025 - val_accuracy: 0.9718\n",
      "Epoch 67/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1070 - accuracy: 0.9593 - val_loss: 0.1000 - val_accuracy: 0.9712\n",
      "Epoch 68/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1061 - accuracy: 0.9594 - val_loss: 0.1115 - val_accuracy: 0.9668\n",
      "Epoch 69/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1048 - accuracy: 0.9605 - val_loss: 0.1074 - val_accuracy: 0.9712\n",
      "Epoch 70/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1051 - accuracy: 0.9604 - val_loss: 0.1035 - val_accuracy: 0.9707\n",
      "Epoch 71/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1051 - accuracy: 0.9601 - val_loss: 0.1281 - val_accuracy: 0.9598\n",
      "Epoch 72/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1058 - accuracy: 0.9591 - val_loss: 0.0983 - val_accuracy: 0.9737\n",
      "Epoch 73/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1043 - accuracy: 0.9599 - val_loss: 0.1193 - val_accuracy: 0.9637\n",
      "Epoch 74/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1064 - accuracy: 0.9591 - val_loss: 0.1106 - val_accuracy: 0.9651\n",
      "Epoch 75/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1018 - accuracy: 0.9608 - val_loss: 0.1243 - val_accuracy: 0.9617\n",
      "Epoch 76/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1007 - accuracy: 0.9617 - val_loss: 0.1068 - val_accuracy: 0.9698\n",
      "Epoch 77/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1021 - accuracy: 0.9616 - val_loss: 0.1037 - val_accuracy: 0.9723\n",
      "Epoch 78/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1029 - accuracy: 0.9609 - val_loss: 0.1202 - val_accuracy: 0.9642\n",
      "Epoch 79/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1027 - accuracy: 0.9602 - val_loss: 0.1331 - val_accuracy: 0.9559\n",
      "Epoch 80/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1031 - accuracy: 0.9608 - val_loss: 0.1325 - val_accuracy: 0.9601\n",
      "Epoch 81/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1010 - accuracy: 0.9614 - val_loss: 0.1197 - val_accuracy: 0.9659\n",
      "Epoch 82/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1001 - accuracy: 0.9621 - val_loss: 0.1165 - val_accuracy: 0.9670\n",
      "Epoch 83/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1007 - accuracy: 0.9627 - val_loss: 0.1155 - val_accuracy: 0.9651\n",
      "Epoch 84/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1001 - accuracy: 0.9609 - val_loss: 0.1208 - val_accuracy: 0.9623\n",
      "Epoch 85/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0990 - accuracy: 0.9624 - val_loss: 0.1037 - val_accuracy: 0.9707\n",
      "Epoch 86/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0985 - accuracy: 0.9620 - val_loss: 0.1104 - val_accuracy: 0.9696\n",
      "Epoch 87/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0982 - accuracy: 0.9634 - val_loss: 0.1213 - val_accuracy: 0.9645\n",
      "Epoch 88/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0965 - accuracy: 0.9629 - val_loss: 0.1234 - val_accuracy: 0.9645\n",
      "Epoch 89/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0978 - accuracy: 0.9626 - val_loss: 0.0950 - val_accuracy: 0.9757\n",
      "Epoch 90/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0986 - accuracy: 0.9609 - val_loss: 0.1246 - val_accuracy: 0.9609\n",
      "Epoch 91/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0959 - accuracy: 0.9628 - val_loss: 0.1210 - val_accuracy: 0.9626\n",
      "Epoch 92/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0967 - accuracy: 0.9632 - val_loss: 0.1057 - val_accuracy: 0.9737\n",
      "Epoch 93/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0972 - accuracy: 0.9626 - val_loss: 0.0983 - val_accuracy: 0.9740\n",
      "Epoch 94/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0987 - accuracy: 0.9622 - val_loss: 0.1202 - val_accuracy: 0.9662\n",
      "Epoch 95/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0980 - accuracy: 0.9623 - val_loss: 0.1090 - val_accuracy: 0.9698\n",
      "Epoch 96/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0963 - accuracy: 0.9621 - val_loss: 0.1340 - val_accuracy: 0.9545\n",
      "Epoch 97/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0960 - accuracy: 0.9642 - val_loss: 0.1154 - val_accuracy: 0.9696\n",
      "Epoch 98/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0954 - accuracy: 0.9637 - val_loss: 0.1389 - val_accuracy: 0.9511\n",
      "Epoch 99/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0966 - accuracy: 0.9611 - val_loss: 0.1066 - val_accuracy: 0.9735\n",
      "Epoch 100/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0969 - accuracy: 0.9629 - val_loss: 0.1158 - val_accuracy: 0.9662\n",
      "Epoch 101/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0944 - accuracy: 0.9639 - val_loss: 0.1179 - val_accuracy: 0.9687\n",
      "Epoch 102/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0934 - accuracy: 0.9641 - val_loss: 0.1221 - val_accuracy: 0.9623\n",
      "Epoch 103/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0947 - accuracy: 0.9633 - val_loss: 0.1110 - val_accuracy: 0.9712\n",
      "Epoch 104/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0935 - accuracy: 0.9633 - val_loss: 0.1036 - val_accuracy: 0.9726\n",
      "Epoch 105/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0943 - accuracy: 0.9646 - val_loss: 0.1226 - val_accuracy: 0.9654\n",
      "Epoch 106/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0951 - accuracy: 0.9627 - val_loss: 0.1089 - val_accuracy: 0.9718\n",
      "Epoch 107/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0911 - accuracy: 0.9649 - val_loss: 0.1251 - val_accuracy: 0.9612\n",
      "Epoch 108/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0932 - accuracy: 0.9639 - val_loss: 0.1180 - val_accuracy: 0.9662\n",
      "Epoch 109/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0914 - accuracy: 0.9650 - val_loss: 0.1101 - val_accuracy: 0.9704\n",
      "Epoch 110/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0923 - accuracy: 0.9643 - val_loss: 0.1296 - val_accuracy: 0.9601\n",
      "Epoch 111/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0913 - accuracy: 0.9653 - val_loss: 0.1023 - val_accuracy: 0.9743\n",
      "Epoch 112/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0915 - accuracy: 0.9654 - val_loss: 0.1235 - val_accuracy: 0.9651\n",
      "Epoch 113/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.0908 - accuracy: 0.9652 - val_loss: 0.1265 - val_accuracy: 0.9642\n",
      "Epoch 114/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.0902 - accuracy: 0.9645 - val_loss: 0.1163 - val_accuracy: 0.9682\n",
      "Epoch 115/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0891 - accuracy: 0.9656 - val_loss: 0.1260 - val_accuracy: 0.9623\n",
      "Epoch 116/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0907 - accuracy: 0.9646 - val_loss: 0.1177 - val_accuracy: 0.9687\n",
      "Epoch 117/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0913 - accuracy: 0.9642 - val_loss: 0.1233 - val_accuracy: 0.9665\n",
      "Epoch 118/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0878 - accuracy: 0.9655 - val_loss: 0.1061 - val_accuracy: 0.9712\n",
      "Epoch 119/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0883 - accuracy: 0.9656 - val_loss: 0.1115 - val_accuracy: 0.9709\n",
      "Epoch 120/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0876 - accuracy: 0.9659 - val_loss: 0.1191 - val_accuracy: 0.9704\n",
      "Epoch 121/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0870 - accuracy: 0.9667 - val_loss: 0.1242 - val_accuracy: 0.9654\n",
      "Epoch 122/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0885 - accuracy: 0.9649 - val_loss: 0.1241 - val_accuracy: 0.9654\n",
      "Epoch 123/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0882 - accuracy: 0.9656 - val_loss: 0.1299 - val_accuracy: 0.9648\n",
      "Epoch 124/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0884 - accuracy: 0.9650 - val_loss: 0.1120 - val_accuracy: 0.9712\n",
      "Epoch 125/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0880 - accuracy: 0.9657 - val_loss: 0.1396 - val_accuracy: 0.9547\n",
      "Epoch 126/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0860 - accuracy: 0.9672 - val_loss: 0.1190 - val_accuracy: 0.9696\n",
      "Epoch 127/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0849 - accuracy: 0.9673 - val_loss: 0.1319 - val_accuracy: 0.9598\n",
      "Epoch 128/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0863 - accuracy: 0.9664 - val_loss: 0.1287 - val_accuracy: 0.9592\n",
      "Epoch 129/300\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.0849 - accuracy: 0.9671 - val_loss: 0.1361 - val_accuracy: 0.9564\n",
      "Epoch 130/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0847 - accuracy: 0.9671 - val_loss: 0.1331 - val_accuracy: 0.9578\n",
      "Epoch 131/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0847 - accuracy: 0.9676 - val_loss: 0.1134 - val_accuracy: 0.9682\n",
      "Epoch 132/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0845 - accuracy: 0.9674 - val_loss: 0.1254 - val_accuracy: 0.9654\n",
      "Epoch 133/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0839 - accuracy: 0.9680 - val_loss: 0.1228 - val_accuracy: 0.9679\n",
      "Epoch 134/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0839 - accuracy: 0.9672 - val_loss: 0.1378 - val_accuracy: 0.9559\n",
      "Epoch 135/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0837 - accuracy: 0.9674 - val_loss: 0.1096 - val_accuracy: 0.9723\n",
      "Epoch 136/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0834 - accuracy: 0.9675 - val_loss: 0.1125 - val_accuracy: 0.9721\n",
      "Epoch 137/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0842 - accuracy: 0.9675 - val_loss: 0.1119 - val_accuracy: 0.9735\n",
      "Epoch 138/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0840 - accuracy: 0.9675 - val_loss: 0.1296 - val_accuracy: 0.9623\n",
      "Epoch 139/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0826 - accuracy: 0.9671 - val_loss: 0.1380 - val_accuracy: 0.9575\n",
      "Epoch 140/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0837 - accuracy: 0.9689 - val_loss: 0.1196 - val_accuracy: 0.9687\n",
      "Epoch 141/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0848 - accuracy: 0.9663 - val_loss: 0.1315 - val_accuracy: 0.9659\n",
      "Epoch 142/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0813 - accuracy: 0.9691 - val_loss: 0.1212 - val_accuracy: 0.9673\n",
      "Epoch 143/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0804 - accuracy: 0.9683 - val_loss: 0.1169 - val_accuracy: 0.9693\n",
      "Epoch 144/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0836 - accuracy: 0.9670 - val_loss: 0.1107 - val_accuracy: 0.9718\n",
      "Epoch 145/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0814 - accuracy: 0.9698 - val_loss: 0.1182 - val_accuracy: 0.9715\n",
      "Epoch 146/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0820 - accuracy: 0.9681 - val_loss: 0.1208 - val_accuracy: 0.9676\n",
      "Epoch 147/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0815 - accuracy: 0.9683 - val_loss: 0.1264 - val_accuracy: 0.9631\n",
      "Epoch 148/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0825 - accuracy: 0.9668 - val_loss: 0.1204 - val_accuracy: 0.9693\n",
      "Epoch 149/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0782 - accuracy: 0.9697 - val_loss: 0.1319 - val_accuracy: 0.9623\n",
      "Epoch 150/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0787 - accuracy: 0.9688 - val_loss: 0.1209 - val_accuracy: 0.9670\n",
      "Epoch 151/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0791 - accuracy: 0.9684 - val_loss: 0.1355 - val_accuracy: 0.9589\n",
      "Epoch 152/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0800 - accuracy: 0.9680 - val_loss: 0.1185 - val_accuracy: 0.9682\n",
      "Epoch 153/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0768 - accuracy: 0.9704 - val_loss: 0.1218 - val_accuracy: 0.9682\n",
      "Epoch 154/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0784 - accuracy: 0.9692 - val_loss: 0.1158 - val_accuracy: 0.9709\n",
      "Epoch 155/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0814 - accuracy: 0.9686 - val_loss: 0.1208 - val_accuracy: 0.9690\n",
      "Epoch 156/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0791 - accuracy: 0.9692 - val_loss: 0.1191 - val_accuracy: 0.9712\n",
      "Epoch 157/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0773 - accuracy: 0.9699 - val_loss: 0.1321 - val_accuracy: 0.9628\n",
      "Epoch 158/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0793 - accuracy: 0.9692 - val_loss: 0.1383 - val_accuracy: 0.9603\n",
      "Epoch 159/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0748 - accuracy: 0.9714 - val_loss: 0.1214 - val_accuracy: 0.9693\n",
      "Epoch 160/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0773 - accuracy: 0.9690 - val_loss: 0.1358 - val_accuracy: 0.9598\n",
      "Epoch 161/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0787 - accuracy: 0.9690 - val_loss: 0.1280 - val_accuracy: 0.9642\n",
      "Epoch 162/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0758 - accuracy: 0.9703 - val_loss: 0.1141 - val_accuracy: 0.9698\n",
      "Epoch 163/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0776 - accuracy: 0.9709 - val_loss: 0.1336 - val_accuracy: 0.9623\n",
      "Epoch 164/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0756 - accuracy: 0.9707 - val_loss: 0.1142 - val_accuracy: 0.9687\n",
      "Epoch 165/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0741 - accuracy: 0.9705 - val_loss: 0.1210 - val_accuracy: 0.9659\n",
      "Epoch 166/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0752 - accuracy: 0.9709 - val_loss: 0.1248 - val_accuracy: 0.9665\n",
      "Epoch 167/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0763 - accuracy: 0.9703 - val_loss: 0.1520 - val_accuracy: 0.9525\n",
      "Epoch 168/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0750 - accuracy: 0.9703 - val_loss: 0.1345 - val_accuracy: 0.9642\n",
      "Epoch 169/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0744 - accuracy: 0.9693 - val_loss: 0.1366 - val_accuracy: 0.9631\n",
      "Epoch 170/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0751 - accuracy: 0.9707 - val_loss: 0.1311 - val_accuracy: 0.9623\n",
      "Epoch 171/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0741 - accuracy: 0.9708 - val_loss: 0.1247 - val_accuracy: 0.9682\n",
      "Epoch 172/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0742 - accuracy: 0.9720 - val_loss: 0.1200 - val_accuracy: 0.9698\n",
      "Epoch 173/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0732 - accuracy: 0.9710 - val_loss: 0.1383 - val_accuracy: 0.9612\n",
      "Epoch 174/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0748 - accuracy: 0.9709 - val_loss: 0.1405 - val_accuracy: 0.9578\n",
      "Epoch 175/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0729 - accuracy: 0.9710 - val_loss: 0.1381 - val_accuracy: 0.9595\n",
      "Epoch 176/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0746 - accuracy: 0.9711 - val_loss: 0.1308 - val_accuracy: 0.9645\n",
      "Epoch 177/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0731 - accuracy: 0.9712 - val_loss: 0.1317 - val_accuracy: 0.9637\n",
      "Epoch 178/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0743 - accuracy: 0.9709 - val_loss: 0.1390 - val_accuracy: 0.9570\n",
      "Epoch 179/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.9714 - val_loss: 0.1241 - val_accuracy: 0.9668\n",
      "Epoch 180/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.9727 - val_loss: 0.1151 - val_accuracy: 0.9704\n",
      "Epoch 181/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.9710 - val_loss: 0.1395 - val_accuracy: 0.9628\n",
      "Epoch 182/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0733 - accuracy: 0.9708 - val_loss: 0.1328 - val_accuracy: 0.9623\n",
      "Epoch 183/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.9722 - val_loss: 0.1203 - val_accuracy: 0.9698\n",
      "Epoch 184/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0727 - accuracy: 0.9722 - val_loss: 0.1459 - val_accuracy: 0.9508\n",
      "Epoch 185/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0711 - accuracy: 0.9724 - val_loss: 0.1296 - val_accuracy: 0.9645\n",
      "Epoch 186/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0710 - accuracy: 0.9719 - val_loss: 0.1202 - val_accuracy: 0.9698\n",
      "Epoch 187/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0687 - accuracy: 0.9737 - val_loss: 0.1306 - val_accuracy: 0.9634\n",
      "Epoch 188/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.9715 - val_loss: 0.1226 - val_accuracy: 0.9684\n",
      "Epoch 189/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0707 - accuracy: 0.9720 - val_loss: 0.1205 - val_accuracy: 0.9693\n",
      "Epoch 190/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0708 - accuracy: 0.9722 - val_loss: 0.1444 - val_accuracy: 0.9564\n",
      "Epoch 191/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.9720 - val_loss: 0.1235 - val_accuracy: 0.9696\n",
      "Epoch 192/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0678 - accuracy: 0.9733 - val_loss: 0.1225 - val_accuracy: 0.9687\n",
      "Epoch 193/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0701 - accuracy: 0.9719 - val_loss: 0.1240 - val_accuracy: 0.9676\n",
      "Epoch 194/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0689 - accuracy: 0.9732 - val_loss: 0.1439 - val_accuracy: 0.9553\n",
      "Epoch 195/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0675 - accuracy: 0.9742 - val_loss: 0.1443 - val_accuracy: 0.9595\n",
      "Epoch 196/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0684 - accuracy: 0.9728 - val_loss: 0.1322 - val_accuracy: 0.9645\n",
      "Epoch 197/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0684 - accuracy: 0.9733 - val_loss: 0.1332 - val_accuracy: 0.9626\n",
      "Epoch 198/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.9722 - val_loss: 0.1177 - val_accuracy: 0.9687\n",
      "Epoch 199/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0677 - accuracy: 0.9737 - val_loss: 0.1435 - val_accuracy: 0.9587\n",
      "Epoch 200/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0679 - accuracy: 0.9738 - val_loss: 0.1419 - val_accuracy: 0.9603\n",
      "Epoch 201/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0664 - accuracy: 0.9737 - val_loss: 0.1338 - val_accuracy: 0.9642\n",
      "Epoch 202/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.9743 - val_loss: 0.1403 - val_accuracy: 0.9626\n",
      "Epoch 203/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0687 - accuracy: 0.9732 - val_loss: 0.1245 - val_accuracy: 0.9690\n",
      "Epoch 204/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0648 - accuracy: 0.9744 - val_loss: 0.1448 - val_accuracy: 0.9589\n",
      "Epoch 205/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0685 - accuracy: 0.9724 - val_loss: 0.1371 - val_accuracy: 0.9612\n",
      "Epoch 206/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0662 - accuracy: 0.9740 - val_loss: 0.1347 - val_accuracy: 0.9598\n",
      "Epoch 207/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0649 - accuracy: 0.9743 - val_loss: 0.1564 - val_accuracy: 0.9503\n",
      "Epoch 208/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0663 - accuracy: 0.9727 - val_loss: 0.1465 - val_accuracy: 0.9598\n",
      "Epoch 209/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0667 - accuracy: 0.9738 - val_loss: 0.1228 - val_accuracy: 0.9690\n",
      "Epoch 210/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0649 - accuracy: 0.9755 - val_loss: 0.1304 - val_accuracy: 0.9682\n",
      "Epoch 211/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0637 - accuracy: 0.9753 - val_loss: 0.1325 - val_accuracy: 0.9642\n",
      "Epoch 212/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0659 - accuracy: 0.9736 - val_loss: 0.1232 - val_accuracy: 0.9696\n",
      "Epoch 213/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0660 - accuracy: 0.9743 - val_loss: 0.1307 - val_accuracy: 0.9640\n",
      "Epoch 214/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0674 - accuracy: 0.9735 - val_loss: 0.1317 - val_accuracy: 0.9687\n",
      "Epoch 215/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0663 - accuracy: 0.9730 - val_loss: 0.1330 - val_accuracy: 0.9654\n",
      "Epoch 216/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0648 - accuracy: 0.9752 - val_loss: 0.1271 - val_accuracy: 0.9665\n",
      "Epoch 217/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0641 - accuracy: 0.9747 - val_loss: 0.1530 - val_accuracy: 0.9575\n",
      "Epoch 218/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0642 - accuracy: 0.9750 - val_loss: 0.1324 - val_accuracy: 0.9659\n",
      "Epoch 219/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0628 - accuracy: 0.9745 - val_loss: 0.1408 - val_accuracy: 0.9609\n",
      "Epoch 220/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0648 - accuracy: 0.9738 - val_loss: 0.1347 - val_accuracy: 0.9648\n",
      "Epoch 221/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0641 - accuracy: 0.9750 - val_loss: 0.1239 - val_accuracy: 0.9707\n",
      "Epoch 222/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0631 - accuracy: 0.9748 - val_loss: 0.1300 - val_accuracy: 0.9690\n",
      "Epoch 223/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0631 - accuracy: 0.9746 - val_loss: 0.1459 - val_accuracy: 0.9584\n",
      "Epoch 224/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0626 - accuracy: 0.9753 - val_loss: 0.1431 - val_accuracy: 0.9606\n",
      "Epoch 225/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0622 - accuracy: 0.9760 - val_loss: 0.1254 - val_accuracy: 0.9709\n",
      "Epoch 226/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0633 - accuracy: 0.9752 - val_loss: 0.1251 - val_accuracy: 0.9729\n",
      "Epoch 227/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0620 - accuracy: 0.9753 - val_loss: 0.1325 - val_accuracy: 0.9662\n",
      "Epoch 228/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0623 - accuracy: 0.9757 - val_loss: 0.1602 - val_accuracy: 0.9522\n",
      "Epoch 229/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0627 - accuracy: 0.9743 - val_loss: 0.1263 - val_accuracy: 0.9682\n",
      "Epoch 230/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0602 - accuracy: 0.9767 - val_loss: 0.1426 - val_accuracy: 0.9615\n",
      "Epoch 231/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0605 - accuracy: 0.9762 - val_loss: 0.1549 - val_accuracy: 0.9520\n",
      "Epoch 232/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0618 - accuracy: 0.9759 - val_loss: 0.1469 - val_accuracy: 0.9587\n",
      "Epoch 233/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0607 - accuracy: 0.9765 - val_loss: 0.1198 - val_accuracy: 0.9746\n",
      "Epoch 234/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0596 - accuracy: 0.9769 - val_loss: 0.1513 - val_accuracy: 0.9584\n",
      "Epoch 235/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0629 - accuracy: 0.9751 - val_loss: 0.1431 - val_accuracy: 0.9601\n",
      "Epoch 236/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0619 - accuracy: 0.9755 - val_loss: 0.1241 - val_accuracy: 0.9693\n",
      "Epoch 237/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0597 - accuracy: 0.9765 - val_loss: 0.1380 - val_accuracy: 0.9659\n",
      "Epoch 238/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0608 - accuracy: 0.9763 - val_loss: 0.1475 - val_accuracy: 0.9584\n",
      "Epoch 239/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0602 - accuracy: 0.9758 - val_loss: 0.1332 - val_accuracy: 0.9687\n",
      "Epoch 240/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0598 - accuracy: 0.9763 - val_loss: 0.1379 - val_accuracy: 0.9634\n",
      "Epoch 241/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0589 - accuracy: 0.9772 - val_loss: 0.1423 - val_accuracy: 0.9640\n",
      "Epoch 242/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0581 - accuracy: 0.9760 - val_loss: 0.1408 - val_accuracy: 0.9662\n",
      "Epoch 243/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0589 - accuracy: 0.9762 - val_loss: 0.1326 - val_accuracy: 0.9693\n",
      "Epoch 244/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.9773 - val_loss: 0.1467 - val_accuracy: 0.9620\n",
      "Epoch 245/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0587 - accuracy: 0.9764 - val_loss: 0.1415 - val_accuracy: 0.9654\n",
      "Epoch 246/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0594 - accuracy: 0.9767 - val_loss: 0.1346 - val_accuracy: 0.9721\n",
      "Epoch 247/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0576 - accuracy: 0.9773 - val_loss: 0.1430 - val_accuracy: 0.9626\n",
      "Epoch 248/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0597 - accuracy: 0.9761 - val_loss: 0.1435 - val_accuracy: 0.9601\n",
      "Epoch 249/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0578 - accuracy: 0.9770 - val_loss: 0.1341 - val_accuracy: 0.9684\n",
      "Epoch 250/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0588 - accuracy: 0.9765 - val_loss: 0.1552 - val_accuracy: 0.9573\n",
      "Epoch 251/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0601 - accuracy: 0.9757 - val_loss: 0.1356 - val_accuracy: 0.9676\n",
      "Epoch 252/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0599 - accuracy: 0.9764 - val_loss: 0.1382 - val_accuracy: 0.9648\n",
      "Epoch 253/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0574 - accuracy: 0.9775 - val_loss: 0.1548 - val_accuracy: 0.9559\n",
      "Epoch 254/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0571 - accuracy: 0.9783 - val_loss: 0.1349 - val_accuracy: 0.9687\n",
      "Epoch 255/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0573 - accuracy: 0.9774 - val_loss: 0.1490 - val_accuracy: 0.9603\n",
      "Epoch 256/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0615 - accuracy: 0.9763 - val_loss: 0.1486 - val_accuracy: 0.9620\n",
      "Epoch 257/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0562 - accuracy: 0.9780 - val_loss: 0.1580 - val_accuracy: 0.9553\n",
      "Epoch 258/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.9779 - val_loss: 0.1276 - val_accuracy: 0.9698\n",
      "Epoch 259/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.9773 - val_loss: 0.1453 - val_accuracy: 0.9645\n",
      "Epoch 260/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0576 - accuracy: 0.9774 - val_loss: 0.1402 - val_accuracy: 0.9642\n",
      "Epoch 261/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0550 - accuracy: 0.9786 - val_loss: 0.1384 - val_accuracy: 0.9670\n",
      "Epoch 262/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0554 - accuracy: 0.9777 - val_loss: 0.1286 - val_accuracy: 0.9679\n",
      "Epoch 263/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0575 - accuracy: 0.9766 - val_loss: 0.1500 - val_accuracy: 0.9670\n",
      "Epoch 264/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0569 - accuracy: 0.9770 - val_loss: 0.1388 - val_accuracy: 0.9651\n",
      "Epoch 265/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0562 - accuracy: 0.9781 - val_loss: 0.1429 - val_accuracy: 0.9665\n",
      "Epoch 266/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0557 - accuracy: 0.9786 - val_loss: 0.1368 - val_accuracy: 0.9670\n",
      "Epoch 267/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0528 - accuracy: 0.9793 - val_loss: 0.1362 - val_accuracy: 0.9676\n",
      "Epoch 268/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0560 - accuracy: 0.9780 - val_loss: 0.1370 - val_accuracy: 0.9707\n",
      "Epoch 269/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0549 - accuracy: 0.9783 - val_loss: 0.1551 - val_accuracy: 0.9553\n",
      "Epoch 270/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0538 - accuracy: 0.9799 - val_loss: 0.1626 - val_accuracy: 0.9545\n",
      "Epoch 271/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0560 - accuracy: 0.9781 - val_loss: 0.1668 - val_accuracy: 0.9547\n",
      "Epoch 272/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0546 - accuracy: 0.9793 - val_loss: 0.1507 - val_accuracy: 0.9612\n",
      "Epoch 273/300\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9790 - val_loss: 0.1426 - val_accuracy: 0.9668\n",
      "Epoch 274/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0549 - accuracy: 0.9780 - val_loss: 0.1344 - val_accuracy: 0.9693\n",
      "Epoch 275/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0545 - accuracy: 0.9795 - val_loss: 0.1466 - val_accuracy: 0.9668\n",
      "Epoch 276/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0552 - accuracy: 0.9786 - val_loss: 0.1521 - val_accuracy: 0.9612\n",
      "Epoch 277/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0529 - accuracy: 0.9798 - val_loss: 0.1740 - val_accuracy: 0.9506\n",
      "Epoch 278/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0538 - accuracy: 0.9780 - val_loss: 0.1489 - val_accuracy: 0.9648\n",
      "Epoch 279/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0519 - accuracy: 0.9805 - val_loss: 0.1495 - val_accuracy: 0.9623\n",
      "Epoch 280/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0541 - accuracy: 0.9787 - val_loss: 0.1341 - val_accuracy: 0.9687\n",
      "Epoch 281/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0530 - accuracy: 0.9796 - val_loss: 0.1544 - val_accuracy: 0.9587\n",
      "Epoch 282/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0515 - accuracy: 0.9801 - val_loss: 0.1327 - val_accuracy: 0.9709\n",
      "Epoch 283/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0516 - accuracy: 0.9799 - val_loss: 0.1338 - val_accuracy: 0.9715\n",
      "Epoch 284/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0523 - accuracy: 0.9795 - val_loss: 0.1454 - val_accuracy: 0.9634\n",
      "Epoch 285/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0519 - accuracy: 0.9791 - val_loss: 0.1475 - val_accuracy: 0.9651\n",
      "Epoch 286/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0502 - accuracy: 0.9798 - val_loss: 0.1498 - val_accuracy: 0.9609\n",
      "Epoch 287/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0543 - accuracy: 0.9780 - val_loss: 0.1740 - val_accuracy: 0.9506\n",
      "Epoch 288/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0519 - accuracy: 0.9801 - val_loss: 0.1499 - val_accuracy: 0.9628\n",
      "Epoch 289/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0514 - accuracy: 0.9805 - val_loss: 0.1419 - val_accuracy: 0.9662\n",
      "Epoch 290/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0500 - accuracy: 0.9810 - val_loss: 0.1474 - val_accuracy: 0.9665\n",
      "Epoch 291/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0515 - accuracy: 0.9791 - val_loss: 0.1388 - val_accuracy: 0.9679\n",
      "Epoch 292/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0521 - accuracy: 0.9799 - val_loss: 0.1368 - val_accuracy: 0.9684\n",
      "Epoch 293/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0503 - accuracy: 0.9806 - val_loss: 0.1555 - val_accuracy: 0.9589\n",
      "Epoch 294/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0525 - accuracy: 0.9803 - val_loss: 0.1420 - val_accuracy: 0.9679\n",
      "Epoch 295/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0509 - accuracy: 0.9804 - val_loss: 0.1699 - val_accuracy: 0.9486\n",
      "Epoch 296/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0500 - accuracy: 0.9806 - val_loss: 0.1347 - val_accuracy: 0.9698\n",
      "Epoch 297/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0487 - accuracy: 0.9814 - val_loss: 0.1575 - val_accuracy: 0.9581\n",
      "Epoch 298/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0523 - accuracy: 0.9798 - val_loss: 0.1645 - val_accuracy: 0.9559\n",
      "Epoch 299/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0493 - accuracy: 0.9811 - val_loss: 0.1547 - val_accuracy: 0.9631\n",
      "Epoch 300/300\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0502 - accuracy: 0.9806 - val_loss: 0.1490 - val_accuracy: 0.9640\n",
      "Accuracy: 0.9639664888381958\n",
      " MSE: 0.14901340007781982\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='elu', input_shape=(X_trains.shape[1],)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(128, activation='elu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(64, activation='elu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(32, activation='elu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adamax()\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 학습 자동 중단 설정\n",
    "es = EarlyStopping(monitor='loss', patience=50, mode='min')\n",
    "rlrp = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=40, mode='min')\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X_train_oversampled, y_train_oversampled, epochs=300, batch_size=300, verbose=1, \n",
    "          validation_data=(X_tests, y_test), callbacks=[es, rlrp])\n",
    "\n",
    "# 모델 평가\n",
    "y_pred = model.predict(X_tests)\n",
    "    \n",
    "loss, accuracy = model.evaluate(X_tests, y_test, verbose=0)\n",
    "print(f'Accuracy: {accuracy}\\n MSE: {loss}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 배치정규화, 클래스 가중치 부여, relu, adamax, es, rlrp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "448/448 [==============================] - 2s 2ms/step - loss: 0.2551 - accuracy: 0.9071 - val_loss: 0.2386 - val_accuracy: 0.9497\n",
      "Epoch 2/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1946 - accuracy: 0.9496 - val_loss: 0.1789 - val_accuracy: 0.9592\n",
      "Epoch 3/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1742 - accuracy: 0.9577 - val_loss: 0.1609 - val_accuracy: 0.9592\n",
      "Epoch 4/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1664 - accuracy: 0.9603 - val_loss: 0.1327 - val_accuracy: 0.9690\n",
      "Epoch 5/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1602 - accuracy: 0.9647 - val_loss: 0.1374 - val_accuracy: 0.9654\n",
      "Epoch 6/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1588 - accuracy: 0.9649 - val_loss: 0.1334 - val_accuracy: 0.9654\n",
      "Epoch 7/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1518 - accuracy: 0.9640 - val_loss: 0.1354 - val_accuracy: 0.9651\n",
      "Epoch 8/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1548 - accuracy: 0.9645 - val_loss: 0.1229 - val_accuracy: 0.9723\n",
      "Epoch 9/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1503 - accuracy: 0.9683 - val_loss: 0.1209 - val_accuracy: 0.9735\n",
      "Epoch 10/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1496 - accuracy: 0.9656 - val_loss: 0.1237 - val_accuracy: 0.9698\n",
      "Epoch 11/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1516 - accuracy: 0.9677 - val_loss: 0.1298 - val_accuracy: 0.9684\n",
      "Epoch 12/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1485 - accuracy: 0.9685 - val_loss: 0.1397 - val_accuracy: 0.9612\n",
      "Epoch 13/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1507 - accuracy: 0.9682 - val_loss: 0.1269 - val_accuracy: 0.9718\n",
      "Epoch 14/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1443 - accuracy: 0.9665 - val_loss: 0.1417 - val_accuracy: 0.9609\n",
      "Epoch 15/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1459 - accuracy: 0.9670 - val_loss: 0.1272 - val_accuracy: 0.9676\n",
      "Epoch 16/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1427 - accuracy: 0.9678 - val_loss: 0.1226 - val_accuracy: 0.9668\n",
      "Epoch 17/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1431 - accuracy: 0.9682 - val_loss: 0.1246 - val_accuracy: 0.9693\n",
      "Epoch 18/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1413 - accuracy: 0.9686 - val_loss: 0.1365 - val_accuracy: 0.9642\n",
      "Epoch 19/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1360 - accuracy: 0.9693 - val_loss: 0.1192 - val_accuracy: 0.9721\n",
      "Epoch 20/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1459 - accuracy: 0.9680 - val_loss: 0.1271 - val_accuracy: 0.9684\n",
      "Epoch 21/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1378 - accuracy: 0.9701 - val_loss: 0.1325 - val_accuracy: 0.9645\n",
      "Epoch 22/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1380 - accuracy: 0.9681 - val_loss: 0.1175 - val_accuracy: 0.9729\n",
      "Epoch 23/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1377 - accuracy: 0.9696 - val_loss: 0.1351 - val_accuracy: 0.9620\n",
      "Epoch 24/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1351 - accuracy: 0.9695 - val_loss: 0.1301 - val_accuracy: 0.9628\n",
      "Epoch 25/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1343 - accuracy: 0.9694 - val_loss: 0.1354 - val_accuracy: 0.9631\n",
      "Epoch 26/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1337 - accuracy: 0.9680 - val_loss: 0.1213 - val_accuracy: 0.9654\n",
      "Epoch 27/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1332 - accuracy: 0.9683 - val_loss: 0.1303 - val_accuracy: 0.9631\n",
      "Epoch 28/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1332 - accuracy: 0.9683 - val_loss: 0.1323 - val_accuracy: 0.9612\n",
      "Epoch 29/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1302 - accuracy: 0.9687 - val_loss: 0.1307 - val_accuracy: 0.9623\n",
      "Epoch 30/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1298 - accuracy: 0.9702 - val_loss: 0.1275 - val_accuracy: 0.9631\n",
      "Epoch 31/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1296 - accuracy: 0.9679 - val_loss: 0.1258 - val_accuracy: 0.9673\n",
      "Epoch 32/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1302 - accuracy: 0.9674 - val_loss: 0.1236 - val_accuracy: 0.9684\n",
      "Epoch 33/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1273 - accuracy: 0.9693 - val_loss: 0.1302 - val_accuracy: 0.9654\n",
      "Epoch 34/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1289 - accuracy: 0.9677 - val_loss: 0.1257 - val_accuracy: 0.9665\n",
      "Epoch 35/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1321 - accuracy: 0.9684 - val_loss: 0.1349 - val_accuracy: 0.9634\n",
      "Epoch 36/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1318 - accuracy: 0.9689 - val_loss: 0.1172 - val_accuracy: 0.9687\n",
      "Epoch 37/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1273 - accuracy: 0.9682 - val_loss: 0.1208 - val_accuracy: 0.9642\n",
      "Epoch 38/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1257 - accuracy: 0.9706 - val_loss: 0.1226 - val_accuracy: 0.9668\n",
      "Epoch 39/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1287 - accuracy: 0.9705 - val_loss: 0.1315 - val_accuracy: 0.9620\n",
      "Epoch 40/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1247 - accuracy: 0.9681 - val_loss: 0.1138 - val_accuracy: 0.9693\n",
      "Epoch 41/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1231 - accuracy: 0.9696 - val_loss: 0.1301 - val_accuracy: 0.9617\n",
      "Epoch 42/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1239 - accuracy: 0.9686 - val_loss: 0.1189 - val_accuracy: 0.9709\n",
      "Epoch 43/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1255 - accuracy: 0.9686 - val_loss: 0.1517 - val_accuracy: 0.9511\n",
      "Epoch 44/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9682 - val_loss: 0.1246 - val_accuracy: 0.9670\n",
      "Epoch 45/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1263 - accuracy: 0.9672 - val_loss: 0.1339 - val_accuracy: 0.9606\n",
      "Epoch 46/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1201 - accuracy: 0.9682 - val_loss: 0.1204 - val_accuracy: 0.9665\n",
      "Epoch 47/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1192 - accuracy: 0.9684 - val_loss: 0.1338 - val_accuracy: 0.9612\n",
      "Epoch 48/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9698 - val_loss: 0.1199 - val_accuracy: 0.9651\n",
      "Epoch 49/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1198 - accuracy: 0.9681 - val_loss: 0.1116 - val_accuracy: 0.9735\n",
      "Epoch 50/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1201 - accuracy: 0.9714 - val_loss: 0.1262 - val_accuracy: 0.9651\n",
      "Epoch 51/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1222 - accuracy: 0.9679 - val_loss: 0.1321 - val_accuracy: 0.9623\n",
      "Epoch 52/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1204 - accuracy: 0.9700 - val_loss: 0.1206 - val_accuracy: 0.9673\n",
      "Epoch 53/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1196 - accuracy: 0.9691 - val_loss: 0.1306 - val_accuracy: 0.9640\n",
      "Epoch 54/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1235 - accuracy: 0.9678 - val_loss: 0.1241 - val_accuracy: 0.9640\n",
      "Epoch 55/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1232 - accuracy: 0.9689 - val_loss: 0.1309 - val_accuracy: 0.9642\n",
      "Epoch 56/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1156 - accuracy: 0.9698 - val_loss: 0.1193 - val_accuracy: 0.9651\n",
      "Epoch 57/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1204 - accuracy: 0.9684 - val_loss: 0.1224 - val_accuracy: 0.9651\n",
      "Epoch 58/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1180 - accuracy: 0.9681 - val_loss: 0.1208 - val_accuracy: 0.9679\n",
      "Epoch 59/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1199 - accuracy: 0.9684 - val_loss: 0.1177 - val_accuracy: 0.9687\n",
      "Epoch 60/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1205 - accuracy: 0.9701 - val_loss: 0.1227 - val_accuracy: 0.9651\n",
      "Epoch 61/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1158 - accuracy: 0.9686 - val_loss: 0.1277 - val_accuracy: 0.9623\n",
      "Epoch 62/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1140 - accuracy: 0.9679 - val_loss: 0.1307 - val_accuracy: 0.9615\n",
      "Epoch 63/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1184 - accuracy: 0.9682 - val_loss: 0.1356 - val_accuracy: 0.9612\n",
      "Epoch 64/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1202 - accuracy: 0.9693 - val_loss: 0.1290 - val_accuracy: 0.9615\n",
      "Epoch 65/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1171 - accuracy: 0.9680 - val_loss: 0.1408 - val_accuracy: 0.9553\n",
      "Epoch 66/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1177 - accuracy: 0.9682 - val_loss: 0.1241 - val_accuracy: 0.9626\n",
      "Epoch 67/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1165 - accuracy: 0.9705 - val_loss: 0.1273 - val_accuracy: 0.9648\n",
      "Epoch 68/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1138 - accuracy: 0.9698 - val_loss: 0.1288 - val_accuracy: 0.9628\n",
      "Epoch 69/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1159 - accuracy: 0.9705 - val_loss: 0.1221 - val_accuracy: 0.9654\n",
      "Epoch 70/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1083 - accuracy: 0.9701 - val_loss: 0.1123 - val_accuracy: 0.9709\n",
      "Epoch 71/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1098 - accuracy: 0.9691 - val_loss: 0.1310 - val_accuracy: 0.9612\n",
      "Epoch 72/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1114 - accuracy: 0.9679 - val_loss: 0.1338 - val_accuracy: 0.9587\n",
      "Epoch 73/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1102 - accuracy: 0.9705 - val_loss: 0.1388 - val_accuracy: 0.9587\n",
      "Epoch 74/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1153 - accuracy: 0.9682 - val_loss: 0.1444 - val_accuracy: 0.9536\n",
      "Epoch 75/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1110 - accuracy: 0.9688 - val_loss: 0.1217 - val_accuracy: 0.9648\n",
      "Epoch 76/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1132 - accuracy: 0.9688 - val_loss: 0.1246 - val_accuracy: 0.9648\n",
      "Epoch 77/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1123 - accuracy: 0.9661 - val_loss: 0.1203 - val_accuracy: 0.9651\n",
      "Epoch 78/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1050 - accuracy: 0.9697 - val_loss: 0.1406 - val_accuracy: 0.9561\n",
      "Epoch 79/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1119 - accuracy: 0.9650 - val_loss: 0.1401 - val_accuracy: 0.9570\n",
      "Epoch 80/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1133 - accuracy: 0.9683 - val_loss: 0.1303 - val_accuracy: 0.9612\n",
      "Epoch 81/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1076 - accuracy: 0.9707 - val_loss: 0.1183 - val_accuracy: 0.9665\n",
      "Epoch 82/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1066 - accuracy: 0.9702 - val_loss: 0.1200 - val_accuracy: 0.9648\n",
      "Epoch 83/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1096 - accuracy: 0.9670 - val_loss: 0.1269 - val_accuracy: 0.9631\n",
      "Epoch 84/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1125 - accuracy: 0.9683 - val_loss: 0.1204 - val_accuracy: 0.9648\n",
      "Epoch 85/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1129 - accuracy: 0.9684 - val_loss: 0.1291 - val_accuracy: 0.9620\n",
      "Epoch 86/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1064 - accuracy: 0.9683 - val_loss: 0.1253 - val_accuracy: 0.9631\n",
      "Epoch 87/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1060 - accuracy: 0.9685 - val_loss: 0.1320 - val_accuracy: 0.9598\n",
      "Epoch 88/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1065 - accuracy: 0.9677 - val_loss: 0.1151 - val_accuracy: 0.9656\n",
      "Epoch 89/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1106 - accuracy: 0.9710 - val_loss: 0.1180 - val_accuracy: 0.9645\n",
      "Epoch 90/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1043 - accuracy: 0.9652 - val_loss: 0.1339 - val_accuracy: 0.9584\n",
      "Epoch 91/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1048 - accuracy: 0.9676 - val_loss: 0.1278 - val_accuracy: 0.9620\n",
      "Epoch 92/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1067 - accuracy: 0.9693 - val_loss: 0.1447 - val_accuracy: 0.9559\n",
      "Epoch 93/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1031 - accuracy: 0.9695 - val_loss: 0.1188 - val_accuracy: 0.9637\n",
      "Epoch 94/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1033 - accuracy: 0.9680 - val_loss: 0.1306 - val_accuracy: 0.9581\n",
      "Epoch 95/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1003 - accuracy: 0.9699 - val_loss: 0.1264 - val_accuracy: 0.9615\n",
      "Epoch 96/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1024 - accuracy: 0.9692 - val_loss: 0.1300 - val_accuracy: 0.9598\n",
      "Epoch 97/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1042 - accuracy: 0.9700 - val_loss: 0.1344 - val_accuracy: 0.9606\n",
      "Epoch 98/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0996 - accuracy: 0.9696 - val_loss: 0.1390 - val_accuracy: 0.9520\n",
      "Epoch 99/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0989 - accuracy: 0.9675 - val_loss: 0.1126 - val_accuracy: 0.9698\n",
      "Epoch 100/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1017 - accuracy: 0.9711 - val_loss: 0.1329 - val_accuracy: 0.9587\n",
      "Epoch 101/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1009 - accuracy: 0.9684 - val_loss: 0.1368 - val_accuracy: 0.9575\n",
      "Epoch 102/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1027 - accuracy: 0.9684 - val_loss: 0.1305 - val_accuracy: 0.9587\n",
      "Epoch 103/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1030 - accuracy: 0.9687 - val_loss: 0.1252 - val_accuracy: 0.9601\n",
      "Epoch 104/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1025 - accuracy: 0.9692 - val_loss: 0.1298 - val_accuracy: 0.9609\n",
      "Epoch 105/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1013 - accuracy: 0.9689 - val_loss: 0.1256 - val_accuracy: 0.9626\n",
      "Epoch 106/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0991 - accuracy: 0.9709 - val_loss: 0.1150 - val_accuracy: 0.9662\n",
      "Epoch 107/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0983 - accuracy: 0.9676 - val_loss: 0.1240 - val_accuracy: 0.9584\n",
      "Epoch 108/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1002 - accuracy: 0.9698 - val_loss: 0.1365 - val_accuracy: 0.9584\n",
      "Epoch 109/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0982 - accuracy: 0.9689 - val_loss: 0.1207 - val_accuracy: 0.9654\n",
      "Epoch 110/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0995 - accuracy: 0.9689 - val_loss: 0.1211 - val_accuracy: 0.9656\n",
      "Epoch 111/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1013 - accuracy: 0.9692 - val_loss: 0.1291 - val_accuracy: 0.9620\n",
      "Epoch 112/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0978 - accuracy: 0.9687 - val_loss: 0.1242 - val_accuracy: 0.9626\n",
      "Epoch 113/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0982 - accuracy: 0.9691 - val_loss: 0.1262 - val_accuracy: 0.9601\n",
      "Epoch 114/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0941 - accuracy: 0.9673 - val_loss: 0.1247 - val_accuracy: 0.9626\n",
      "Epoch 115/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1002 - accuracy: 0.9688 - val_loss: 0.1303 - val_accuracy: 0.9603\n",
      "Epoch 116/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0961 - accuracy: 0.9688 - val_loss: 0.1217 - val_accuracy: 0.9642\n",
      "Epoch 117/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0984 - accuracy: 0.9693 - val_loss: 0.1179 - val_accuracy: 0.9648\n",
      "Epoch 118/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0961 - accuracy: 0.9678 - val_loss: 0.1257 - val_accuracy: 0.9615\n",
      "Epoch 119/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0986 - accuracy: 0.9677 - val_loss: 0.1502 - val_accuracy: 0.9464\n",
      "Epoch 120/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0958 - accuracy: 0.9695 - val_loss: 0.1343 - val_accuracy: 0.9584\n",
      "Epoch 121/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0953 - accuracy: 0.9691 - val_loss: 0.1286 - val_accuracy: 0.9587\n",
      "Epoch 122/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0918 - accuracy: 0.9715 - val_loss: 0.1429 - val_accuracy: 0.9514\n",
      "Epoch 123/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0938 - accuracy: 0.9699 - val_loss: 0.1491 - val_accuracy: 0.9475\n",
      "Epoch 124/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0976 - accuracy: 0.9678 - val_loss: 0.1338 - val_accuracy: 0.9573\n",
      "Epoch 125/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0905 - accuracy: 0.9700 - val_loss: 0.1414 - val_accuracy: 0.9517\n",
      "Epoch 126/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0962 - accuracy: 0.9669 - val_loss: 0.1371 - val_accuracy: 0.9553\n",
      "Epoch 127/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0938 - accuracy: 0.9693 - val_loss: 0.1253 - val_accuracy: 0.9617\n",
      "Epoch 128/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0962 - accuracy: 0.9702 - val_loss: 0.1260 - val_accuracy: 0.9601\n",
      "Epoch 129/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0950 - accuracy: 0.9696 - val_loss: 0.1342 - val_accuracy: 0.9567\n",
      "Epoch 130/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0920 - accuracy: 0.9698 - val_loss: 0.1458 - val_accuracy: 0.9517\n",
      "Epoch 131/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0909 - accuracy: 0.9684 - val_loss: 0.1244 - val_accuracy: 0.9637\n",
      "Epoch 132/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0944 - accuracy: 0.9678 - val_loss: 0.1325 - val_accuracy: 0.9581\n",
      "Epoch 133/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0897 - accuracy: 0.9700 - val_loss: 0.1107 - val_accuracy: 0.9693\n",
      "Epoch 134/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0933 - accuracy: 0.9697 - val_loss: 0.1331 - val_accuracy: 0.9595\n",
      "Epoch 135/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0899 - accuracy: 0.9710 - val_loss: 0.1380 - val_accuracy: 0.9534\n",
      "Epoch 136/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0885 - accuracy: 0.9702 - val_loss: 0.1276 - val_accuracy: 0.9631\n",
      "Epoch 137/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0910 - accuracy: 0.9687 - val_loss: 0.1481 - val_accuracy: 0.9511\n",
      "Epoch 138/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0893 - accuracy: 0.9693 - val_loss: 0.1296 - val_accuracy: 0.9609\n",
      "Epoch 139/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0950 - accuracy: 0.9680 - val_loss: 0.1263 - val_accuracy: 0.9615\n",
      "Epoch 140/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0936 - accuracy: 0.9693 - val_loss: 0.1323 - val_accuracy: 0.9545\n",
      "Epoch 141/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0905 - accuracy: 0.9700 - val_loss: 0.1437 - val_accuracy: 0.9539\n",
      "Epoch 142/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0928 - accuracy: 0.9700 - val_loss: 0.1300 - val_accuracy: 0.9581\n",
      "Epoch 143/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0894 - accuracy: 0.9686 - val_loss: 0.1328 - val_accuracy: 0.9567\n",
      "Epoch 144/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0887 - accuracy: 0.9695 - val_loss: 0.1248 - val_accuracy: 0.9612\n",
      "Epoch 145/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0844 - accuracy: 0.9709 - val_loss: 0.1506 - val_accuracy: 0.9497\n",
      "Epoch 146/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0933 - accuracy: 0.9689 - val_loss: 0.1349 - val_accuracy: 0.9584\n",
      "Epoch 147/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0877 - accuracy: 0.9705 - val_loss: 0.1357 - val_accuracy: 0.9531\n",
      "Epoch 148/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0925 - accuracy: 0.9691 - val_loss: 0.1477 - val_accuracy: 0.9469\n",
      "Epoch 149/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0910 - accuracy: 0.9681 - val_loss: 0.1294 - val_accuracy: 0.9587\n",
      "Epoch 150/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0868 - accuracy: 0.9693 - val_loss: 0.1070 - val_accuracy: 0.9696\n",
      "Epoch 151/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0916 - accuracy: 0.9705 - val_loss: 0.1347 - val_accuracy: 0.9525\n",
      "Epoch 152/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0902 - accuracy: 0.9690 - val_loss: 0.1294 - val_accuracy: 0.9525\n",
      "Epoch 153/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0881 - accuracy: 0.9692 - val_loss: 0.1236 - val_accuracy: 0.9592\n",
      "Epoch 154/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0914 - accuracy: 0.9682 - val_loss: 0.1351 - val_accuracy: 0.9550\n",
      "Epoch 155/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0828 - accuracy: 0.9719 - val_loss: 0.1205 - val_accuracy: 0.9648\n",
      "Epoch 156/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0890 - accuracy: 0.9701 - val_loss: 0.1247 - val_accuracy: 0.9587\n",
      "Epoch 157/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0879 - accuracy: 0.9675 - val_loss: 0.1542 - val_accuracy: 0.9436\n",
      "Epoch 158/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0884 - accuracy: 0.9694 - val_loss: 0.1199 - val_accuracy: 0.9640\n",
      "Epoch 159/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0921 - accuracy: 0.9700 - val_loss: 0.1477 - val_accuracy: 0.9500\n",
      "Epoch 160/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0799 - accuracy: 0.9730 - val_loss: 0.1379 - val_accuracy: 0.9567\n",
      "Epoch 161/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0843 - accuracy: 0.9715 - val_loss: 0.1339 - val_accuracy: 0.9550\n",
      "Epoch 162/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0842 - accuracy: 0.9717 - val_loss: 0.1272 - val_accuracy: 0.9595\n",
      "Epoch 163/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0856 - accuracy: 0.9707 - val_loss: 0.1272 - val_accuracy: 0.9628\n",
      "Epoch 164/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0869 - accuracy: 0.9694 - val_loss: 0.1354 - val_accuracy: 0.9589\n",
      "Epoch 165/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0918 - accuracy: 0.9712 - val_loss: 0.1350 - val_accuracy: 0.9567\n",
      "Epoch 166/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0872 - accuracy: 0.9714 - val_loss: 0.1432 - val_accuracy: 0.9528\n",
      "Epoch 167/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0805 - accuracy: 0.9721 - val_loss: 0.1222 - val_accuracy: 0.9609\n",
      "Epoch 168/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0841 - accuracy: 0.9714 - val_loss: 0.1345 - val_accuracy: 0.9575\n",
      "Epoch 169/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0857 - accuracy: 0.9708 - val_loss: 0.1248 - val_accuracy: 0.9620\n",
      "Epoch 170/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0839 - accuracy: 0.9718 - val_loss: 0.1341 - val_accuracy: 0.9587\n",
      "Epoch 171/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0800 - accuracy: 0.9703 - val_loss: 0.1251 - val_accuracy: 0.9603\n",
      "Epoch 172/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0887 - accuracy: 0.9710 - val_loss: 0.1373 - val_accuracy: 0.9545\n",
      "Epoch 173/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0872 - accuracy: 0.9682 - val_loss: 0.1319 - val_accuracy: 0.9592\n",
      "Epoch 174/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0811 - accuracy: 0.9726 - val_loss: 0.1410 - val_accuracy: 0.9475\n",
      "Epoch 175/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0834 - accuracy: 0.9714 - val_loss: 0.1474 - val_accuracy: 0.9478\n",
      "Epoch 176/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0788 - accuracy: 0.9714 - val_loss: 0.1452 - val_accuracy: 0.9486\n",
      "Epoch 177/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0806 - accuracy: 0.9718 - val_loss: 0.1343 - val_accuracy: 0.9603\n",
      "Epoch 178/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0858 - accuracy: 0.9698 - val_loss: 0.1349 - val_accuracy: 0.9550\n",
      "Epoch 179/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0812 - accuracy: 0.9711 - val_loss: 0.1278 - val_accuracy: 0.9575\n",
      "Epoch 180/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0826 - accuracy: 0.9702 - val_loss: 0.1332 - val_accuracy: 0.9581\n",
      "Epoch 181/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0833 - accuracy: 0.9694 - val_loss: 0.1255 - val_accuracy: 0.9615\n",
      "Epoch 182/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0766 - accuracy: 0.9739 - val_loss: 0.1340 - val_accuracy: 0.9570\n",
      "Epoch 183/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0844 - accuracy: 0.9686 - val_loss: 0.1292 - val_accuracy: 0.9584\n",
      "Epoch 184/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0827 - accuracy: 0.9720 - val_loss: 0.1513 - val_accuracy: 0.9475\n",
      "Epoch 185/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0829 - accuracy: 0.9706 - val_loss: 0.1256 - val_accuracy: 0.9612\n",
      "Epoch 186/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0784 - accuracy: 0.9715 - val_loss: 0.1244 - val_accuracy: 0.9617\n",
      "Epoch 187/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0840 - accuracy: 0.9721 - val_loss: 0.1320 - val_accuracy: 0.9561\n",
      "Epoch 188/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0835 - accuracy: 0.9723 - val_loss: 0.1434 - val_accuracy: 0.9478\n",
      "Epoch 189/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0845 - accuracy: 0.9713 - val_loss: 0.1284 - val_accuracy: 0.9592\n",
      "Epoch 190/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0810 - accuracy: 0.9732 - val_loss: 0.1355 - val_accuracy: 0.9570\n",
      "Epoch 191/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0776 - accuracy: 0.9721 - val_loss: 0.1069 - val_accuracy: 0.9709\n",
      "Epoch 192/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0814 - accuracy: 0.9698 - val_loss: 0.1270 - val_accuracy: 0.9626\n",
      "Epoch 193/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0768 - accuracy: 0.9734 - val_loss: 0.1275 - val_accuracy: 0.9589\n",
      "Epoch 194/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0753 - accuracy: 0.9732 - val_loss: 0.1338 - val_accuracy: 0.9570\n",
      "Epoch 195/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0821 - accuracy: 0.9711 - val_loss: 0.1345 - val_accuracy: 0.9595\n",
      "Epoch 196/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0743 - accuracy: 0.9730 - val_loss: 0.1288 - val_accuracy: 0.9601\n",
      "Epoch 197/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0835 - accuracy: 0.9679 - val_loss: 0.1377 - val_accuracy: 0.9578\n",
      "Epoch 198/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0829 - accuracy: 0.9712 - val_loss: 0.1373 - val_accuracy: 0.9517\n",
      "Epoch 199/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0868 - accuracy: 0.9700 - val_loss: 0.1485 - val_accuracy: 0.9514\n",
      "Epoch 200/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0818 - accuracy: 0.9721 - val_loss: 0.1342 - val_accuracy: 0.9550\n",
      "Epoch 201/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0739 - accuracy: 0.9730 - val_loss: 0.1223 - val_accuracy: 0.9634\n",
      "Epoch 202/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0779 - accuracy: 0.9729 - val_loss: 0.1376 - val_accuracy: 0.9559\n",
      "Epoch 203/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0772 - accuracy: 0.9726 - val_loss: 0.1372 - val_accuracy: 0.9556\n",
      "Epoch 204/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0802 - accuracy: 0.9714 - val_loss: 0.1335 - val_accuracy: 0.9584\n",
      "Epoch 205/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0756 - accuracy: 0.9746 - val_loss: 0.1534 - val_accuracy: 0.9455\n",
      "Epoch 206/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0765 - accuracy: 0.9726 - val_loss: 0.1339 - val_accuracy: 0.9550\n",
      "Epoch 207/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0834 - accuracy: 0.9707 - val_loss: 0.1397 - val_accuracy: 0.9550\n",
      "Epoch 208/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0721 - accuracy: 0.9741 - val_loss: 0.1210 - val_accuracy: 0.9631\n",
      "Epoch 209/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0753 - accuracy: 0.9721 - val_loss: 0.1499 - val_accuracy: 0.9511\n",
      "Epoch 210/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0803 - accuracy: 0.9719 - val_loss: 0.1400 - val_accuracy: 0.9522\n",
      "Epoch 211/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0756 - accuracy: 0.9715 - val_loss: 0.1331 - val_accuracy: 0.9578\n",
      "Epoch 212/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0784 - accuracy: 0.9739 - val_loss: 0.1206 - val_accuracy: 0.9615\n",
      "Epoch 213/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0677 - accuracy: 0.9752 - val_loss: 0.1370 - val_accuracy: 0.9581\n",
      "Epoch 214/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0822 - accuracy: 0.9692 - val_loss: 0.1345 - val_accuracy: 0.9575\n",
      "Epoch 215/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0723 - accuracy: 0.9735 - val_loss: 0.1262 - val_accuracy: 0.9615\n",
      "Epoch 216/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0736 - accuracy: 0.9744 - val_loss: 0.1243 - val_accuracy: 0.9637\n",
      "Epoch 217/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0694 - accuracy: 0.9745 - val_loss: 0.1445 - val_accuracy: 0.9483\n",
      "Epoch 218/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0782 - accuracy: 0.9726 - val_loss: 0.1439 - val_accuracy: 0.9506\n",
      "Epoch 219/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0760 - accuracy: 0.9702 - val_loss: 0.1360 - val_accuracy: 0.9550\n",
      "Epoch 220/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0735 - accuracy: 0.9726 - val_loss: 0.1437 - val_accuracy: 0.9500\n",
      "Epoch 221/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0730 - accuracy: 0.9737 - val_loss: 0.1290 - val_accuracy: 0.9595\n",
      "Epoch 222/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0756 - accuracy: 0.9722 - val_loss: 0.1236 - val_accuracy: 0.9609\n",
      "Epoch 223/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0783 - accuracy: 0.9707 - val_loss: 0.1387 - val_accuracy: 0.9561\n",
      "Epoch 224/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0736 - accuracy: 0.9737 - val_loss: 0.1274 - val_accuracy: 0.9592\n",
      "Epoch 225/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0760 - accuracy: 0.9737 - val_loss: 0.1393 - val_accuracy: 0.9592\n",
      "Epoch 226/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0780 - accuracy: 0.9724 - val_loss: 0.1366 - val_accuracy: 0.9564\n",
      "Epoch 227/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0699 - accuracy: 0.9749 - val_loss: 0.1305 - val_accuracy: 0.9589\n",
      "Epoch 228/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0785 - accuracy: 0.9707 - val_loss: 0.1323 - val_accuracy: 0.9589\n",
      "Epoch 229/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0731 - accuracy: 0.9736 - val_loss: 0.1324 - val_accuracy: 0.9584\n",
      "Epoch 230/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0695 - accuracy: 0.9735 - val_loss: 0.1173 - val_accuracy: 0.9656\n",
      "Epoch 231/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0777 - accuracy: 0.9733 - val_loss: 0.1461 - val_accuracy: 0.9511\n",
      "Epoch 232/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0712 - accuracy: 0.9729 - val_loss: 0.1277 - val_accuracy: 0.9642\n",
      "Epoch 233/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0651 - accuracy: 0.9740 - val_loss: 0.1219 - val_accuracy: 0.9623\n",
      "Epoch 234/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0782 - accuracy: 0.9716 - val_loss: 0.1346 - val_accuracy: 0.9592\n",
      "Epoch 235/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0746 - accuracy: 0.9738 - val_loss: 0.1199 - val_accuracy: 0.9620\n",
      "Epoch 236/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0722 - accuracy: 0.9726 - val_loss: 0.1278 - val_accuracy: 0.9592\n",
      "Epoch 237/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0725 - accuracy: 0.9738 - val_loss: 0.1274 - val_accuracy: 0.9606\n",
      "Epoch 238/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0742 - accuracy: 0.9733 - val_loss: 0.1257 - val_accuracy: 0.9612\n",
      "Epoch 239/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0686 - accuracy: 0.9758 - val_loss: 0.1316 - val_accuracy: 0.9589\n",
      "Epoch 240/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0674 - accuracy: 0.9749 - val_loss: 0.1241 - val_accuracy: 0.9634\n",
      "Epoch 241/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0727 - accuracy: 0.9733 - val_loss: 0.1362 - val_accuracy: 0.9595\n",
      "Epoch 242/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0747 - accuracy: 0.9725 - val_loss: 0.1215 - val_accuracy: 0.9648\n",
      "Epoch 243/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0685 - accuracy: 0.9741 - val_loss: 0.1335 - val_accuracy: 0.9559\n",
      "Epoch 244/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0738 - accuracy: 0.9750 - val_loss: 0.1279 - val_accuracy: 0.9601\n",
      "Epoch 245/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0808 - accuracy: 0.9719 - val_loss: 0.1334 - val_accuracy: 0.9581\n",
      "Epoch 246/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0654 - accuracy: 0.9753 - val_loss: 0.1310 - val_accuracy: 0.9634\n",
      "Epoch 247/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0662 - accuracy: 0.9743 - val_loss: 0.1202 - val_accuracy: 0.9637\n",
      "Epoch 248/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0710 - accuracy: 0.9742 - val_loss: 0.1375 - val_accuracy: 0.9528\n",
      "Epoch 249/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0699 - accuracy: 0.9728 - val_loss: 0.1351 - val_accuracy: 0.9573\n",
      "Epoch 250/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0678 - accuracy: 0.9751 - val_loss: 0.1305 - val_accuracy: 0.9595\n",
      "Epoch 251/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0684 - accuracy: 0.9749 - val_loss: 0.1281 - val_accuracy: 0.9631\n",
      "Epoch 252/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0709 - accuracy: 0.9731 - val_loss: 0.1273 - val_accuracy: 0.9609\n",
      "Epoch 253/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0698 - accuracy: 0.9742 - val_loss: 0.1389 - val_accuracy: 0.9561\n",
      "Epoch 254/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0656 - accuracy: 0.9757 - val_loss: 0.1431 - val_accuracy: 0.9514\n",
      "Epoch 255/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0671 - accuracy: 0.9746 - val_loss: 0.1344 - val_accuracy: 0.9592\n",
      "Epoch 256/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0691 - accuracy: 0.9733 - val_loss: 0.1323 - val_accuracy: 0.9631\n",
      "Epoch 257/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0714 - accuracy: 0.9728 - val_loss: 0.1296 - val_accuracy: 0.9601\n",
      "Epoch 258/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0673 - accuracy: 0.9745 - val_loss: 0.1215 - val_accuracy: 0.9640\n",
      "Epoch 259/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0660 - accuracy: 0.9764 - val_loss: 0.1279 - val_accuracy: 0.9595\n",
      "Epoch 260/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0719 - accuracy: 0.9738 - val_loss: 0.1308 - val_accuracy: 0.9587\n",
      "Epoch 261/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0733 - accuracy: 0.9710 - val_loss: 0.1368 - val_accuracy: 0.9556\n",
      "Epoch 262/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0631 - accuracy: 0.9765 - val_loss: 0.1310 - val_accuracy: 0.9603\n",
      "Epoch 263/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0654 - accuracy: 0.9746 - val_loss: 0.1295 - val_accuracy: 0.9617\n",
      "Epoch 264/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0711 - accuracy: 0.9723 - val_loss: 0.1352 - val_accuracy: 0.9601\n",
      "Epoch 265/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0652 - accuracy: 0.9739 - val_loss: 0.1242 - val_accuracy: 0.9637\n",
      "Epoch 266/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0673 - accuracy: 0.9751 - val_loss: 0.1437 - val_accuracy: 0.9584\n",
      "Epoch 267/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0638 - accuracy: 0.9754 - val_loss: 0.1275 - val_accuracy: 0.9645\n",
      "Epoch 268/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0708 - accuracy: 0.9760 - val_loss: 0.1339 - val_accuracy: 0.9620\n",
      "Epoch 269/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0620 - accuracy: 0.9754 - val_loss: 0.1505 - val_accuracy: 0.9500\n",
      "Epoch 270/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0690 - accuracy: 0.9740 - val_loss: 0.1390 - val_accuracy: 0.9547\n",
      "Epoch 271/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0650 - accuracy: 0.9743 - val_loss: 0.1328 - val_accuracy: 0.9587\n",
      "Epoch 272/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0738 - accuracy: 0.9762 - val_loss: 0.1347 - val_accuracy: 0.9587\n",
      "Epoch 273/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0641 - accuracy: 0.9764 - val_loss: 0.1308 - val_accuracy: 0.9570\n",
      "Epoch 274/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0681 - accuracy: 0.9756 - val_loss: 0.1327 - val_accuracy: 0.9559\n",
      "Epoch 275/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0692 - accuracy: 0.9732 - val_loss: 0.1310 - val_accuracy: 0.9642\n",
      "Epoch 276/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0656 - accuracy: 0.9760 - val_loss: 0.1319 - val_accuracy: 0.9612\n",
      "Epoch 277/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0653 - accuracy: 0.9744 - val_loss: 0.1346 - val_accuracy: 0.9612\n",
      "Epoch 278/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0652 - accuracy: 0.9765 - val_loss: 0.1469 - val_accuracy: 0.9522\n",
      "Epoch 279/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0645 - accuracy: 0.9765 - val_loss: 0.1406 - val_accuracy: 0.9578\n",
      "Epoch 280/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0663 - accuracy: 0.9759 - val_loss: 0.1335 - val_accuracy: 0.9609\n",
      "Epoch 281/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0633 - accuracy: 0.9765 - val_loss: 0.1341 - val_accuracy: 0.9603\n",
      "Epoch 282/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0630 - accuracy: 0.9764 - val_loss: 0.1295 - val_accuracy: 0.9615\n",
      "Epoch 283/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0659 - accuracy: 0.9769 - val_loss: 0.1522 - val_accuracy: 0.9480\n",
      "Epoch 284/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0620 - accuracy: 0.9750 - val_loss: 0.1409 - val_accuracy: 0.9584\n",
      "Epoch 285/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0671 - accuracy: 0.9744 - val_loss: 0.1523 - val_accuracy: 0.9522\n",
      "Epoch 286/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0653 - accuracy: 0.9765 - val_loss: 0.1250 - val_accuracy: 0.9679\n",
      "Epoch 287/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0665 - accuracy: 0.9739 - val_loss: 0.1380 - val_accuracy: 0.9601\n",
      "Epoch 288/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0600 - accuracy: 0.9779 - val_loss: 0.1470 - val_accuracy: 0.9545\n",
      "Epoch 289/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0640 - accuracy: 0.9744 - val_loss: 0.1309 - val_accuracy: 0.9628\n",
      "Epoch 290/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0691 - accuracy: 0.9755 - val_loss: 0.1348 - val_accuracy: 0.9615\n",
      "Epoch 291/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0688 - accuracy: 0.9744 - val_loss: 0.1413 - val_accuracy: 0.9573\n",
      "Epoch 292/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0637 - accuracy: 0.9761 - val_loss: 0.1444 - val_accuracy: 0.9570\n",
      "Epoch 293/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0562 - accuracy: 0.9767 - val_loss: 0.1287 - val_accuracy: 0.9631\n",
      "Epoch 294/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0625 - accuracy: 0.9774 - val_loss: 0.1249 - val_accuracy: 0.9648\n",
      "Epoch 295/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0627 - accuracy: 0.9746 - val_loss: 0.1335 - val_accuracy: 0.9620\n",
      "Epoch 296/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0609 - accuracy: 0.9760 - val_loss: 0.1488 - val_accuracy: 0.9539\n",
      "Epoch 297/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0644 - accuracy: 0.9758 - val_loss: 0.1309 - val_accuracy: 0.9637\n",
      "Epoch 298/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0652 - accuracy: 0.9744 - val_loss: 0.1378 - val_accuracy: 0.9570\n",
      "Epoch 299/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0596 - accuracy: 0.9767 - val_loss: 0.1353 - val_accuracy: 0.9634\n",
      "Epoch 300/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0634 - accuracy: 0.9750 - val_loss: 0.1387 - val_accuracy: 0.9584\n",
      "Epoch 301/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0614 - accuracy: 0.9748 - val_loss: 0.1303 - val_accuracy: 0.9654\n",
      "Epoch 302/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0670 - accuracy: 0.9749 - val_loss: 0.1384 - val_accuracy: 0.9612\n",
      "Epoch 303/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0666 - accuracy: 0.9752 - val_loss: 0.1349 - val_accuracy: 0.9575\n",
      "Epoch 304/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0615 - accuracy: 0.9758 - val_loss: 0.1264 - val_accuracy: 0.9651\n",
      "Epoch 305/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0664 - accuracy: 0.9760 - val_loss: 0.1320 - val_accuracy: 0.9615\n",
      "Epoch 306/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0634 - accuracy: 0.9743 - val_loss: 0.1347 - val_accuracy: 0.9598\n",
      "Epoch 307/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0656 - accuracy: 0.9765 - val_loss: 0.1404 - val_accuracy: 0.9606\n",
      "Epoch 308/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0608 - accuracy: 0.9768 - val_loss: 0.1256 - val_accuracy: 0.9640\n",
      "Epoch 309/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0586 - accuracy: 0.9767 - val_loss: 0.1294 - val_accuracy: 0.9626\n",
      "Epoch 310/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0589 - accuracy: 0.9798 - val_loss: 0.1405 - val_accuracy: 0.9589\n",
      "Epoch 311/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0589 - accuracy: 0.9767 - val_loss: 0.1263 - val_accuracy: 0.9654\n",
      "Epoch 312/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0598 - accuracy: 0.9765 - val_loss: 0.1427 - val_accuracy: 0.9603\n",
      "Epoch 313/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0592 - accuracy: 0.9772 - val_loss: 0.1368 - val_accuracy: 0.9584\n",
      "Epoch 314/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0621 - accuracy: 0.9758 - val_loss: 0.1288 - val_accuracy: 0.9620\n",
      "Epoch 315/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0596 - accuracy: 0.9781 - val_loss: 0.1325 - val_accuracy: 0.9617\n",
      "Epoch 316/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0629 - accuracy: 0.9760 - val_loss: 0.1445 - val_accuracy: 0.9545\n",
      "Epoch 317/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0624 - accuracy: 0.9765 - val_loss: 0.1262 - val_accuracy: 0.9634\n",
      "Epoch 318/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0644 - accuracy: 0.9768 - val_loss: 0.1368 - val_accuracy: 0.9592\n",
      "Epoch 319/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0656 - accuracy: 0.9770 - val_loss: 0.1327 - val_accuracy: 0.9631\n",
      "Epoch 320/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0635 - accuracy: 0.9767 - val_loss: 0.1522 - val_accuracy: 0.9528\n",
      "Epoch 321/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0617 - accuracy: 0.9753 - val_loss: 0.1397 - val_accuracy: 0.9603\n",
      "Epoch 322/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0544 - accuracy: 0.9776 - val_loss: 0.1591 - val_accuracy: 0.9517\n",
      "Epoch 323/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0590 - accuracy: 0.9783 - val_loss: 0.1283 - val_accuracy: 0.9634\n",
      "Epoch 324/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0643 - accuracy: 0.9753 - val_loss: 0.1349 - val_accuracy: 0.9615\n",
      "Epoch 325/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0654 - accuracy: 0.9757 - val_loss: 0.1244 - val_accuracy: 0.9656\n",
      "Epoch 326/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0611 - accuracy: 0.9768 - val_loss: 0.1343 - val_accuracy: 0.9637\n",
      "Epoch 327/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0623 - accuracy: 0.9774 - val_loss: 0.1379 - val_accuracy: 0.9640\n",
      "Epoch 328/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0621 - accuracy: 0.9775 - val_loss: 0.1325 - val_accuracy: 0.9606\n",
      "Epoch 329/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0575 - accuracy: 0.9771 - val_loss: 0.1487 - val_accuracy: 0.9553\n",
      "Epoch 330/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0678 - accuracy: 0.9746 - val_loss: 0.1342 - val_accuracy: 0.9623\n",
      "Epoch 331/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0543 - accuracy: 0.9786 - val_loss: 0.1334 - val_accuracy: 0.9634\n",
      "Epoch 332/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0588 - accuracy: 0.9774 - val_loss: 0.1338 - val_accuracy: 0.9606\n",
      "Epoch 333/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0639 - accuracy: 0.9750 - val_loss: 0.1296 - val_accuracy: 0.9603\n",
      "Epoch 334/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0577 - accuracy: 0.9787 - val_loss: 0.1474 - val_accuracy: 0.9581\n",
      "Epoch 335/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0585 - accuracy: 0.9772 - val_loss: 0.1301 - val_accuracy: 0.9642\n",
      "Epoch 336/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0595 - accuracy: 0.9786 - val_loss: 0.1261 - val_accuracy: 0.9634\n",
      "Epoch 337/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0603 - accuracy: 0.9754 - val_loss: 0.1356 - val_accuracy: 0.9623\n",
      "Epoch 338/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0564 - accuracy: 0.9779 - val_loss: 0.1373 - val_accuracy: 0.9609\n",
      "Epoch 339/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0600 - accuracy: 0.9768 - val_loss: 0.1368 - val_accuracy: 0.9626\n",
      "Epoch 340/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0556 - accuracy: 0.9800 - val_loss: 0.1357 - val_accuracy: 0.9628\n",
      "Epoch 341/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0575 - accuracy: 0.9794 - val_loss: 0.1310 - val_accuracy: 0.9623\n",
      "Epoch 342/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0568 - accuracy: 0.9770 - val_loss: 0.1462 - val_accuracy: 0.9573\n",
      "Epoch 343/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0602 - accuracy: 0.9775 - val_loss: 0.1401 - val_accuracy: 0.9631\n",
      "Epoch 344/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0539 - accuracy: 0.9799 - val_loss: 0.1336 - val_accuracy: 0.9640\n",
      "Epoch 345/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0619 - accuracy: 0.9765 - val_loss: 0.1334 - val_accuracy: 0.9634\n",
      "Epoch 346/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0598 - accuracy: 0.9784 - val_loss: 0.1417 - val_accuracy: 0.9573\n",
      "Epoch 347/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0580 - accuracy: 0.9773 - val_loss: 0.1372 - val_accuracy: 0.9656\n",
      "Epoch 348/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0542 - accuracy: 0.9791 - val_loss: 0.1373 - val_accuracy: 0.9598\n",
      "Epoch 349/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0656 - accuracy: 0.9777 - val_loss: 0.1442 - val_accuracy: 0.9603\n",
      "Epoch 350/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0572 - accuracy: 0.9754 - val_loss: 0.1376 - val_accuracy: 0.9648\n",
      "Epoch 351/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0522 - accuracy: 0.9798 - val_loss: 0.1613 - val_accuracy: 0.9545\n",
      "Epoch 352/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0555 - accuracy: 0.9778 - val_loss: 0.1356 - val_accuracy: 0.9656\n",
      "Epoch 353/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0580 - accuracy: 0.9793 - val_loss: 0.1386 - val_accuracy: 0.9637\n",
      "Epoch 354/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0561 - accuracy: 0.9779 - val_loss: 0.1460 - val_accuracy: 0.9634\n",
      "Epoch 355/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0604 - accuracy: 0.9772 - val_loss: 0.1332 - val_accuracy: 0.9642\n",
      "Epoch 356/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0588 - accuracy: 0.9767 - val_loss: 0.1516 - val_accuracy: 0.9609\n",
      "Epoch 357/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0600 - accuracy: 0.9768 - val_loss: 0.1441 - val_accuracy: 0.9584\n",
      "Epoch 358/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0566 - accuracy: 0.9787 - val_loss: 0.1443 - val_accuracy: 0.9578\n",
      "Epoch 359/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0583 - accuracy: 0.9770 - val_loss: 0.1431 - val_accuracy: 0.9570\n",
      "Epoch 360/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0573 - accuracy: 0.9773 - val_loss: 0.1394 - val_accuracy: 0.9601\n",
      "Epoch 361/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0516 - accuracy: 0.9800 - val_loss: 0.1359 - val_accuracy: 0.9617\n",
      "Epoch 362/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0541 - accuracy: 0.9788 - val_loss: 0.1346 - val_accuracy: 0.9670\n",
      "Epoch 363/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0566 - accuracy: 0.9783 - val_loss: 0.1442 - val_accuracy: 0.9587\n",
      "Epoch 364/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0601 - accuracy: 0.9763 - val_loss: 0.1445 - val_accuracy: 0.9587\n",
      "Epoch 365/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0519 - accuracy: 0.9800 - val_loss: 0.1296 - val_accuracy: 0.9645\n",
      "Epoch 366/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0590 - accuracy: 0.9783 - val_loss: 0.1344 - val_accuracy: 0.9673\n",
      "Epoch 367/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0601 - accuracy: 0.9786 - val_loss: 0.1523 - val_accuracy: 0.9536\n",
      "Epoch 368/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0577 - accuracy: 0.9775 - val_loss: 0.1373 - val_accuracy: 0.9679\n",
      "Epoch 369/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0585 - accuracy: 0.9776 - val_loss: 0.1457 - val_accuracy: 0.9592\n",
      "Epoch 370/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0518 - accuracy: 0.9800 - val_loss: 0.1309 - val_accuracy: 0.9654\n",
      "Epoch 371/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0547 - accuracy: 0.9784 - val_loss: 0.1631 - val_accuracy: 0.9466\n",
      "Epoch 372/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0551 - accuracy: 0.9777 - val_loss: 0.1427 - val_accuracy: 0.9620\n",
      "Epoch 373/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0557 - accuracy: 0.9786 - val_loss: 0.1259 - val_accuracy: 0.9628\n",
      "Epoch 374/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0520 - accuracy: 0.9800 - val_loss: 0.1347 - val_accuracy: 0.9662\n",
      "Epoch 375/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0522 - accuracy: 0.9804 - val_loss: 0.1453 - val_accuracy: 0.9589\n",
      "Epoch 376/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0526 - accuracy: 0.9793 - val_loss: 0.1326 - val_accuracy: 0.9656\n",
      "Epoch 377/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0576 - accuracy: 0.9780 - val_loss: 0.1420 - val_accuracy: 0.9626\n",
      "Epoch 378/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0528 - accuracy: 0.9777 - val_loss: 0.1384 - val_accuracy: 0.9626\n",
      "Epoch 379/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0569 - accuracy: 0.9796 - val_loss: 0.1332 - val_accuracy: 0.9592\n",
      "Epoch 380/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0643 - accuracy: 0.9779 - val_loss: 0.1458 - val_accuracy: 0.9587\n",
      "Epoch 381/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0475 - accuracy: 0.9801 - val_loss: 0.1322 - val_accuracy: 0.9651\n",
      "Epoch 382/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0565 - accuracy: 0.9781 - val_loss: 0.1397 - val_accuracy: 0.9654\n",
      "Epoch 383/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0642 - accuracy: 0.9765 - val_loss: 0.1430 - val_accuracy: 0.9598\n",
      "Epoch 384/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0530 - accuracy: 0.9798 - val_loss: 0.1503 - val_accuracy: 0.9584\n",
      "Epoch 385/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0550 - accuracy: 0.9783 - val_loss: 0.1353 - val_accuracy: 0.9659\n",
      "Epoch 386/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0555 - accuracy: 0.9781 - val_loss: 0.1406 - val_accuracy: 0.9603\n",
      "Epoch 387/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0512 - accuracy: 0.9804 - val_loss: 0.1317 - val_accuracy: 0.9640\n",
      "Epoch 388/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0526 - accuracy: 0.9807 - val_loss: 0.1472 - val_accuracy: 0.9578\n",
      "Epoch 389/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0539 - accuracy: 0.9790 - val_loss: 0.1339 - val_accuracy: 0.9637\n",
      "Epoch 390/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0530 - accuracy: 0.9816 - val_loss: 0.1316 - val_accuracy: 0.9634\n",
      "Epoch 391/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0536 - accuracy: 0.9800 - val_loss: 0.1370 - val_accuracy: 0.9645\n",
      "Epoch 392/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0544 - accuracy: 0.9807 - val_loss: 0.1473 - val_accuracy: 0.9620\n",
      "Epoch 393/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0529 - accuracy: 0.9795 - val_loss: 0.1324 - val_accuracy: 0.9673\n",
      "Epoch 394/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0518 - accuracy: 0.9807 - val_loss: 0.1291 - val_accuracy: 0.9662\n",
      "Epoch 395/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0494 - accuracy: 0.9818 - val_loss: 0.1457 - val_accuracy: 0.9628\n",
      "Epoch 396/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0560 - accuracy: 0.9796 - val_loss: 0.1474 - val_accuracy: 0.9584\n",
      "Epoch 397/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0487 - accuracy: 0.9807 - val_loss: 0.1393 - val_accuracy: 0.9634\n",
      "Epoch 398/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0466 - accuracy: 0.9823 - val_loss: 0.1358 - val_accuracy: 0.9640\n",
      "Epoch 399/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0459 - accuracy: 0.9814 - val_loss: 0.1387 - val_accuracy: 0.9617\n",
      "Epoch 400/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0484 - accuracy: 0.9803 - val_loss: 0.1502 - val_accuracy: 0.9584\n",
      "Epoch 401/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0544 - accuracy: 0.9776 - val_loss: 0.1439 - val_accuracy: 0.9615\n",
      "Epoch 402/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0548 - accuracy: 0.9789 - val_loss: 0.1513 - val_accuracy: 0.9626\n",
      "Epoch 403/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0516 - accuracy: 0.9801 - val_loss: 0.1444 - val_accuracy: 0.9612\n",
      "Epoch 404/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0517 - accuracy: 0.9787 - val_loss: 0.1562 - val_accuracy: 0.9561\n",
      "Epoch 405/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0504 - accuracy: 0.9801 - val_loss: 0.1522 - val_accuracy: 0.9581\n",
      "Epoch 406/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0507 - accuracy: 0.9805 - val_loss: 0.1584 - val_accuracy: 0.9539\n",
      "Epoch 407/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0545 - accuracy: 0.9790 - val_loss: 0.1409 - val_accuracy: 0.9592\n",
      "Epoch 408/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0576 - accuracy: 0.9790 - val_loss: 0.1431 - val_accuracy: 0.9628\n",
      "Epoch 409/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0540 - accuracy: 0.9793 - val_loss: 0.1507 - val_accuracy: 0.9575\n",
      "Epoch 410/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0491 - accuracy: 0.9816 - val_loss: 0.1315 - val_accuracy: 0.9648\n",
      "Epoch 411/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0514 - accuracy: 0.9794 - val_loss: 0.1370 - val_accuracy: 0.9628\n",
      "Epoch 412/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0502 - accuracy: 0.9823 - val_loss: 0.1372 - val_accuracy: 0.9623\n",
      "Epoch 413/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0566 - accuracy: 0.9797 - val_loss: 0.1394 - val_accuracy: 0.9634\n",
      "Epoch 414/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0545 - accuracy: 0.9804 - val_loss: 0.1494 - val_accuracy: 0.9573\n",
      "Epoch 415/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0552 - accuracy: 0.9803 - val_loss: 0.1475 - val_accuracy: 0.9592\n",
      "Epoch 416/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0503 - accuracy: 0.9800 - val_loss: 0.1335 - val_accuracy: 0.9628\n",
      "Epoch 417/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0524 - accuracy: 0.9792 - val_loss: 0.1422 - val_accuracy: 0.9626\n",
      "Epoch 418/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0515 - accuracy: 0.9800 - val_loss: 0.1469 - val_accuracy: 0.9589\n",
      "Epoch 419/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0511 - accuracy: 0.9797 - val_loss: 0.1400 - val_accuracy: 0.9615\n",
      "Epoch 420/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0525 - accuracy: 0.9806 - val_loss: 0.1379 - val_accuracy: 0.9598\n",
      "Epoch 421/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0553 - accuracy: 0.9771 - val_loss: 0.1413 - val_accuracy: 0.9623\n",
      "Epoch 422/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0508 - accuracy: 0.9803 - val_loss: 0.1566 - val_accuracy: 0.9503\n",
      "Epoch 423/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0465 - accuracy: 0.9822 - val_loss: 0.1654 - val_accuracy: 0.9570\n",
      "Epoch 424/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0574 - accuracy: 0.9783 - val_loss: 0.1510 - val_accuracy: 0.9615\n",
      "Epoch 425/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0508 - accuracy: 0.9795 - val_loss: 0.1451 - val_accuracy: 0.9659\n",
      "Epoch 426/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0502 - accuracy: 0.9797 - val_loss: 0.1415 - val_accuracy: 0.9612\n",
      "Epoch 427/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0409 - accuracy: 0.9823 - val_loss: 0.1305 - val_accuracy: 0.9687\n",
      "Epoch 428/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0476 - accuracy: 0.9821 - val_loss: 0.1300 - val_accuracy: 0.9645\n",
      "Epoch 429/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0469 - accuracy: 0.9818 - val_loss: 0.1480 - val_accuracy: 0.9603\n",
      "Epoch 430/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0560 - accuracy: 0.9784 - val_loss: 0.1408 - val_accuracy: 0.9598\n",
      "Epoch 431/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0454 - accuracy: 0.9816 - val_loss: 0.1383 - val_accuracy: 0.9684\n",
      "Epoch 432/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0515 - accuracy: 0.9793 - val_loss: 0.1448 - val_accuracy: 0.9640\n",
      "Epoch 433/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0494 - accuracy: 0.9818 - val_loss: 0.1463 - val_accuracy: 0.9609\n",
      "Epoch 434/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0483 - accuracy: 0.9809 - val_loss: 0.1312 - val_accuracy: 0.9670\n",
      "Epoch 435/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0530 - accuracy: 0.9797 - val_loss: 0.1304 - val_accuracy: 0.9662\n",
      "Epoch 436/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0537 - accuracy: 0.9794 - val_loss: 0.1451 - val_accuracy: 0.9648\n",
      "Epoch 437/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0519 - accuracy: 0.9803 - val_loss: 0.1376 - val_accuracy: 0.9654\n",
      "Epoch 438/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0515 - accuracy: 0.9802 - val_loss: 0.1400 - val_accuracy: 0.9620\n",
      "Epoch 439/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0437 - accuracy: 0.9834 - val_loss: 0.1402 - val_accuracy: 0.9637\n",
      "Epoch 440/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0522 - accuracy: 0.9809 - val_loss: 0.1325 - val_accuracy: 0.9668\n",
      "Epoch 441/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0464 - accuracy: 0.9823 - val_loss: 0.1399 - val_accuracy: 0.9634\n",
      "Epoch 442/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0485 - accuracy: 0.9804 - val_loss: 0.1316 - val_accuracy: 0.9696\n",
      "Epoch 443/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0514 - accuracy: 0.9809 - val_loss: 0.1466 - val_accuracy: 0.9609\n",
      "Epoch 444/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0472 - accuracy: 0.9820 - val_loss: 0.1370 - val_accuracy: 0.9640\n",
      "Epoch 445/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0609 - accuracy: 0.9779 - val_loss: 0.1455 - val_accuracy: 0.9612\n",
      "Epoch 446/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0479 - accuracy: 0.9799 - val_loss: 0.1246 - val_accuracy: 0.9668\n",
      "Epoch 447/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0470 - accuracy: 0.9818 - val_loss: 0.1359 - val_accuracy: 0.9651\n",
      "Epoch 448/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0548 - accuracy: 0.9797 - val_loss: 0.1411 - val_accuracy: 0.9626\n",
      "Epoch 449/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0488 - accuracy: 0.9811 - val_loss: 0.1427 - val_accuracy: 0.9628\n",
      "Epoch 450/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0454 - accuracy: 0.9808 - val_loss: 0.1439 - val_accuracy: 0.9620\n",
      "Epoch 451/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0499 - accuracy: 0.9805 - val_loss: 0.1380 - val_accuracy: 0.9665\n",
      "Epoch 452/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0456 - accuracy: 0.9812 - val_loss: 0.1509 - val_accuracy: 0.9626\n",
      "Epoch 453/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0498 - accuracy: 0.9827 - val_loss: 0.1422 - val_accuracy: 0.9640\n",
      "Epoch 454/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0546 - accuracy: 0.9803 - val_loss: 0.1368 - val_accuracy: 0.9637\n",
      "Epoch 455/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0494 - accuracy: 0.9806 - val_loss: 0.1407 - val_accuracy: 0.9656\n",
      "Epoch 456/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0457 - accuracy: 0.9823 - val_loss: 0.1408 - val_accuracy: 0.9640\n",
      "Epoch 457/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0450 - accuracy: 0.9825 - val_loss: 0.1471 - val_accuracy: 0.9634\n",
      "Epoch 458/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0533 - accuracy: 0.9810 - val_loss: 0.1508 - val_accuracy: 0.9601\n",
      "Epoch 459/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0505 - accuracy: 0.9790 - val_loss: 0.1787 - val_accuracy: 0.9483\n",
      "Epoch 460/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0469 - accuracy: 0.9825 - val_loss: 0.1440 - val_accuracy: 0.9628\n",
      "Epoch 461/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0470 - accuracy: 0.9816 - val_loss: 0.1519 - val_accuracy: 0.9570\n",
      "Epoch 462/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0466 - accuracy: 0.9814 - val_loss: 0.1468 - val_accuracy: 0.9620\n",
      "Epoch 463/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0468 - accuracy: 0.9814 - val_loss: 0.1481 - val_accuracy: 0.9620\n",
      "Epoch 464/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0461 - accuracy: 0.9822 - val_loss: 0.1417 - val_accuracy: 0.9642\n",
      "Epoch 465/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0535 - accuracy: 0.9807 - val_loss: 0.1496 - val_accuracy: 0.9609\n",
      "Epoch 466/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0456 - accuracy: 0.9821 - val_loss: 0.1471 - val_accuracy: 0.9626\n",
      "Epoch 467/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0441 - accuracy: 0.9830 - val_loss: 0.1334 - val_accuracy: 0.9656\n",
      "Epoch 468/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0426 - accuracy: 0.9841 - val_loss: 0.1436 - val_accuracy: 0.9617\n",
      "Epoch 469/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0367 - accuracy: 0.9848 - val_loss: 0.1359 - val_accuracy: 0.9654\n",
      "Epoch 470/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0383 - accuracy: 0.9853 - val_loss: 0.1379 - val_accuracy: 0.9642\n",
      "Epoch 471/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0369 - accuracy: 0.9851 - val_loss: 0.1378 - val_accuracy: 0.9645\n",
      "Epoch 472/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0350 - accuracy: 0.9855 - val_loss: 0.1369 - val_accuracy: 0.9651\n",
      "Epoch 473/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0382 - accuracy: 0.9843 - val_loss: 0.1371 - val_accuracy: 0.9642\n",
      "Epoch 474/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0350 - accuracy: 0.9860 - val_loss: 0.1356 - val_accuracy: 0.9637\n",
      "Epoch 475/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0344 - accuracy: 0.9871 - val_loss: 0.1347 - val_accuracy: 0.9640\n",
      "Epoch 476/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0353 - accuracy: 0.9865 - val_loss: 0.1383 - val_accuracy: 0.9645\n",
      "Epoch 477/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0346 - accuracy: 0.9855 - val_loss: 0.1424 - val_accuracy: 0.9623\n",
      "Epoch 478/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0300 - accuracy: 0.9878 - val_loss: 0.1401 - val_accuracy: 0.9642\n",
      "Epoch 479/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0332 - accuracy: 0.9857 - val_loss: 0.1427 - val_accuracy: 0.9645\n",
      "Epoch 480/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0322 - accuracy: 0.9871 - val_loss: 0.1396 - val_accuracy: 0.9659\n",
      "Epoch 481/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0338 - accuracy: 0.9862 - val_loss: 0.1428 - val_accuracy: 0.9631\n",
      "Epoch 482/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0358 - accuracy: 0.9858 - val_loss: 0.1412 - val_accuracy: 0.9640\n",
      "Epoch 483/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0343 - accuracy: 0.9860 - val_loss: 0.1423 - val_accuracy: 0.9637\n",
      "Epoch 484/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0332 - accuracy: 0.9864 - val_loss: 0.1435 - val_accuracy: 0.9640\n",
      "Epoch 485/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0305 - accuracy: 0.9886 - val_loss: 0.1379 - val_accuracy: 0.9670\n",
      "Epoch 486/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0321 - accuracy: 0.9871 - val_loss: 0.1381 - val_accuracy: 0.9648\n",
      "Epoch 487/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0337 - accuracy: 0.9860 - val_loss: 0.1448 - val_accuracy: 0.9651\n",
      "Epoch 488/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0349 - accuracy: 0.9871 - val_loss: 0.1429 - val_accuracy: 0.9648\n",
      "Epoch 489/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0326 - accuracy: 0.9867 - val_loss: 0.1413 - val_accuracy: 0.9648\n",
      "Epoch 490/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0326 - accuracy: 0.9871 - val_loss: 0.1447 - val_accuracy: 0.9654\n",
      "Epoch 491/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0314 - accuracy: 0.9873 - val_loss: 0.1432 - val_accuracy: 0.9659\n",
      "Epoch 492/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0354 - accuracy: 0.9867 - val_loss: 0.1452 - val_accuracy: 0.9642\n",
      "Epoch 493/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0304 - accuracy: 0.9859 - val_loss: 0.1427 - val_accuracy: 0.9654\n",
      "Epoch 494/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0333 - accuracy: 0.9862 - val_loss: 0.1477 - val_accuracy: 0.9623\n",
      "Epoch 495/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0316 - accuracy: 0.9873 - val_loss: 0.1456 - val_accuracy: 0.9645\n",
      "Epoch 496/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0363 - accuracy: 0.9875 - val_loss: 0.1505 - val_accuracy: 0.9615\n",
      "Epoch 497/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0331 - accuracy: 0.9863 - val_loss: 0.1442 - val_accuracy: 0.9634\n",
      "Epoch 498/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0304 - accuracy: 0.9874 - val_loss: 0.1410 - val_accuracy: 0.9659\n",
      "Epoch 499/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0360 - accuracy: 0.9876 - val_loss: 0.1404 - val_accuracy: 0.9648\n",
      "Epoch 500/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0321 - accuracy: 0.9870 - val_loss: 0.1422 - val_accuracy: 0.9640\n",
      "Accuracy: 0.9639664888381958\n",
      " MSE: 0.14220590889453888\n"
     ]
    }
   ],
   "source": [
    "model2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu', input_shape=(X_trains.shape[1],)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# 클래스 가중치 계산\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adamax()\n",
    "model2.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 학습 자동 중단 설정\n",
    "es = EarlyStopping(monitor='loss', patience=50, mode='min')\n",
    "rlrp = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=40, mode='min')\n",
    "\n",
    "# 모델 학습\n",
    "model2.fit(X_trains, y_train, epochs=500, batch_size=32, verbose=1, \n",
    "          validation_data=(X_tests, y_test), callbacks=[es, rlrp], class_weight=class_weights)\n",
    "\n",
    "# 모델 평가\n",
    "y_pred = model2.predict(X_tests)\n",
    "    \n",
    "loss, accuracy = model2.evaluate(X_tests, y_test, verbose=0)\n",
    "print(f'Accuracy: {accuracy}\\n MSE: {loss}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 배치정규화, relu, adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "448/448 [==============================] - 2s 2ms/step - loss: 0.2318 - accuracy: 0.9574 - val_loss: 0.1031 - val_accuracy: 0.9788\n",
      "Epoch 2/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.1073 - accuracy: 0.9763 - val_loss: 0.0789 - val_accuracy: 0.9768\n",
      "Epoch 3/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0874 - accuracy: 0.9773 - val_loss: 0.0728 - val_accuracy: 0.9779\n",
      "Epoch 4/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0750 - accuracy: 0.9793 - val_loss: 0.0732 - val_accuracy: 0.9793\n",
      "Epoch 5/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0747 - accuracy: 0.9783 - val_loss: 0.0768 - val_accuracy: 0.9785\n",
      "Epoch 6/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0705 - accuracy: 0.9797 - val_loss: 0.0706 - val_accuracy: 0.9793\n",
      "Epoch 7/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0703 - accuracy: 0.9792 - val_loss: 0.0740 - val_accuracy: 0.9782\n",
      "Epoch 8/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0680 - accuracy: 0.9809 - val_loss: 0.0710 - val_accuracy: 0.9788\n",
      "Epoch 9/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0688 - accuracy: 0.9809 - val_loss: 0.0703 - val_accuracy: 0.9788\n",
      "Epoch 10/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0671 - accuracy: 0.9802 - val_loss: 0.0720 - val_accuracy: 0.9791\n",
      "Epoch 11/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0663 - accuracy: 0.9803 - val_loss: 0.0725 - val_accuracy: 0.9785\n",
      "Epoch 12/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0665 - accuracy: 0.9802 - val_loss: 0.0709 - val_accuracy: 0.9796\n",
      "Epoch 13/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0651 - accuracy: 0.9808 - val_loss: 0.0730 - val_accuracy: 0.9791\n",
      "Epoch 14/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0654 - accuracy: 0.9808 - val_loss: 0.0700 - val_accuracy: 0.9793\n",
      "Epoch 15/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0644 - accuracy: 0.9817 - val_loss: 0.0725 - val_accuracy: 0.9799\n",
      "Epoch 16/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0641 - accuracy: 0.9812 - val_loss: 0.0734 - val_accuracy: 0.9788\n",
      "Epoch 17/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0630 - accuracy: 0.9805 - val_loss: 0.0717 - val_accuracy: 0.9785\n",
      "Epoch 18/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0629 - accuracy: 0.9811 - val_loss: 0.0736 - val_accuracy: 0.9779\n",
      "Epoch 19/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0629 - accuracy: 0.9808 - val_loss: 0.0721 - val_accuracy: 0.9788\n",
      "Epoch 20/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0617 - accuracy: 0.9823 - val_loss: 0.0728 - val_accuracy: 0.9782\n",
      "Epoch 21/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0616 - accuracy: 0.9811 - val_loss: 0.0740 - val_accuracy: 0.9788\n",
      "Epoch 22/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0612 - accuracy: 0.9814 - val_loss: 0.0707 - val_accuracy: 0.9793\n",
      "Epoch 23/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0621 - accuracy: 0.9809 - val_loss: 0.0736 - val_accuracy: 0.9793\n",
      "Epoch 24/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0617 - accuracy: 0.9813 - val_loss: 0.0725 - val_accuracy: 0.9796\n",
      "Epoch 25/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0603 - accuracy: 0.9825 - val_loss: 0.0746 - val_accuracy: 0.9782\n",
      "Epoch 26/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0618 - accuracy: 0.9818 - val_loss: 0.0804 - val_accuracy: 0.9751\n",
      "Epoch 27/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0621 - accuracy: 0.9809 - val_loss: 0.0743 - val_accuracy: 0.9782\n",
      "Epoch 28/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0597 - accuracy: 0.9820 - val_loss: 0.0722 - val_accuracy: 0.9782\n",
      "Epoch 29/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0605 - accuracy: 0.9810 - val_loss: 0.0776 - val_accuracy: 0.9782\n",
      "Epoch 30/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0603 - accuracy: 0.9814 - val_loss: 0.0720 - val_accuracy: 0.9791\n",
      "Epoch 31/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0599 - accuracy: 0.9818 - val_loss: 0.0757 - val_accuracy: 0.9782\n",
      "Epoch 32/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0594 - accuracy: 0.9816 - val_loss: 0.0746 - val_accuracy: 0.9788\n",
      "Epoch 33/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0591 - accuracy: 0.9816 - val_loss: 0.0738 - val_accuracy: 0.9788\n",
      "Epoch 34/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0597 - accuracy: 0.9815 - val_loss: 0.0723 - val_accuracy: 0.9796\n",
      "Epoch 35/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0599 - accuracy: 0.9814 - val_loss: 0.0758 - val_accuracy: 0.9782\n",
      "Epoch 36/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0574 - accuracy: 0.9821 - val_loss: 0.0737 - val_accuracy: 0.9788\n",
      "Epoch 37/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0590 - accuracy: 0.9826 - val_loss: 0.0740 - val_accuracy: 0.9796\n",
      "Epoch 38/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0581 - accuracy: 0.9825 - val_loss: 0.0734 - val_accuracy: 0.9782\n",
      "Epoch 39/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0581 - accuracy: 0.9819 - val_loss: 0.0759 - val_accuracy: 0.9791\n",
      "Epoch 40/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0585 - accuracy: 0.9818 - val_loss: 0.0793 - val_accuracy: 0.9763\n",
      "Epoch 41/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0577 - accuracy: 0.9824 - val_loss: 0.0744 - val_accuracy: 0.9785\n",
      "Epoch 42/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0584 - accuracy: 0.9828 - val_loss: 0.0762 - val_accuracy: 0.9785\n",
      "Epoch 43/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0578 - accuracy: 0.9825 - val_loss: 0.0756 - val_accuracy: 0.9793\n",
      "Epoch 44/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0569 - accuracy: 0.9826 - val_loss: 0.0773 - val_accuracy: 0.9788\n",
      "Epoch 45/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0562 - accuracy: 0.9827 - val_loss: 0.0740 - val_accuracy: 0.9793\n",
      "Epoch 46/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0580 - accuracy: 0.9820 - val_loss: 0.0771 - val_accuracy: 0.9779\n",
      "Epoch 47/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0571 - accuracy: 0.9826 - val_loss: 0.0750 - val_accuracy: 0.9788\n",
      "Epoch 48/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0561 - accuracy: 0.9817 - val_loss: 0.0773 - val_accuracy: 0.9777\n",
      "Epoch 49/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0580 - accuracy: 0.9824 - val_loss: 0.0753 - val_accuracy: 0.9788\n",
      "Epoch 50/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0555 - accuracy: 0.9824 - val_loss: 0.0762 - val_accuracy: 0.9777\n",
      "Epoch 51/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0555 - accuracy: 0.9823 - val_loss: 0.0757 - val_accuracy: 0.9799\n",
      "Epoch 52/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0532 - accuracy: 0.9834 - val_loss: 0.0771 - val_accuracy: 0.9804\n",
      "Epoch 53/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0561 - accuracy: 0.9828 - val_loss: 0.0740 - val_accuracy: 0.9807\n",
      "Epoch 54/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0552 - accuracy: 0.9827 - val_loss: 0.0751 - val_accuracy: 0.9793\n",
      "Epoch 55/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0539 - accuracy: 0.9838 - val_loss: 0.0764 - val_accuracy: 0.9771\n",
      "Epoch 56/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0557 - accuracy: 0.9830 - val_loss: 0.0769 - val_accuracy: 0.9782\n",
      "Epoch 57/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0542 - accuracy: 0.9829 - val_loss: 0.0772 - val_accuracy: 0.9785\n",
      "Epoch 58/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0523 - accuracy: 0.9838 - val_loss: 0.0782 - val_accuracy: 0.9782\n",
      "Epoch 59/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0553 - accuracy: 0.9829 - val_loss: 0.0773 - val_accuracy: 0.9793\n",
      "Epoch 60/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0541 - accuracy: 0.9829 - val_loss: 0.0802 - val_accuracy: 0.9777\n",
      "Epoch 61/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0541 - accuracy: 0.9841 - val_loss: 0.0765 - val_accuracy: 0.9793\n",
      "Epoch 62/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0542 - accuracy: 0.9838 - val_loss: 0.0745 - val_accuracy: 0.9799\n",
      "Epoch 63/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0525 - accuracy: 0.9845 - val_loss: 0.0787 - val_accuracy: 0.9785\n",
      "Epoch 64/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0533 - accuracy: 0.9832 - val_loss: 0.0775 - val_accuracy: 0.9785\n",
      "Epoch 65/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0517 - accuracy: 0.9843 - val_loss: 0.0781 - val_accuracy: 0.9785\n",
      "Epoch 66/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0540 - accuracy: 0.9834 - val_loss: 0.0779 - val_accuracy: 0.9788\n",
      "Epoch 67/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0525 - accuracy: 0.9832 - val_loss: 0.0820 - val_accuracy: 0.9777\n",
      "Epoch 68/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0524 - accuracy: 0.9838 - val_loss: 0.0785 - val_accuracy: 0.9785\n",
      "Epoch 69/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0519 - accuracy: 0.9831 - val_loss: 0.0784 - val_accuracy: 0.9782\n",
      "Epoch 70/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0521 - accuracy: 0.9840 - val_loss: 0.0781 - val_accuracy: 0.9788\n",
      "Epoch 71/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0503 - accuracy: 0.9833 - val_loss: 0.0795 - val_accuracy: 0.9796\n",
      "Epoch 72/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0514 - accuracy: 0.9849 - val_loss: 0.0782 - val_accuracy: 0.9779\n",
      "Epoch 73/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0532 - accuracy: 0.9832 - val_loss: 0.0773 - val_accuracy: 0.9777\n",
      "Epoch 74/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0515 - accuracy: 0.9839 - val_loss: 0.0787 - val_accuracy: 0.9782\n",
      "Epoch 75/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0513 - accuracy: 0.9837 - val_loss: 0.0792 - val_accuracy: 0.9791\n",
      "Epoch 76/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0500 - accuracy: 0.9841 - val_loss: 0.0799 - val_accuracy: 0.9782\n",
      "Epoch 77/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0510 - accuracy: 0.9833 - val_loss: 0.0776 - val_accuracy: 0.9802\n",
      "Epoch 78/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0513 - accuracy: 0.9838 - val_loss: 0.0788 - val_accuracy: 0.9765\n",
      "Epoch 79/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0505 - accuracy: 0.9836 - val_loss: 0.0794 - val_accuracy: 0.9765\n",
      "Epoch 80/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0510 - accuracy: 0.9839 - val_loss: 0.0800 - val_accuracy: 0.9774\n",
      "Epoch 81/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0486 - accuracy: 0.9847 - val_loss: 0.0793 - val_accuracy: 0.9771\n",
      "Epoch 82/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0511 - accuracy: 0.9839 - val_loss: 0.0822 - val_accuracy: 0.9782\n",
      "Epoch 83/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0498 - accuracy: 0.9840 - val_loss: 0.0787 - val_accuracy: 0.9788\n",
      "Epoch 84/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0497 - accuracy: 0.9841 - val_loss: 0.0779 - val_accuracy: 0.9791\n",
      "Epoch 85/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0492 - accuracy: 0.9846 - val_loss: 0.0807 - val_accuracy: 0.9777\n",
      "Epoch 86/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0477 - accuracy: 0.9846 - val_loss: 0.0805 - val_accuracy: 0.9782\n",
      "Epoch 87/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0489 - accuracy: 0.9853 - val_loss: 0.0838 - val_accuracy: 0.9774\n",
      "Epoch 88/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0479 - accuracy: 0.9848 - val_loss: 0.0801 - val_accuracy: 0.9774\n",
      "Epoch 89/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0472 - accuracy: 0.9848 - val_loss: 0.0836 - val_accuracy: 0.9746\n",
      "Epoch 90/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0480 - accuracy: 0.9847 - val_loss: 0.0815 - val_accuracy: 0.9785\n",
      "Epoch 91/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0491 - accuracy: 0.9851 - val_loss: 0.0830 - val_accuracy: 0.9785\n",
      "Epoch 92/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0490 - accuracy: 0.9842 - val_loss: 0.0817 - val_accuracy: 0.9777\n",
      "Epoch 93/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0480 - accuracy: 0.9843 - val_loss: 0.0815 - val_accuracy: 0.9779\n",
      "Epoch 94/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0473 - accuracy: 0.9845 - val_loss: 0.0803 - val_accuracy: 0.9788\n",
      "Epoch 95/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0486 - accuracy: 0.9842 - val_loss: 0.0809 - val_accuracy: 0.9771\n",
      "Epoch 96/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0473 - accuracy: 0.9848 - val_loss: 0.0813 - val_accuracy: 0.9782\n",
      "Epoch 97/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0485 - accuracy: 0.9849 - val_loss: 0.0820 - val_accuracy: 0.9771\n",
      "Epoch 98/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0469 - accuracy: 0.9848 - val_loss: 0.0815 - val_accuracy: 0.9785\n",
      "Epoch 99/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0477 - accuracy: 0.9844 - val_loss: 0.0828 - val_accuracy: 0.9782\n",
      "Epoch 100/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0459 - accuracy: 0.9848 - val_loss: 0.0836 - val_accuracy: 0.9779\n",
      "Epoch 101/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0471 - accuracy: 0.9844 - val_loss: 0.0797 - val_accuracy: 0.9771\n",
      "Epoch 102/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0469 - accuracy: 0.9853 - val_loss: 0.0844 - val_accuracy: 0.9763\n",
      "Epoch 103/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0480 - accuracy: 0.9848 - val_loss: 0.0820 - val_accuracy: 0.9774\n",
      "Epoch 104/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0459 - accuracy: 0.9858 - val_loss: 0.0841 - val_accuracy: 0.9763\n",
      "Epoch 105/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0463 - accuracy: 0.9851 - val_loss: 0.0858 - val_accuracy: 0.9779\n",
      "Epoch 106/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0470 - accuracy: 0.9846 - val_loss: 0.0838 - val_accuracy: 0.9777\n",
      "Epoch 107/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0461 - accuracy: 0.9848 - val_loss: 0.0847 - val_accuracy: 0.9771\n",
      "Epoch 108/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0448 - accuracy: 0.9855 - val_loss: 0.0866 - val_accuracy: 0.9763\n",
      "Epoch 109/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0465 - accuracy: 0.9852 - val_loss: 0.0832 - val_accuracy: 0.9777\n",
      "Epoch 110/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0458 - accuracy: 0.9853 - val_loss: 0.0841 - val_accuracy: 0.9771\n",
      "Epoch 111/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0455 - accuracy: 0.9859 - val_loss: 0.0827 - val_accuracy: 0.9793\n",
      "Epoch 112/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0446 - accuracy: 0.9854 - val_loss: 0.0857 - val_accuracy: 0.9768\n",
      "Epoch 113/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0468 - accuracy: 0.9849 - val_loss: 0.0827 - val_accuracy: 0.9788\n",
      "Epoch 114/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0441 - accuracy: 0.9864 - val_loss: 0.0847 - val_accuracy: 0.9779\n",
      "Epoch 115/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0458 - accuracy: 0.9841 - val_loss: 0.0887 - val_accuracy: 0.9746\n",
      "Epoch 116/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0450 - accuracy: 0.9858 - val_loss: 0.0854 - val_accuracy: 0.9765\n",
      "Epoch 117/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0454 - accuracy: 0.9859 - val_loss: 0.0880 - val_accuracy: 0.9754\n",
      "Epoch 118/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0432 - accuracy: 0.9865 - val_loss: 0.0853 - val_accuracy: 0.9760\n",
      "Epoch 119/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0450 - accuracy: 0.9858 - val_loss: 0.0865 - val_accuracy: 0.9782\n",
      "Epoch 120/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0442 - accuracy: 0.9866 - val_loss: 0.0907 - val_accuracy: 0.9771\n",
      "Epoch 121/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0453 - accuracy: 0.9865 - val_loss: 0.0976 - val_accuracy: 0.9757\n",
      "Epoch 122/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0439 - accuracy: 0.9860 - val_loss: 0.0891 - val_accuracy: 0.9765\n",
      "Epoch 123/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0440 - accuracy: 0.9855 - val_loss: 0.0872 - val_accuracy: 0.9771\n",
      "Epoch 124/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0444 - accuracy: 0.9850 - val_loss: 0.0873 - val_accuracy: 0.9771\n",
      "Epoch 125/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0438 - accuracy: 0.9858 - val_loss: 0.0907 - val_accuracy: 0.9777\n",
      "Epoch 126/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0421 - accuracy: 0.9862 - val_loss: 0.0934 - val_accuracy: 0.9774\n",
      "Epoch 127/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0446 - accuracy: 0.9860 - val_loss: 0.0884 - val_accuracy: 0.9763\n",
      "Epoch 128/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0439 - accuracy: 0.9851 - val_loss: 0.0866 - val_accuracy: 0.9765\n",
      "Epoch 129/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0445 - accuracy: 0.9841 - val_loss: 0.0854 - val_accuracy: 0.9779\n",
      "Epoch 130/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0429 - accuracy: 0.9862 - val_loss: 0.0865 - val_accuracy: 0.9785\n",
      "Epoch 131/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0420 - accuracy: 0.9867 - val_loss: 0.0836 - val_accuracy: 0.9763\n",
      "Epoch 132/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0428 - accuracy: 0.9859 - val_loss: 0.0834 - val_accuracy: 0.9788\n",
      "Epoch 133/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0428 - accuracy: 0.9862 - val_loss: 0.0933 - val_accuracy: 0.9757\n",
      "Epoch 134/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0423 - accuracy: 0.9860 - val_loss: 0.0862 - val_accuracy: 0.9774\n",
      "Epoch 135/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0430 - accuracy: 0.9857 - val_loss: 0.0889 - val_accuracy: 0.9765\n",
      "Epoch 136/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0420 - accuracy: 0.9866 - val_loss: 0.0901 - val_accuracy: 0.9771\n",
      "Epoch 137/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0431 - accuracy: 0.9855 - val_loss: 0.0891 - val_accuracy: 0.9791\n",
      "Epoch 138/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0426 - accuracy: 0.9859 - val_loss: 0.0905 - val_accuracy: 0.9774\n",
      "Epoch 139/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0419 - accuracy: 0.9862 - val_loss: 0.0903 - val_accuracy: 0.9779\n",
      "Epoch 140/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0423 - accuracy: 0.9865 - val_loss: 0.0927 - val_accuracy: 0.9763\n",
      "Epoch 141/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0420 - accuracy: 0.9854 - val_loss: 0.0871 - val_accuracy: 0.9785\n",
      "Epoch 142/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0433 - accuracy: 0.9862 - val_loss: 0.0865 - val_accuracy: 0.9799\n",
      "Epoch 143/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0418 - accuracy: 0.9862 - val_loss: 0.0914 - val_accuracy: 0.9768\n",
      "Epoch 144/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0417 - accuracy: 0.9864 - val_loss: 0.0931 - val_accuracy: 0.9777\n",
      "Epoch 145/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0413 - accuracy: 0.9865 - val_loss: 0.0910 - val_accuracy: 0.9763\n",
      "Epoch 146/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0412 - accuracy: 0.9861 - val_loss: 0.0878 - val_accuracy: 0.9771\n",
      "Epoch 147/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0413 - accuracy: 0.9868 - val_loss: 0.0940 - val_accuracy: 0.9768\n",
      "Epoch 148/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0423 - accuracy: 0.9862 - val_loss: 0.0856 - val_accuracy: 0.9782\n",
      "Epoch 149/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0412 - accuracy: 0.9865 - val_loss: 0.0870 - val_accuracy: 0.9782\n",
      "Epoch 150/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0401 - accuracy: 0.9874 - val_loss: 0.0886 - val_accuracy: 0.9788\n",
      "Epoch 151/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0401 - accuracy: 0.9866 - val_loss: 0.0966 - val_accuracy: 0.9760\n",
      "Epoch 152/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0404 - accuracy: 0.9864 - val_loss: 0.0887 - val_accuracy: 0.9796\n",
      "Epoch 153/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0401 - accuracy: 0.9858 - val_loss: 0.0886 - val_accuracy: 0.9768\n",
      "Epoch 154/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0402 - accuracy: 0.9870 - val_loss: 0.0916 - val_accuracy: 0.9779\n",
      "Epoch 155/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0387 - accuracy: 0.9865 - val_loss: 0.0906 - val_accuracy: 0.9779\n",
      "Epoch 156/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0403 - accuracy: 0.9870 - val_loss: 0.0894 - val_accuracy: 0.9785\n",
      "Epoch 157/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0385 - accuracy: 0.9878 - val_loss: 0.0940 - val_accuracy: 0.9768\n",
      "Epoch 158/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0402 - accuracy: 0.9868 - val_loss: 0.0944 - val_accuracy: 0.9777\n",
      "Epoch 159/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0391 - accuracy: 0.9868 - val_loss: 0.0927 - val_accuracy: 0.9782\n",
      "Epoch 160/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0391 - accuracy: 0.9867 - val_loss: 0.0940 - val_accuracy: 0.9757\n",
      "Epoch 161/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0391 - accuracy: 0.9868 - val_loss: 0.0927 - val_accuracy: 0.9777\n",
      "Epoch 162/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0376 - accuracy: 0.9869 - val_loss: 0.0999 - val_accuracy: 0.9763\n",
      "Epoch 163/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0386 - accuracy: 0.9869 - val_loss: 0.0947 - val_accuracy: 0.9765\n",
      "Epoch 164/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0381 - accuracy: 0.9873 - val_loss: 0.0973 - val_accuracy: 0.9779\n",
      "Epoch 165/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0385 - accuracy: 0.9869 - val_loss: 0.0997 - val_accuracy: 0.9765\n",
      "Epoch 166/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0397 - accuracy: 0.9870 - val_loss: 0.1030 - val_accuracy: 0.9732\n",
      "Epoch 167/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0380 - accuracy: 0.9867 - val_loss: 0.0974 - val_accuracy: 0.9751\n",
      "Epoch 168/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0388 - accuracy: 0.9870 - val_loss: 0.0967 - val_accuracy: 0.9760\n",
      "Epoch 169/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0381 - accuracy: 0.9874 - val_loss: 0.0935 - val_accuracy: 0.9777\n",
      "Epoch 170/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0387 - accuracy: 0.9874 - val_loss: 0.0931 - val_accuracy: 0.9768\n",
      "Epoch 171/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0379 - accuracy: 0.9873 - val_loss: 0.0964 - val_accuracy: 0.9779\n",
      "Epoch 172/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0393 - accuracy: 0.9861 - val_loss: 0.0947 - val_accuracy: 0.9774\n",
      "Epoch 173/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0393 - accuracy: 0.9876 - val_loss: 0.0920 - val_accuracy: 0.9777\n",
      "Epoch 174/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0383 - accuracy: 0.9878 - val_loss: 0.0915 - val_accuracy: 0.9771\n",
      "Epoch 175/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0375 - accuracy: 0.9862 - val_loss: 0.0987 - val_accuracy: 0.9771\n",
      "Epoch 176/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0369 - accuracy: 0.9878 - val_loss: 0.1011 - val_accuracy: 0.9765\n",
      "Epoch 177/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0374 - accuracy: 0.9868 - val_loss: 0.0985 - val_accuracy: 0.9760\n",
      "Epoch 178/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0394 - accuracy: 0.9867 - val_loss: 0.1015 - val_accuracy: 0.9763\n",
      "Epoch 179/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0371 - accuracy: 0.9873 - val_loss: 0.1060 - val_accuracy: 0.9760\n",
      "Epoch 180/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0359 - accuracy: 0.9888 - val_loss: 0.0984 - val_accuracy: 0.9765\n",
      "Epoch 181/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0384 - accuracy: 0.9875 - val_loss: 0.0974 - val_accuracy: 0.9782\n",
      "Epoch 182/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0362 - accuracy: 0.9882 - val_loss: 0.0987 - val_accuracy: 0.9754\n",
      "Epoch 183/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0376 - accuracy: 0.9871 - val_loss: 0.0950 - val_accuracy: 0.9771\n",
      "Epoch 184/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0376 - accuracy: 0.9878 - val_loss: 0.1009 - val_accuracy: 0.9757\n",
      "Epoch 185/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0383 - accuracy: 0.9876 - val_loss: 0.0992 - val_accuracy: 0.9763\n",
      "Epoch 186/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0367 - accuracy: 0.9875 - val_loss: 0.1001 - val_accuracy: 0.9768\n",
      "Epoch 187/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0372 - accuracy: 0.9871 - val_loss: 0.1051 - val_accuracy: 0.9774\n",
      "Epoch 188/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0369 - accuracy: 0.9878 - val_loss: 0.1033 - val_accuracy: 0.9765\n",
      "Epoch 189/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0391 - accuracy: 0.9873 - val_loss: 0.1042 - val_accuracy: 0.9754\n",
      "Epoch 190/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0361 - accuracy: 0.9876 - val_loss: 0.1011 - val_accuracy: 0.9771\n",
      "Epoch 191/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0356 - accuracy: 0.9872 - val_loss: 0.1030 - val_accuracy: 0.9757\n",
      "Epoch 192/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0375 - accuracy: 0.9867 - val_loss: 0.1004 - val_accuracy: 0.9785\n",
      "Epoch 193/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0351 - accuracy: 0.9878 - val_loss: 0.1033 - val_accuracy: 0.9757\n",
      "Epoch 194/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0352 - accuracy: 0.9879 - val_loss: 0.1064 - val_accuracy: 0.9751\n",
      "Epoch 195/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0358 - accuracy: 0.9885 - val_loss: 0.0992 - val_accuracy: 0.9743\n",
      "Epoch 196/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0394 - accuracy: 0.9869 - val_loss: 0.1015 - val_accuracy: 0.9768\n",
      "Epoch 197/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0357 - accuracy: 0.9888 - val_loss: 0.0964 - val_accuracy: 0.9788\n",
      "Epoch 198/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0362 - accuracy: 0.9880 - val_loss: 0.0959 - val_accuracy: 0.9765\n",
      "Epoch 199/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0341 - accuracy: 0.9883 - val_loss: 0.1041 - val_accuracy: 0.9746\n",
      "Epoch 200/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0369 - accuracy: 0.9876 - val_loss: 0.1032 - val_accuracy: 0.9763\n",
      "Epoch 201/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0360 - accuracy: 0.9878 - val_loss: 0.1005 - val_accuracy: 0.9763\n",
      "Epoch 202/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0360 - accuracy: 0.9886 - val_loss: 0.1101 - val_accuracy: 0.9737\n",
      "Epoch 203/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0338 - accuracy: 0.9892 - val_loss: 0.1035 - val_accuracy: 0.9763\n",
      "Epoch 204/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0347 - accuracy: 0.9876 - val_loss: 0.1066 - val_accuracy: 0.9746\n",
      "Epoch 205/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0328 - accuracy: 0.9879 - val_loss: 0.1101 - val_accuracy: 0.9763\n",
      "Epoch 206/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0343 - accuracy: 0.9887 - val_loss: 0.1116 - val_accuracy: 0.9751\n",
      "Epoch 207/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0331 - accuracy: 0.9881 - val_loss: 0.1076 - val_accuracy: 0.9740\n",
      "Epoch 208/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0343 - accuracy: 0.9886 - val_loss: 0.1049 - val_accuracy: 0.9746\n",
      "Epoch 209/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0350 - accuracy: 0.9878 - val_loss: 0.1010 - val_accuracy: 0.9763\n",
      "Epoch 210/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0339 - accuracy: 0.9889 - val_loss: 0.1155 - val_accuracy: 0.9749\n",
      "Epoch 211/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0359 - accuracy: 0.9881 - val_loss: 0.1035 - val_accuracy: 0.9754\n",
      "Epoch 212/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0340 - accuracy: 0.9885 - val_loss: 0.1044 - val_accuracy: 0.9779\n",
      "Epoch 213/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0338 - accuracy: 0.9883 - val_loss: 0.1020 - val_accuracy: 0.9785\n",
      "Epoch 214/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0348 - accuracy: 0.9881 - val_loss: 0.1062 - val_accuracy: 0.9754\n",
      "Epoch 215/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0334 - accuracy: 0.9883 - val_loss: 0.1063 - val_accuracy: 0.9760\n",
      "Epoch 216/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0348 - accuracy: 0.9885 - val_loss: 0.1024 - val_accuracy: 0.9771\n",
      "Epoch 217/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0358 - accuracy: 0.9876 - val_loss: 0.1010 - val_accuracy: 0.9793\n",
      "Epoch 218/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0340 - accuracy: 0.9878 - val_loss: 0.1023 - val_accuracy: 0.9754\n",
      "Epoch 219/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0323 - accuracy: 0.9897 - val_loss: 0.1005 - val_accuracy: 0.9774\n",
      "Epoch 220/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0316 - accuracy: 0.9893 - val_loss: 0.1018 - val_accuracy: 0.9765\n",
      "Epoch 221/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0317 - accuracy: 0.9889 - val_loss: 0.1126 - val_accuracy: 0.9757\n",
      "Epoch 222/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0340 - accuracy: 0.9887 - val_loss: 0.1044 - val_accuracy: 0.9757\n",
      "Epoch 223/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0326 - accuracy: 0.9886 - val_loss: 0.1042 - val_accuracy: 0.9765\n",
      "Epoch 224/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0328 - accuracy: 0.9888 - val_loss: 0.1030 - val_accuracy: 0.9774\n",
      "Epoch 225/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0331 - accuracy: 0.9886 - val_loss: 0.1103 - val_accuracy: 0.9737\n",
      "Epoch 226/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0325 - accuracy: 0.9887 - val_loss: 0.1063 - val_accuracy: 0.9751\n",
      "Epoch 227/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0318 - accuracy: 0.9885 - val_loss: 0.1063 - val_accuracy: 0.9757\n",
      "Epoch 228/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0326 - accuracy: 0.9885 - val_loss: 0.1065 - val_accuracy: 0.9751\n",
      "Epoch 229/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0321 - accuracy: 0.9895 - val_loss: 0.1058 - val_accuracy: 0.9760\n",
      "Epoch 230/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0333 - accuracy: 0.9883 - val_loss: 0.1095 - val_accuracy: 0.9754\n",
      "Epoch 231/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0328 - accuracy: 0.9878 - val_loss: 0.1063 - val_accuracy: 0.9763\n",
      "Epoch 232/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0324 - accuracy: 0.9891 - val_loss: 0.1048 - val_accuracy: 0.9760\n",
      "Epoch 233/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0320 - accuracy: 0.9888 - val_loss: 0.1029 - val_accuracy: 0.9749\n",
      "Epoch 234/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0318 - accuracy: 0.9892 - val_loss: 0.1044 - val_accuracy: 0.9757\n",
      "Epoch 235/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0320 - accuracy: 0.9878 - val_loss: 0.1043 - val_accuracy: 0.9777\n",
      "Epoch 236/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0329 - accuracy: 0.9879 - val_loss: 0.1036 - val_accuracy: 0.9754\n",
      "Epoch 237/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0314 - accuracy: 0.9892 - val_loss: 0.1025 - val_accuracy: 0.9757\n",
      "Epoch 238/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0311 - accuracy: 0.9888 - val_loss: 0.1073 - val_accuracy: 0.9746\n",
      "Epoch 239/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0314 - accuracy: 0.9888 - val_loss: 0.1076 - val_accuracy: 0.9757\n",
      "Epoch 240/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0330 - accuracy: 0.9877 - val_loss: 0.1104 - val_accuracy: 0.9757\n",
      "Epoch 241/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0317 - accuracy: 0.9889 - val_loss: 0.1096 - val_accuracy: 0.9749\n",
      "Epoch 242/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0320 - accuracy: 0.9887 - val_loss: 0.1124 - val_accuracy: 0.9749\n",
      "Epoch 243/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0306 - accuracy: 0.9899 - val_loss: 0.1122 - val_accuracy: 0.9754\n",
      "Epoch 244/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0330 - accuracy: 0.9896 - val_loss: 0.1142 - val_accuracy: 0.9735\n",
      "Epoch 245/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0318 - accuracy: 0.9892 - val_loss: 0.1125 - val_accuracy: 0.9729\n",
      "Epoch 246/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0316 - accuracy: 0.9890 - val_loss: 0.1173 - val_accuracy: 0.9749\n",
      "Epoch 247/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0320 - accuracy: 0.9888 - val_loss: 0.1070 - val_accuracy: 0.9751\n",
      "Epoch 248/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0303 - accuracy: 0.9892 - val_loss: 0.1113 - val_accuracy: 0.9735\n",
      "Epoch 249/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0317 - accuracy: 0.9899 - val_loss: 0.1120 - val_accuracy: 0.9757\n",
      "Epoch 250/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0303 - accuracy: 0.9899 - val_loss: 0.1101 - val_accuracy: 0.9749\n",
      "Epoch 251/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0302 - accuracy: 0.9900 - val_loss: 0.1069 - val_accuracy: 0.9771\n",
      "Epoch 252/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0331 - accuracy: 0.9880 - val_loss: 0.1078 - val_accuracy: 0.9765\n",
      "Epoch 253/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0310 - accuracy: 0.9888 - val_loss: 0.1118 - val_accuracy: 0.9760\n",
      "Epoch 254/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0290 - accuracy: 0.9903 - val_loss: 0.1095 - val_accuracy: 0.9754\n",
      "Epoch 255/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0287 - accuracy: 0.9900 - val_loss: 0.1127 - val_accuracy: 0.9757\n",
      "Epoch 256/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0321 - accuracy: 0.9889 - val_loss: 0.1149 - val_accuracy: 0.9754\n",
      "Epoch 257/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0298 - accuracy: 0.9893 - val_loss: 0.1148 - val_accuracy: 0.9754\n",
      "Epoch 258/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0324 - accuracy: 0.9889 - val_loss: 0.1135 - val_accuracy: 0.9746\n",
      "Epoch 259/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0318 - accuracy: 0.9886 - val_loss: 0.1078 - val_accuracy: 0.9765\n",
      "Epoch 260/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0301 - accuracy: 0.9895 - val_loss: 0.1092 - val_accuracy: 0.9749\n",
      "Epoch 261/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0298 - accuracy: 0.9889 - val_loss: 0.1127 - val_accuracy: 0.9746\n",
      "Epoch 262/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0281 - accuracy: 0.9897 - val_loss: 0.1100 - val_accuracy: 0.9751\n",
      "Epoch 263/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0298 - accuracy: 0.9889 - val_loss: 0.1090 - val_accuracy: 0.9768\n",
      "Epoch 264/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0305 - accuracy: 0.9892 - val_loss: 0.1088 - val_accuracy: 0.9729\n",
      "Epoch 265/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0299 - accuracy: 0.9899 - val_loss: 0.1139 - val_accuracy: 0.9763\n",
      "Epoch 266/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0306 - accuracy: 0.9895 - val_loss: 0.1126 - val_accuracy: 0.9751\n",
      "Epoch 267/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0294 - accuracy: 0.9903 - val_loss: 0.1143 - val_accuracy: 0.9771\n",
      "Epoch 268/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0302 - accuracy: 0.9896 - val_loss: 0.1107 - val_accuracy: 0.9740\n",
      "Epoch 269/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0290 - accuracy: 0.9900 - val_loss: 0.1153 - val_accuracy: 0.9746\n",
      "Epoch 270/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0297 - accuracy: 0.9897 - val_loss: 0.1149 - val_accuracy: 0.9749\n",
      "Epoch 271/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0290 - accuracy: 0.9901 - val_loss: 0.1109 - val_accuracy: 0.9751\n",
      "Epoch 272/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0305 - accuracy: 0.9896 - val_loss: 0.1097 - val_accuracy: 0.9754\n",
      "Epoch 273/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0279 - accuracy: 0.9906 - val_loss: 0.1138 - val_accuracy: 0.9751\n",
      "Epoch 274/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0292 - accuracy: 0.9891 - val_loss: 0.1098 - val_accuracy: 0.9751\n",
      "Epoch 275/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0290 - accuracy: 0.9903 - val_loss: 0.1071 - val_accuracy: 0.9774\n",
      "Epoch 276/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0292 - accuracy: 0.9897 - val_loss: 0.1147 - val_accuracy: 0.9740\n",
      "Epoch 277/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0289 - accuracy: 0.9900 - val_loss: 0.1131 - val_accuracy: 0.9737\n",
      "Epoch 278/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0283 - accuracy: 0.9899 - val_loss: 0.1108 - val_accuracy: 0.9763\n",
      "Epoch 279/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0305 - accuracy: 0.9895 - val_loss: 0.1112 - val_accuracy: 0.9765\n",
      "Epoch 280/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0272 - accuracy: 0.9909 - val_loss: 0.1189 - val_accuracy: 0.9751\n",
      "Epoch 281/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0296 - accuracy: 0.9900 - val_loss: 0.1120 - val_accuracy: 0.9754\n",
      "Epoch 282/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0282 - accuracy: 0.9900 - val_loss: 0.1131 - val_accuracy: 0.9765\n",
      "Epoch 283/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0309 - accuracy: 0.9888 - val_loss: 0.1156 - val_accuracy: 0.9771\n",
      "Epoch 284/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0315 - accuracy: 0.9886 - val_loss: 0.1133 - val_accuracy: 0.9743\n",
      "Epoch 285/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0282 - accuracy: 0.9899 - val_loss: 0.1136 - val_accuracy: 0.9760\n",
      "Epoch 286/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0280 - accuracy: 0.9895 - val_loss: 0.1109 - val_accuracy: 0.9751\n",
      "Epoch 287/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0275 - accuracy: 0.9904 - val_loss: 0.1108 - val_accuracy: 0.9782\n",
      "Epoch 288/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0307 - accuracy: 0.9881 - val_loss: 0.1154 - val_accuracy: 0.9765\n",
      "Epoch 289/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0289 - accuracy: 0.9892 - val_loss: 0.1132 - val_accuracy: 0.9760\n",
      "Epoch 290/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0266 - accuracy: 0.9908 - val_loss: 0.1184 - val_accuracy: 0.9732\n",
      "Epoch 291/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0304 - accuracy: 0.9894 - val_loss: 0.1163 - val_accuracy: 0.9768\n",
      "Epoch 292/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0280 - accuracy: 0.9902 - val_loss: 0.1130 - val_accuracy: 0.9746\n",
      "Epoch 293/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0289 - accuracy: 0.9899 - val_loss: 0.1186 - val_accuracy: 0.9746\n",
      "Epoch 294/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0273 - accuracy: 0.9900 - val_loss: 0.1096 - val_accuracy: 0.9774\n",
      "Epoch 295/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0265 - accuracy: 0.9902 - val_loss: 0.1146 - val_accuracy: 0.9765\n",
      "Epoch 296/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0272 - accuracy: 0.9899 - val_loss: 0.1105 - val_accuracy: 0.9765\n",
      "Epoch 297/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0284 - accuracy: 0.9899 - val_loss: 0.1149 - val_accuracy: 0.9740\n",
      "Epoch 298/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0303 - accuracy: 0.9892 - val_loss: 0.1111 - val_accuracy: 0.9774\n",
      "Epoch 299/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0276 - accuracy: 0.9900 - val_loss: 0.1160 - val_accuracy: 0.9768\n",
      "Epoch 300/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0268 - accuracy: 0.9902 - val_loss: 0.1198 - val_accuracy: 0.9763\n",
      "Epoch 301/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0285 - accuracy: 0.9903 - val_loss: 0.1249 - val_accuracy: 0.9754\n",
      "Epoch 302/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0261 - accuracy: 0.9907 - val_loss: 0.1208 - val_accuracy: 0.9735\n",
      "Epoch 303/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0275 - accuracy: 0.9900 - val_loss: 0.1198 - val_accuracy: 0.9729\n",
      "Epoch 304/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0295 - accuracy: 0.9895 - val_loss: 0.1224 - val_accuracy: 0.9740\n",
      "Epoch 305/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0286 - accuracy: 0.9899 - val_loss: 0.1236 - val_accuracy: 0.9732\n",
      "Epoch 306/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0262 - accuracy: 0.9904 - val_loss: 0.1215 - val_accuracy: 0.9763\n",
      "Epoch 307/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0281 - accuracy: 0.9897 - val_loss: 0.1219 - val_accuracy: 0.9765\n",
      "Epoch 308/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0269 - accuracy: 0.9913 - val_loss: 0.1150 - val_accuracy: 0.9760\n",
      "Epoch 309/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0270 - accuracy: 0.9899 - val_loss: 0.1164 - val_accuracy: 0.9765\n",
      "Epoch 310/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0265 - accuracy: 0.9906 - val_loss: 0.1165 - val_accuracy: 0.9749\n",
      "Epoch 311/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0274 - accuracy: 0.9899 - val_loss: 0.1217 - val_accuracy: 0.9768\n",
      "Epoch 312/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0282 - accuracy: 0.9903 - val_loss: 0.1170 - val_accuracy: 0.9743\n",
      "Epoch 313/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0259 - accuracy: 0.9906 - val_loss: 0.1228 - val_accuracy: 0.9726\n",
      "Epoch 314/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0263 - accuracy: 0.9904 - val_loss: 0.1252 - val_accuracy: 0.9735\n",
      "Epoch 315/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0286 - accuracy: 0.9899 - val_loss: 0.1169 - val_accuracy: 0.9765\n",
      "Epoch 316/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0284 - accuracy: 0.9892 - val_loss: 0.1235 - val_accuracy: 0.9754\n",
      "Epoch 317/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0259 - accuracy: 0.9910 - val_loss: 0.1202 - val_accuracy: 0.9715\n",
      "Epoch 318/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0282 - accuracy: 0.9906 - val_loss: 0.1119 - val_accuracy: 0.9737\n",
      "Epoch 319/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0268 - accuracy: 0.9909 - val_loss: 0.1166 - val_accuracy: 0.9763\n",
      "Epoch 320/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0271 - accuracy: 0.9911 - val_loss: 0.1228 - val_accuracy: 0.9749\n",
      "Epoch 321/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0274 - accuracy: 0.9906 - val_loss: 0.1214 - val_accuracy: 0.9743\n",
      "Epoch 322/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0253 - accuracy: 0.9913 - val_loss: 0.1183 - val_accuracy: 0.9751\n",
      "Epoch 323/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0288 - accuracy: 0.9908 - val_loss: 0.1274 - val_accuracy: 0.9743\n",
      "Epoch 324/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0262 - accuracy: 0.9908 - val_loss: 0.1177 - val_accuracy: 0.9746\n",
      "Epoch 325/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0265 - accuracy: 0.9914 - val_loss: 0.1201 - val_accuracy: 0.9729\n",
      "Epoch 326/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0254 - accuracy: 0.9906 - val_loss: 0.1182 - val_accuracy: 0.9743\n",
      "Epoch 327/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0256 - accuracy: 0.9908 - val_loss: 0.1177 - val_accuracy: 0.9740\n",
      "Epoch 328/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0261 - accuracy: 0.9904 - val_loss: 0.1152 - val_accuracy: 0.9757\n",
      "Epoch 329/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0244 - accuracy: 0.9909 - val_loss: 0.1221 - val_accuracy: 0.9743\n",
      "Epoch 330/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0246 - accuracy: 0.9905 - val_loss: 0.1198 - val_accuracy: 0.9749\n",
      "Epoch 331/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0254 - accuracy: 0.9911 - val_loss: 0.1195 - val_accuracy: 0.9751\n",
      "Epoch 332/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0238 - accuracy: 0.9914 - val_loss: 0.1264 - val_accuracy: 0.9749\n",
      "Epoch 333/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0270 - accuracy: 0.9900 - val_loss: 0.1230 - val_accuracy: 0.9735\n",
      "Epoch 334/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0260 - accuracy: 0.9904 - val_loss: 0.1258 - val_accuracy: 0.9726\n",
      "Epoch 335/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0253 - accuracy: 0.9913 - val_loss: 0.1210 - val_accuracy: 0.9757\n",
      "Epoch 336/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0266 - accuracy: 0.9909 - val_loss: 0.1197 - val_accuracy: 0.9757\n",
      "Epoch 337/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0270 - accuracy: 0.9907 - val_loss: 0.1226 - val_accuracy: 0.9749\n",
      "Epoch 338/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0240 - accuracy: 0.9918 - val_loss: 0.1197 - val_accuracy: 0.9760\n",
      "Epoch 339/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0263 - accuracy: 0.9903 - val_loss: 0.1231 - val_accuracy: 0.9749\n",
      "Epoch 340/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0246 - accuracy: 0.9915 - val_loss: 0.1220 - val_accuracy: 0.9749\n",
      "Epoch 341/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0249 - accuracy: 0.9918 - val_loss: 0.1203 - val_accuracy: 0.9740\n",
      "Epoch 342/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0247 - accuracy: 0.9915 - val_loss: 0.1310 - val_accuracy: 0.9743\n",
      "Epoch 343/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0276 - accuracy: 0.9911 - val_loss: 0.1205 - val_accuracy: 0.9757\n",
      "Epoch 344/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0243 - accuracy: 0.9910 - val_loss: 0.1152 - val_accuracy: 0.9757\n",
      "Epoch 345/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0254 - accuracy: 0.9905 - val_loss: 0.1208 - val_accuracy: 0.9754\n",
      "Epoch 346/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0254 - accuracy: 0.9915 - val_loss: 0.1307 - val_accuracy: 0.9723\n",
      "Epoch 347/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0251 - accuracy: 0.9909 - val_loss: 0.1226 - val_accuracy: 0.9763\n",
      "Epoch 348/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0266 - accuracy: 0.9899 - val_loss: 0.1264 - val_accuracy: 0.9751\n",
      "Epoch 349/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0242 - accuracy: 0.9912 - val_loss: 0.1302 - val_accuracy: 0.9732\n",
      "Epoch 350/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0243 - accuracy: 0.9912 - val_loss: 0.1255 - val_accuracy: 0.9743\n",
      "Epoch 351/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0239 - accuracy: 0.9908 - val_loss: 0.1163 - val_accuracy: 0.9765\n",
      "Epoch 352/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0239 - accuracy: 0.9915 - val_loss: 0.1214 - val_accuracy: 0.9754\n",
      "Epoch 353/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0263 - accuracy: 0.9901 - val_loss: 0.1257 - val_accuracy: 0.9743\n",
      "Epoch 354/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0241 - accuracy: 0.9915 - val_loss: 0.1249 - val_accuracy: 0.9746\n",
      "Epoch 355/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0261 - accuracy: 0.9904 - val_loss: 0.1206 - val_accuracy: 0.9735\n",
      "Epoch 356/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0246 - accuracy: 0.9906 - val_loss: 0.1268 - val_accuracy: 0.9737\n",
      "Epoch 357/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0246 - accuracy: 0.9922 - val_loss: 0.1286 - val_accuracy: 0.9740\n",
      "Epoch 358/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0223 - accuracy: 0.9914 - val_loss: 0.1197 - val_accuracy: 0.9763\n",
      "Epoch 359/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0264 - accuracy: 0.9909 - val_loss: 0.1268 - val_accuracy: 0.9749\n",
      "Epoch 360/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0245 - accuracy: 0.9911 - val_loss: 0.1234 - val_accuracy: 0.9737\n",
      "Epoch 361/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0239 - accuracy: 0.9912 - val_loss: 0.1310 - val_accuracy: 0.9735\n",
      "Epoch 362/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0241 - accuracy: 0.9910 - val_loss: 0.1248 - val_accuracy: 0.9760\n",
      "Epoch 363/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0242 - accuracy: 0.9905 - val_loss: 0.1258 - val_accuracy: 0.9754\n",
      "Epoch 364/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0252 - accuracy: 0.9905 - val_loss: 0.1287 - val_accuracy: 0.9746\n",
      "Epoch 365/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0231 - accuracy: 0.9912 - val_loss: 0.1247 - val_accuracy: 0.9757\n",
      "Epoch 366/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0259 - accuracy: 0.9907 - val_loss: 0.1289 - val_accuracy: 0.9757\n",
      "Epoch 367/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0230 - accuracy: 0.9918 - val_loss: 0.1240 - val_accuracy: 0.9746\n",
      "Epoch 368/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0240 - accuracy: 0.9916 - val_loss: 0.1289 - val_accuracy: 0.9737\n",
      "Epoch 369/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0251 - accuracy: 0.9909 - val_loss: 0.1239 - val_accuracy: 0.9749\n",
      "Epoch 370/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0211 - accuracy: 0.9913 - val_loss: 0.1336 - val_accuracy: 0.9735\n",
      "Epoch 371/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0254 - accuracy: 0.9902 - val_loss: 0.1233 - val_accuracy: 0.9754\n",
      "Epoch 372/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0233 - accuracy: 0.9915 - val_loss: 0.1272 - val_accuracy: 0.9763\n",
      "Epoch 373/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0231 - accuracy: 0.9918 - val_loss: 0.1259 - val_accuracy: 0.9746\n",
      "Epoch 374/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0236 - accuracy: 0.9918 - val_loss: 0.1274 - val_accuracy: 0.9757\n",
      "Epoch 375/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0248 - accuracy: 0.9911 - val_loss: 0.1340 - val_accuracy: 0.9751\n",
      "Epoch 376/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0224 - accuracy: 0.9918 - val_loss: 0.1282 - val_accuracy: 0.9751\n",
      "Epoch 377/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0240 - accuracy: 0.9913 - val_loss: 0.1343 - val_accuracy: 0.9740\n",
      "Epoch 378/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0199 - accuracy: 0.9936 - val_loss: 0.1362 - val_accuracy: 0.9723\n",
      "Epoch 379/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0240 - accuracy: 0.9917 - val_loss: 0.1279 - val_accuracy: 0.9746\n",
      "Epoch 380/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0240 - accuracy: 0.9911 - val_loss: 0.1336 - val_accuracy: 0.9754\n",
      "Epoch 381/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0223 - accuracy: 0.9925 - val_loss: 0.1307 - val_accuracy: 0.9757\n",
      "Epoch 382/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0216 - accuracy: 0.9914 - val_loss: 0.1413 - val_accuracy: 0.9754\n",
      "Epoch 383/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0233 - accuracy: 0.9914 - val_loss: 0.1362 - val_accuracy: 0.9743\n",
      "Epoch 384/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0210 - accuracy: 0.9920 - val_loss: 0.1310 - val_accuracy: 0.9746\n",
      "Epoch 385/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0220 - accuracy: 0.9929 - val_loss: 0.1391 - val_accuracy: 0.9743\n",
      "Epoch 386/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0214 - accuracy: 0.9919 - val_loss: 0.1381 - val_accuracy: 0.9721\n",
      "Epoch 387/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0235 - accuracy: 0.9911 - val_loss: 0.1337 - val_accuracy: 0.9743\n",
      "Epoch 388/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0215 - accuracy: 0.9928 - val_loss: 0.1330 - val_accuracy: 0.9740\n",
      "Epoch 389/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0211 - accuracy: 0.9932 - val_loss: 0.1350 - val_accuracy: 0.9723\n",
      "Epoch 390/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0224 - accuracy: 0.9913 - val_loss: 0.1275 - val_accuracy: 0.9765\n",
      "Epoch 391/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0235 - accuracy: 0.9918 - val_loss: 0.1355 - val_accuracy: 0.9737\n",
      "Epoch 392/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0238 - accuracy: 0.9915 - val_loss: 0.1313 - val_accuracy: 0.9760\n",
      "Epoch 393/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0261 - accuracy: 0.9913 - val_loss: 0.1343 - val_accuracy: 0.9763\n",
      "Epoch 394/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0221 - accuracy: 0.9922 - val_loss: 0.1369 - val_accuracy: 0.9729\n",
      "Epoch 395/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0252 - accuracy: 0.9906 - val_loss: 0.1314 - val_accuracy: 0.9763\n",
      "Epoch 396/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0220 - accuracy: 0.9918 - val_loss: 0.1371 - val_accuracy: 0.9740\n",
      "Epoch 397/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0237 - accuracy: 0.9916 - val_loss: 0.1344 - val_accuracy: 0.9737\n",
      "Epoch 398/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0200 - accuracy: 0.9929 - val_loss: 0.1344 - val_accuracy: 0.9737\n",
      "Epoch 399/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0243 - accuracy: 0.9911 - val_loss: 0.1338 - val_accuracy: 0.9754\n",
      "Epoch 400/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0231 - accuracy: 0.9915 - val_loss: 0.1354 - val_accuracy: 0.9743\n",
      "Epoch 401/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0214 - accuracy: 0.9923 - val_loss: 0.1327 - val_accuracy: 0.9754\n",
      "Epoch 402/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0229 - accuracy: 0.9914 - val_loss: 0.1306 - val_accuracy: 0.9754\n",
      "Epoch 403/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0225 - accuracy: 0.9917 - val_loss: 0.1327 - val_accuracy: 0.9757\n",
      "Epoch 404/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0213 - accuracy: 0.9927 - val_loss: 0.1343 - val_accuracy: 0.9740\n",
      "Epoch 405/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0213 - accuracy: 0.9924 - val_loss: 0.1376 - val_accuracy: 0.9732\n",
      "Epoch 406/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0231 - accuracy: 0.9917 - val_loss: 0.1350 - val_accuracy: 0.9743\n",
      "Epoch 407/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0211 - accuracy: 0.9927 - val_loss: 0.1328 - val_accuracy: 0.9737\n",
      "Epoch 408/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0215 - accuracy: 0.9920 - val_loss: 0.1398 - val_accuracy: 0.9737\n",
      "Epoch 409/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0222 - accuracy: 0.9920 - val_loss: 0.1396 - val_accuracy: 0.9760\n",
      "Epoch 410/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0224 - accuracy: 0.9925 - val_loss: 0.1420 - val_accuracy: 0.9740\n",
      "Epoch 411/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0211 - accuracy: 0.9918 - val_loss: 0.1384 - val_accuracy: 0.9754\n",
      "Epoch 412/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0208 - accuracy: 0.9926 - val_loss: 0.1407 - val_accuracy: 0.9715\n",
      "Epoch 413/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0243 - accuracy: 0.9914 - val_loss: 0.1454 - val_accuracy: 0.9729\n",
      "Epoch 414/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0215 - accuracy: 0.9925 - val_loss: 0.1405 - val_accuracy: 0.9751\n",
      "Epoch 415/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0226 - accuracy: 0.9921 - val_loss: 0.1488 - val_accuracy: 0.9715\n",
      "Epoch 416/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0208 - accuracy: 0.9918 - val_loss: 0.1461 - val_accuracy: 0.9751\n",
      "Epoch 417/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0213 - accuracy: 0.9920 - val_loss: 0.1393 - val_accuracy: 0.9754\n",
      "Epoch 418/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0222 - accuracy: 0.9918 - val_loss: 0.1334 - val_accuracy: 0.9760\n",
      "Epoch 419/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0214 - accuracy: 0.9920 - val_loss: 0.1366 - val_accuracy: 0.9732\n",
      "Epoch 420/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0224 - accuracy: 0.9920 - val_loss: 0.1358 - val_accuracy: 0.9737\n",
      "Epoch 421/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0216 - accuracy: 0.9918 - val_loss: 0.1361 - val_accuracy: 0.9746\n",
      "Epoch 422/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0223 - accuracy: 0.9913 - val_loss: 0.1413 - val_accuracy: 0.9737\n",
      "Epoch 423/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0207 - accuracy: 0.9925 - val_loss: 0.1441 - val_accuracy: 0.9715\n",
      "Epoch 424/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0206 - accuracy: 0.9925 - val_loss: 0.1486 - val_accuracy: 0.9726\n",
      "Epoch 425/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0211 - accuracy: 0.9929 - val_loss: 0.1477 - val_accuracy: 0.9757\n",
      "Epoch 426/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0231 - accuracy: 0.9921 - val_loss: 0.1376 - val_accuracy: 0.9751\n",
      "Epoch 427/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0208 - accuracy: 0.9921 - val_loss: 0.1432 - val_accuracy: 0.9757\n",
      "Epoch 428/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0230 - accuracy: 0.9919 - val_loss: 0.1410 - val_accuracy: 0.9732\n",
      "Epoch 429/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0192 - accuracy: 0.9926 - val_loss: 0.1455 - val_accuracy: 0.9712\n",
      "Epoch 430/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0206 - accuracy: 0.9929 - val_loss: 0.1399 - val_accuracy: 0.9743\n",
      "Epoch 431/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0207 - accuracy: 0.9926 - val_loss: 0.1404 - val_accuracy: 0.9737\n",
      "Epoch 432/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0229 - accuracy: 0.9917 - val_loss: 0.1395 - val_accuracy: 0.9751\n",
      "Epoch 433/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0202 - accuracy: 0.9927 - val_loss: 0.1426 - val_accuracy: 0.9723\n",
      "Epoch 434/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0239 - accuracy: 0.9920 - val_loss: 0.1352 - val_accuracy: 0.9729\n",
      "Epoch 435/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0177 - accuracy: 0.9938 - val_loss: 0.1368 - val_accuracy: 0.9763\n",
      "Epoch 436/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0204 - accuracy: 0.9929 - val_loss: 0.1295 - val_accuracy: 0.9746\n",
      "Epoch 437/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0220 - accuracy: 0.9923 - val_loss: 0.1391 - val_accuracy: 0.9763\n",
      "Epoch 438/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0198 - accuracy: 0.9923 - val_loss: 0.1424 - val_accuracy: 0.9732\n",
      "Epoch 439/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0210 - accuracy: 0.9925 - val_loss: 0.1483 - val_accuracy: 0.9732\n",
      "Epoch 440/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0190 - accuracy: 0.9932 - val_loss: 0.1452 - val_accuracy: 0.9721\n",
      "Epoch 441/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0225 - accuracy: 0.9925 - val_loss: 0.1364 - val_accuracy: 0.9757\n",
      "Epoch 442/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0215 - accuracy: 0.9916 - val_loss: 0.1356 - val_accuracy: 0.9757\n",
      "Epoch 443/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0187 - accuracy: 0.9931 - val_loss: 0.1377 - val_accuracy: 0.9754\n",
      "Epoch 444/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0174 - accuracy: 0.9943 - val_loss: 0.1487 - val_accuracy: 0.9737\n",
      "Epoch 445/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0181 - accuracy: 0.9929 - val_loss: 0.1412 - val_accuracy: 0.9749\n",
      "Epoch 446/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0214 - accuracy: 0.9922 - val_loss: 0.1437 - val_accuracy: 0.9746\n",
      "Epoch 447/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0210 - accuracy: 0.9922 - val_loss: 0.1390 - val_accuracy: 0.9768\n",
      "Epoch 448/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0172 - accuracy: 0.9939 - val_loss: 0.1400 - val_accuracy: 0.9732\n",
      "Epoch 449/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0199 - accuracy: 0.9930 - val_loss: 0.1434 - val_accuracy: 0.9749\n",
      "Epoch 450/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0223 - accuracy: 0.9927 - val_loss: 0.1362 - val_accuracy: 0.9749\n",
      "Epoch 451/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0195 - accuracy: 0.9926 - val_loss: 0.1505 - val_accuracy: 0.9740\n",
      "Epoch 452/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0196 - accuracy: 0.9934 - val_loss: 0.1402 - val_accuracy: 0.9743\n",
      "Epoch 453/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0208 - accuracy: 0.9925 - val_loss: 0.1412 - val_accuracy: 0.9749\n",
      "Epoch 454/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0194 - accuracy: 0.9929 - val_loss: 0.1353 - val_accuracy: 0.9749\n",
      "Epoch 455/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0224 - accuracy: 0.9913 - val_loss: 0.1450 - val_accuracy: 0.9735\n",
      "Epoch 456/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0184 - accuracy: 0.9932 - val_loss: 0.1404 - val_accuracy: 0.9760\n",
      "Epoch 457/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0195 - accuracy: 0.9927 - val_loss: 0.1440 - val_accuracy: 0.9743\n",
      "Epoch 458/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0213 - accuracy: 0.9925 - val_loss: 0.1409 - val_accuracy: 0.9757\n",
      "Epoch 459/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0178 - accuracy: 0.9936 - val_loss: 0.1462 - val_accuracy: 0.9704\n",
      "Epoch 460/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0207 - accuracy: 0.9918 - val_loss: 0.1385 - val_accuracy: 0.9746\n",
      "Epoch 461/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0209 - accuracy: 0.9929 - val_loss: 0.1461 - val_accuracy: 0.9740\n",
      "Epoch 462/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0193 - accuracy: 0.9930 - val_loss: 0.1358 - val_accuracy: 0.9760\n",
      "Epoch 463/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0187 - accuracy: 0.9933 - val_loss: 0.1477 - val_accuracy: 0.9729\n",
      "Epoch 464/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0206 - accuracy: 0.9925 - val_loss: 0.1490 - val_accuracy: 0.9732\n",
      "Epoch 465/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0195 - accuracy: 0.9928 - val_loss: 0.1458 - val_accuracy: 0.9749\n",
      "Epoch 466/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0218 - accuracy: 0.9925 - val_loss: 0.1476 - val_accuracy: 0.9712\n",
      "Epoch 467/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0184 - accuracy: 0.9932 - val_loss: 0.1456 - val_accuracy: 0.9760\n",
      "Epoch 468/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0225 - accuracy: 0.9913 - val_loss: 0.1477 - val_accuracy: 0.9729\n",
      "Epoch 469/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0169 - accuracy: 0.9943 - val_loss: 0.1485 - val_accuracy: 0.9732\n",
      "Epoch 470/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0192 - accuracy: 0.9922 - val_loss: 0.1532 - val_accuracy: 0.9732\n",
      "Epoch 471/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0203 - accuracy: 0.9920 - val_loss: 0.1458 - val_accuracy: 0.9737\n",
      "Epoch 472/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0188 - accuracy: 0.9928 - val_loss: 0.1500 - val_accuracy: 0.9737\n",
      "Epoch 473/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0188 - accuracy: 0.9934 - val_loss: 0.1443 - val_accuracy: 0.9735\n",
      "Epoch 474/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0195 - accuracy: 0.9926 - val_loss: 0.1461 - val_accuracy: 0.9723\n",
      "Epoch 475/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0180 - accuracy: 0.9929 - val_loss: 0.1500 - val_accuracy: 0.9754\n",
      "Epoch 476/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0204 - accuracy: 0.9921 - val_loss: 0.1428 - val_accuracy: 0.9732\n",
      "Epoch 477/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0195 - accuracy: 0.9927 - val_loss: 0.1458 - val_accuracy: 0.9718\n",
      "Epoch 478/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0221 - accuracy: 0.9920 - val_loss: 0.1474 - val_accuracy: 0.9726\n",
      "Epoch 479/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0194 - accuracy: 0.9926 - val_loss: 0.1454 - val_accuracy: 0.9760\n",
      "Epoch 480/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0196 - accuracy: 0.9928 - val_loss: 0.1479 - val_accuracy: 0.9726\n",
      "Epoch 481/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0185 - accuracy: 0.9933 - val_loss: 0.1503 - val_accuracy: 0.9754\n",
      "Epoch 482/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0194 - accuracy: 0.9931 - val_loss: 0.1484 - val_accuracy: 0.9740\n",
      "Epoch 483/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0198 - accuracy: 0.9931 - val_loss: 0.1457 - val_accuracy: 0.9749\n",
      "Epoch 484/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0184 - accuracy: 0.9928 - val_loss: 0.1461 - val_accuracy: 0.9735\n",
      "Epoch 485/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0193 - accuracy: 0.9928 - val_loss: 0.1479 - val_accuracy: 0.9760\n",
      "Epoch 486/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0179 - accuracy: 0.9927 - val_loss: 0.1451 - val_accuracy: 0.9751\n",
      "Epoch 487/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0184 - accuracy: 0.9932 - val_loss: 0.1430 - val_accuracy: 0.9749\n",
      "Epoch 488/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0197 - accuracy: 0.9927 - val_loss: 0.1619 - val_accuracy: 0.9709\n",
      "Epoch 489/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0163 - accuracy: 0.9944 - val_loss: 0.1514 - val_accuracy: 0.9740\n",
      "Epoch 490/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0187 - accuracy: 0.9936 - val_loss: 0.1531 - val_accuracy: 0.9735\n",
      "Epoch 491/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.0192 - accuracy: 0.9932 - val_loss: 0.1433 - val_accuracy: 0.9746\n",
      "Epoch 492/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0197 - accuracy: 0.9932 - val_loss: 0.1452 - val_accuracy: 0.9757\n",
      "Epoch 493/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0168 - accuracy: 0.9943 - val_loss: 0.1493 - val_accuracy: 0.9718\n",
      "Epoch 494/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0171 - accuracy: 0.9942 - val_loss: 0.1432 - val_accuracy: 0.9740\n",
      "Epoch 495/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0195 - accuracy: 0.9927 - val_loss: 0.1425 - val_accuracy: 0.9746\n",
      "Epoch 496/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0188 - accuracy: 0.9925 - val_loss: 0.1517 - val_accuracy: 0.9704\n",
      "Epoch 497/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0180 - accuracy: 0.9938 - val_loss: 0.1561 - val_accuracy: 0.9746\n",
      "Epoch 498/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0179 - accuracy: 0.9931 - val_loss: 0.1589 - val_accuracy: 0.9718\n",
      "Epoch 499/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0197 - accuracy: 0.9925 - val_loss: 0.1569 - val_accuracy: 0.9698\n",
      "Epoch 500/500\n",
      "448/448 [==============================] - 1s 1ms/step - loss: 0.0185 - accuracy: 0.9927 - val_loss: 0.1438 - val_accuracy: 0.9740\n",
      "Accuracy: 0.9740223288536072\n",
      " MSE: 0.14375793933868408\n"
     ]
    }
   ],
   "source": [
    "model3 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu', input_shape=(X_trains.shape[1],)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adamax()\n",
    "model3.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "model3.fit(X_trains, y_train, epochs=500, batch_size=32, verbose=1, \n",
    "          validation_data=(X_tests, y_test))\n",
    "\n",
    "# 모델 평가\n",
    "y_pred = model3.predict(X_tests)\n",
    "    \n",
    "loss, accuracy = model3.evaluate(X_tests, y_test, verbose=0)\n",
    "print(f'Accuracy: {accuracy}\\n MSE: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "448/448 [==============================] - 2s 2ms/step - loss: 15.4434 - accuracy: 0.8892 - val_loss: 6.5679 - val_accuracy: 0.9575\n",
      "Epoch 2/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 4.0537 - accuracy: 0.9609 - val_loss: 2.4679 - val_accuracy: 0.9645\n",
      "Epoch 3/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 1.7779 - accuracy: 0.9649 - val_loss: 1.2675 - val_accuracy: 0.9656\n",
      "Epoch 4/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.9972 - accuracy: 0.9654 - val_loss: 0.7429 - val_accuracy: 0.9740\n",
      "Epoch 5/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.6501 - accuracy: 0.9651 - val_loss: 0.5038 - val_accuracy: 0.9749\n",
      "Epoch 6/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.4412 - accuracy: 0.9687 - val_loss: 0.3675 - val_accuracy: 0.9757\n",
      "Epoch 7/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.3209 - accuracy: 0.9695 - val_loss: 0.2580 - val_accuracy: 0.9785\n",
      "Epoch 8/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.2506 - accuracy: 0.9708 - val_loss: 0.1947 - val_accuracy: 0.9785\n",
      "Epoch 9/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.2127 - accuracy: 0.9709 - val_loss: 0.1875 - val_accuracy: 0.9788\n",
      "Epoch 10/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.2025 - accuracy: 0.9703 - val_loss: 0.1770 - val_accuracy: 0.9788\n",
      "Epoch 11/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.2008 - accuracy: 0.9698 - val_loss: 0.1730 - val_accuracy: 0.9791\n",
      "Epoch 12/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1942 - accuracy: 0.9719 - val_loss: 0.1738 - val_accuracy: 0.9782\n",
      "Epoch 13/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1891 - accuracy: 0.9736 - val_loss: 0.1793 - val_accuracy: 0.9771\n",
      "Epoch 14/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1789 - accuracy: 0.9742 - val_loss: 0.1539 - val_accuracy: 0.9791\n",
      "Epoch 15/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1760 - accuracy: 0.9736 - val_loss: 0.1615 - val_accuracy: 0.9788\n",
      "Epoch 16/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1732 - accuracy: 0.9744 - val_loss: 0.1716 - val_accuracy: 0.9782\n",
      "Epoch 17/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1775 - accuracy: 0.9748 - val_loss: 0.1801 - val_accuracy: 0.9788\n",
      "Epoch 18/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1746 - accuracy: 0.9730 - val_loss: 0.1520 - val_accuracy: 0.9799\n",
      "Epoch 19/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1666 - accuracy: 0.9749 - val_loss: 0.1551 - val_accuracy: 0.9796\n",
      "Epoch 20/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1685 - accuracy: 0.9744 - val_loss: 0.1602 - val_accuracy: 0.9799\n",
      "Epoch 21/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1784 - accuracy: 0.9734 - val_loss: 0.1748 - val_accuracy: 0.9793\n",
      "Epoch 22/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1816 - accuracy: 0.9754 - val_loss: 0.1629 - val_accuracy: 0.9796\n",
      "Epoch 23/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1681 - accuracy: 0.9758 - val_loss: 0.1569 - val_accuracy: 0.9804\n",
      "Epoch 24/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1714 - accuracy: 0.9756 - val_loss: 0.1595 - val_accuracy: 0.9802\n",
      "Epoch 25/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1698 - accuracy: 0.9751 - val_loss: 0.1673 - val_accuracy: 0.9785\n",
      "Epoch 26/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1667 - accuracy: 0.9756 - val_loss: 0.1565 - val_accuracy: 0.9796\n",
      "Epoch 27/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1689 - accuracy: 0.9752 - val_loss: 0.1638 - val_accuracy: 0.9796\n",
      "Epoch 28/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1716 - accuracy: 0.9760 - val_loss: 0.1613 - val_accuracy: 0.9802\n",
      "Epoch 29/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1772 - accuracy: 0.9738 - val_loss: 0.1595 - val_accuracy: 0.9802\n",
      "Epoch 30/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1656 - accuracy: 0.9757 - val_loss: 0.1502 - val_accuracy: 0.9791\n",
      "Epoch 31/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1646 - accuracy: 0.9753 - val_loss: 0.1583 - val_accuracy: 0.9807\n",
      "Epoch 32/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1699 - accuracy: 0.9751 - val_loss: 0.1685 - val_accuracy: 0.9793\n",
      "Epoch 33/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1723 - accuracy: 0.9751 - val_loss: 0.1624 - val_accuracy: 0.9791\n",
      "Epoch 34/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1722 - accuracy: 0.9747 - val_loss: 0.1611 - val_accuracy: 0.9799\n",
      "Epoch 35/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1690 - accuracy: 0.9753 - val_loss: 0.1569 - val_accuracy: 0.9793\n",
      "Epoch 36/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1702 - accuracy: 0.9769 - val_loss: 0.1540 - val_accuracy: 0.9793\n",
      "Epoch 37/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1666 - accuracy: 0.9765 - val_loss: 0.1626 - val_accuracy: 0.9802\n",
      "Epoch 38/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1697 - accuracy: 0.9767 - val_loss: 0.1551 - val_accuracy: 0.9813\n",
      "Epoch 39/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1680 - accuracy: 0.9767 - val_loss: 0.1527 - val_accuracy: 0.9804\n",
      "Epoch 40/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1685 - accuracy: 0.9763 - val_loss: 0.1587 - val_accuracy: 0.9802\n",
      "Epoch 41/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1756 - accuracy: 0.9762 - val_loss: 0.1653 - val_accuracy: 0.9791\n",
      "Epoch 42/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1654 - accuracy: 0.9756 - val_loss: 0.1590 - val_accuracy: 0.9785\n",
      "Epoch 43/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1597 - accuracy: 0.9763 - val_loss: 0.1523 - val_accuracy: 0.9796\n",
      "Epoch 44/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1626 - accuracy: 0.9757 - val_loss: 0.1646 - val_accuracy: 0.9788\n",
      "Epoch 45/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1647 - accuracy: 0.9766 - val_loss: 0.1473 - val_accuracy: 0.9816\n",
      "Epoch 46/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1579 - accuracy: 0.9770 - val_loss: 0.1524 - val_accuracy: 0.9802\n",
      "Epoch 47/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1703 - accuracy: 0.9765 - val_loss: 0.1649 - val_accuracy: 0.9799\n",
      "Epoch 48/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1662 - accuracy: 0.9769 - val_loss: 0.1579 - val_accuracy: 0.9802\n",
      "Epoch 49/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1646 - accuracy: 0.9758 - val_loss: 0.1519 - val_accuracy: 0.9793\n",
      "Epoch 50/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1617 - accuracy: 0.9756 - val_loss: 0.1600 - val_accuracy: 0.9799\n",
      "Epoch 51/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1597 - accuracy: 0.9750 - val_loss: 0.1519 - val_accuracy: 0.9788\n",
      "Epoch 52/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1642 - accuracy: 0.9758 - val_loss: 0.1462 - val_accuracy: 0.9810\n",
      "Epoch 53/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1637 - accuracy: 0.9761 - val_loss: 0.1534 - val_accuracy: 0.9793\n",
      "Epoch 54/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1621 - accuracy: 0.9762 - val_loss: 0.1630 - val_accuracy: 0.9813\n",
      "Epoch 55/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1655 - accuracy: 0.9764 - val_loss: 0.1515 - val_accuracy: 0.9804\n",
      "Epoch 56/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1628 - accuracy: 0.9772 - val_loss: 0.1536 - val_accuracy: 0.9804\n",
      "Epoch 57/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1634 - accuracy: 0.9773 - val_loss: 0.1639 - val_accuracy: 0.9799\n",
      "Epoch 58/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1653 - accuracy: 0.9762 - val_loss: 0.1508 - val_accuracy: 0.9799\n",
      "Epoch 59/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.1659 - accuracy: 0.9767 - val_loss: 0.1719 - val_accuracy: 0.9782\n",
      "Epoch 60/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1711 - accuracy: 0.9760 - val_loss: 0.1584 - val_accuracy: 0.9793\n",
      "Epoch 61/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1590 - accuracy: 0.9780 - val_loss: 0.1602 - val_accuracy: 0.9802\n",
      "Epoch 62/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1620 - accuracy: 0.9771 - val_loss: 0.1587 - val_accuracy: 0.9796\n",
      "Epoch 63/500\n",
      "448/448 [==============================] - 1s 2ms/step - loss: 0.1642 - accuracy: 0.9766 - val_loss: 0.1782 - val_accuracy: 0.9799\n",
      "Epoch 64/500\n",
      " 86/448 [====>.........................] - ETA: 0s - loss: 0.1657 - accuracy: 0.9789"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-fe86859a0d31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# 모델 학습\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m model9.fit(X_trains, y_train, epochs=500, batch_size=32, verbose=1, \n\u001b[1;32m---> 26\u001b[1;33m           validation_data=(X_tests, y_test), callbacks=[es, rlrp])\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# 모델 평가\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\82105\\anaconda3\\envs\\project1\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\82105\\anaconda3\\envs\\project1\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\82105\\anaconda3\\envs\\project1\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\82105\\anaconda3\\envs\\project1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3040\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3042\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\82105\\anaconda3\\envs\\project1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1964\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\82105\\anaconda3\\envs\\project1\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\Users\\82105\\anaconda3\\envs\\project1\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model9 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu', input_shape=(X_trains.shape[1],)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),  # Dropout 추가\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)),  # L1/L2 정규화 추가\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),  # Dropout 추가\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)),  # L1/L2 정규화 추가\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)),  # L1/L2 정규화 추가\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adamax()\n",
    "model9.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 학습 자동 중단 설정\n",
    "es = EarlyStopping(monitor='val_loss', patience=50, mode='min')\n",
    "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=40, mode='min')\n",
    "\n",
    "# 모델 학습\n",
    "model9.fit(X_trains, y_train, epochs=500, batch_size=32, verbose=1, \n",
    "          validation_data=(X_tests, y_test), callbacks=[es, rlrp])\n",
    "\n",
    "# 모델 평가\n",
    "y_pred = model9.predict(X_tests)\n",
    "    \n",
    "loss, accuracy = model9.evaluate(X_tests, y_test, verbose=0)\n",
    "print(f'Accuracy: {accuracy}\\n MSE: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 7.1820 - accuracy: 0.9435 - val_loss: 4.8532 - val_accuracy: 0.9749\n",
      "Epoch 2/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 3.5471 - accuracy: 0.9732 - val_loss: 2.5883 - val_accuracy: 0.9785\n",
      "Epoch 3/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 2.0488 - accuracy: 0.9757 - val_loss: 1.5871 - val_accuracy: 0.9707\n",
      "Epoch 4/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 1.3478 - accuracy: 0.9720 - val_loss: 1.1792 - val_accuracy: 0.9601\n",
      "Epoch 5/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 1.0037 - accuracy: 0.9730 - val_loss: 0.8589 - val_accuracy: 0.9791\n",
      "Epoch 6/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.7839 - accuracy: 0.9740 - val_loss: 0.7305 - val_accuracy: 0.9623\n",
      "Epoch 7/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.6355 - accuracy: 0.9739 - val_loss: 0.5896 - val_accuracy: 0.9791\n",
      "Epoch 8/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.5354 - accuracy: 0.9753 - val_loss: 0.4814 - val_accuracy: 0.9737\n",
      "Epoch 9/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.4491 - accuracy: 0.9758 - val_loss: 0.4429 - val_accuracy: 0.9595\n",
      "Epoch 10/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.3806 - accuracy: 0.9765 - val_loss: 0.3701 - val_accuracy: 0.9670\n",
      "Epoch 11/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.3323 - accuracy: 0.9758 - val_loss: 0.3190 - val_accuracy: 0.9785\n",
      "Epoch 12/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.2895 - accuracy: 0.9771 - val_loss: 0.2715 - val_accuracy: 0.9796\n",
      "Epoch 13/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.2620 - accuracy: 0.9753 - val_loss: 0.2597 - val_accuracy: 0.9793\n",
      "Epoch 14/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.2375 - accuracy: 0.9757 - val_loss: 0.2369 - val_accuracy: 0.9704\n",
      "Epoch 15/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.2043 - accuracy: 0.9780 - val_loss: 0.2118 - val_accuracy: 0.9626\n",
      "Epoch 16/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1923 - accuracy: 0.9775 - val_loss: 0.1680 - val_accuracy: 0.9802\n",
      "Epoch 17/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1803 - accuracy: 0.9758 - val_loss: 0.2007 - val_accuracy: 0.9598\n",
      "Epoch 18/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1690 - accuracy: 0.9779 - val_loss: 0.1557 - val_accuracy: 0.9796\n",
      "Epoch 19/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1549 - accuracy: 0.9779 - val_loss: 0.1540 - val_accuracy: 0.9785\n",
      "Epoch 20/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1467 - accuracy: 0.9786 - val_loss: 0.1476 - val_accuracy: 0.9793\n",
      "Epoch 21/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1381 - accuracy: 0.9783 - val_loss: 0.1328 - val_accuracy: 0.9802\n",
      "Epoch 22/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1362 - accuracy: 0.9789 - val_loss: 0.1272 - val_accuracy: 0.9796\n",
      "Epoch 23/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1323 - accuracy: 0.9787 - val_loss: 0.1318 - val_accuracy: 0.9804\n",
      "Epoch 24/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1298 - accuracy: 0.9777 - val_loss: 0.1207 - val_accuracy: 0.9796\n",
      "Epoch 25/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1233 - accuracy: 0.9790 - val_loss: 0.1257 - val_accuracy: 0.9771\n",
      "Epoch 26/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1195 - accuracy: 0.9789 - val_loss: 0.1119 - val_accuracy: 0.9807\n",
      "Epoch 27/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1175 - accuracy: 0.9795 - val_loss: 0.1137 - val_accuracy: 0.9802\n",
      "Epoch 28/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1155 - accuracy: 0.9787 - val_loss: 0.1134 - val_accuracy: 0.9785\n",
      "Epoch 29/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1150 - accuracy: 0.9792 - val_loss: 0.1172 - val_accuracy: 0.9796\n",
      "Epoch 30/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1114 - accuracy: 0.9794 - val_loss: 0.1127 - val_accuracy: 0.9793\n",
      "Epoch 31/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1163 - accuracy: 0.9790 - val_loss: 0.1135 - val_accuracy: 0.9791\n",
      "Epoch 32/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1069 - accuracy: 0.9787 - val_loss: 0.1087 - val_accuracy: 0.9807\n",
      "Epoch 33/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1091 - accuracy: 0.9797 - val_loss: 0.1096 - val_accuracy: 0.9802\n",
      "Epoch 34/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1103 - accuracy: 0.9805 - val_loss: 0.1109 - val_accuracy: 0.9796\n",
      "Epoch 35/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1078 - accuracy: 0.9783 - val_loss: 0.1056 - val_accuracy: 0.9791\n",
      "Epoch 36/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1070 - accuracy: 0.9795 - val_loss: 0.1064 - val_accuracy: 0.9799\n",
      "Epoch 37/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1065 - accuracy: 0.9787 - val_loss: 0.1023 - val_accuracy: 0.9799\n",
      "Epoch 38/100\n",
      "1432/1432 [==============================] - 4s 2ms/step - loss: 0.1067 - accuracy: 0.9790 - val_loss: 0.1110 - val_accuracy: 0.9796\n",
      "Epoch 39/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1064 - accuracy: 0.9787 - val_loss: 0.1060 - val_accuracy: 0.9802\n",
      "Epoch 40/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1051 - accuracy: 0.9791 - val_loss: 0.1081 - val_accuracy: 0.9785\n",
      "Epoch 41/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1032 - accuracy: 0.9799 - val_loss: 0.1068 - val_accuracy: 0.9799\n",
      "Epoch 42/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1036 - accuracy: 0.9795 - val_loss: 0.1083 - val_accuracy: 0.9804\n",
      "Epoch 43/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1047 - accuracy: 0.9799 - val_loss: 0.1045 - val_accuracy: 0.9802\n",
      "Epoch 44/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1029 - accuracy: 0.9793 - val_loss: 0.1044 - val_accuracy: 0.9804\n",
      "Epoch 45/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1026 - accuracy: 0.9797 - val_loss: 0.1034 - val_accuracy: 0.9802\n",
      "Epoch 46/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1020 - accuracy: 0.9800 - val_loss: 0.1036 - val_accuracy: 0.9793\n",
      "Epoch 47/100\n",
      "1432/1432 [==============================] - 4s 2ms/step - loss: 0.1005 - accuracy: 0.9794 - val_loss: 0.1005 - val_accuracy: 0.9793\n",
      "Epoch 48/100\n",
      "1432/1432 [==============================] - 4s 2ms/step - loss: 0.1016 - accuracy: 0.9804 - val_loss: 0.1063 - val_accuracy: 0.9796\n",
      "Epoch 49/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0987 - accuracy: 0.9799 - val_loss: 0.1206 - val_accuracy: 0.9785\n",
      "Epoch 50/100\n",
      "1432/1432 [==============================] - 3s 2ms/step - loss: 0.1015 - accuracy: 0.9795 - val_loss: 0.1078 - val_accuracy: 0.9793\n",
      "Epoch 51/100\n",
      "1432/1432 [==============================] - 3s 2ms/step - loss: 0.1011 - accuracy: 0.9800 - val_loss: 0.1040 - val_accuracy: 0.9807\n",
      "Epoch 52/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1023 - accuracy: 0.9796 - val_loss: 0.1096 - val_accuracy: 0.9799\n",
      "Epoch 53/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0998 - accuracy: 0.9797 - val_loss: 0.1018 - val_accuracy: 0.9802\n",
      "Epoch 54/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0999 - accuracy: 0.9800 - val_loss: 0.1044 - val_accuracy: 0.9799\n",
      "Epoch 55/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0996 - accuracy: 0.9804 - val_loss: 0.1009 - val_accuracy: 0.9804\n",
      "Epoch 56/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0994 - accuracy: 0.9805 - val_loss: 0.1044 - val_accuracy: 0.9799\n",
      "Epoch 57/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0989 - accuracy: 0.9791 - val_loss: 0.1023 - val_accuracy: 0.9802\n",
      "Epoch 58/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0985 - accuracy: 0.9793 - val_loss: 0.1026 - val_accuracy: 0.9799\n",
      "Epoch 59/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0996 - accuracy: 0.9802 - val_loss: 0.1051 - val_accuracy: 0.9807\n",
      "Epoch 60/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0986 - accuracy: 0.9806 - val_loss: 0.1077 - val_accuracy: 0.9785\n",
      "Epoch 61/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0985 - accuracy: 0.9797 - val_loss: 0.1096 - val_accuracy: 0.9774\n",
      "Epoch 62/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0978 - accuracy: 0.9801 - val_loss: 0.1037 - val_accuracy: 0.9799\n",
      "Epoch 63/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0974 - accuracy: 0.9797 - val_loss: 0.1023 - val_accuracy: 0.9793\n",
      "Epoch 64/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0991 - accuracy: 0.9792 - val_loss: 0.1006 - val_accuracy: 0.9799\n",
      "Epoch 65/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0962 - accuracy: 0.9804 - val_loss: 0.1095 - val_accuracy: 0.9779\n",
      "Epoch 66/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0977 - accuracy: 0.9802 - val_loss: 0.0989 - val_accuracy: 0.9804\n",
      "Epoch 67/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0979 - accuracy: 0.9796 - val_loss: 0.1075 - val_accuracy: 0.9793\n",
      "Epoch 68/100\n",
      "1432/1432 [==============================] - 3s 2ms/step - loss: 0.0966 - accuracy: 0.9807 - val_loss: 0.1008 - val_accuracy: 0.9802\n",
      "Epoch 69/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0962 - accuracy: 0.9805 - val_loss: 0.1026 - val_accuracy: 0.9793\n",
      "Epoch 70/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0964 - accuracy: 0.9808 - val_loss: 0.1024 - val_accuracy: 0.9810\n",
      "Epoch 71/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0975 - accuracy: 0.9801 - val_loss: 0.1195 - val_accuracy: 0.9740\n",
      "Epoch 72/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0954 - accuracy: 0.9807 - val_loss: 0.1071 - val_accuracy: 0.9796\n",
      "Epoch 73/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0965 - accuracy: 0.9800 - val_loss: 0.1081 - val_accuracy: 0.9779\n",
      "Epoch 74/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0961 - accuracy: 0.9809 - val_loss: 0.1020 - val_accuracy: 0.9804\n",
      "Epoch 75/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0944 - accuracy: 0.9807 - val_loss: 0.1024 - val_accuracy: 0.9791\n",
      "Epoch 76/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0961 - accuracy: 0.9795 - val_loss: 0.1007 - val_accuracy: 0.9799\n",
      "Epoch 77/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0969 - accuracy: 0.9800 - val_loss: 0.1061 - val_accuracy: 0.9802\n",
      "Epoch 78/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0952 - accuracy: 0.9811 - val_loss: 0.1078 - val_accuracy: 0.9793\n",
      "Epoch 79/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0972 - accuracy: 0.9801 - val_loss: 0.1004 - val_accuracy: 0.9802\n",
      "Epoch 80/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0942 - accuracy: 0.9810 - val_loss: 0.1107 - val_accuracy: 0.9785\n",
      "Epoch 81/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0961 - accuracy: 0.9799 - val_loss: 0.1003 - val_accuracy: 0.9799\n",
      "Epoch 82/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0948 - accuracy: 0.9809 - val_loss: 0.0970 - val_accuracy: 0.9810\n",
      "Epoch 83/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0935 - accuracy: 0.9807 - val_loss: 0.0979 - val_accuracy: 0.9804\n",
      "Epoch 84/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0948 - accuracy: 0.9803 - val_loss: 0.1161 - val_accuracy: 0.9757\n",
      "Epoch 85/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0922 - accuracy: 0.9804 - val_loss: 0.1097 - val_accuracy: 0.9779\n",
      "Epoch 86/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0939 - accuracy: 0.9816 - val_loss: 0.1001 - val_accuracy: 0.9796\n",
      "Epoch 87/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0949 - accuracy: 0.9809 - val_loss: 0.0983 - val_accuracy: 0.9804\n",
      "Epoch 88/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0927 - accuracy: 0.9809 - val_loss: 0.1052 - val_accuracy: 0.9796\n",
      "Epoch 89/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0933 - accuracy: 0.9806 - val_loss: 0.1005 - val_accuracy: 0.9807\n",
      "Epoch 90/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0927 - accuracy: 0.9808 - val_loss: 0.1021 - val_accuracy: 0.9802\n",
      "Epoch 91/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0924 - accuracy: 0.9806 - val_loss: 0.1015 - val_accuracy: 0.9802\n",
      "Epoch 92/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0940 - accuracy: 0.9807 - val_loss: 0.1133 - val_accuracy: 0.9785\n",
      "Epoch 93/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0929 - accuracy: 0.9807 - val_loss: 0.1074 - val_accuracy: 0.9779\n",
      "Epoch 94/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0955 - accuracy: 0.9804 - val_loss: 0.1135 - val_accuracy: 0.9754\n",
      "Epoch 95/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0931 - accuracy: 0.9809 - val_loss: 0.1039 - val_accuracy: 0.9796\n",
      "Epoch 96/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0935 - accuracy: 0.9811 - val_loss: 0.1013 - val_accuracy: 0.9799\n",
      "Epoch 97/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0951 - accuracy: 0.9807 - val_loss: 0.1068 - val_accuracy: 0.9774\n",
      "Epoch 98/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0948 - accuracy: 0.9802 - val_loss: 0.1010 - val_accuracy: 0.9788\n",
      "Epoch 99/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0930 - accuracy: 0.9810 - val_loss: 0.1065 - val_accuracy: 0.9774\n",
      "Epoch 100/100\n",
      "1432/1432 [==============================] - 4s 2ms/step - loss: 0.0937 - accuracy: 0.9804 - val_loss: 0.0992 - val_accuracy: 0.9810\n",
      "Accuracy: 0.9790502786636353\n",
      " MSE: 0.15688163042068481\n"
     ]
    }
   ],
   "source": [
    "model10 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(512, activation='relu', input_shape=(X_trains.shape[1],)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.001),\n",
    "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.001, l2=0.001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.001),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.001, l2=0.001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.001),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.001, l2=0.001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.001),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.001, l2=0.001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.001),\n",
    "    tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.001, l2=0.001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.001),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adamax()\n",
    "model10.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 학습 자동 중단 설정\n",
    "rlrp = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=40, mode='min')\n",
    "\n",
    "# 모델 학습\n",
    "model10.fit(X_trains, y_train, epochs=100, batch_size=10, verbose=1, validation_data=(X_tests, y_test), callbacks=[rlrp])\n",
    "\n",
    "# 모델 평가\n",
    "y_pred = model9.predict(X_tests)\n",
    "    \n",
    "loss, accuracy = model9.evaluate(X_tests, y_test, verbose=0)\n",
    "print(f'Accuracy: {accuracy}\\n MSE: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "448/448 [==============================] - 3s 4ms/step - loss: 0.2071 - accuracy: 0.9529 - val_loss: 0.1055 - val_accuracy: 0.9765\n",
      "Epoch 2/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.1033 - accuracy: 0.9756 - val_loss: 0.0826 - val_accuracy: 0.9771\n",
      "Epoch 3/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0857 - accuracy: 0.9792 - val_loss: 0.0763 - val_accuracy: 0.9777\n",
      "Epoch 4/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0800 - accuracy: 0.9788 - val_loss: 0.0741 - val_accuracy: 0.9791\n",
      "Epoch 5/500\n",
      "448/448 [==============================] - 2s 3ms/step - loss: 0.0732 - accuracy: 0.9797 - val_loss: 0.0718 - val_accuracy: 0.9785\n",
      "Epoch 6/500\n",
      "448/448 [==============================] - 2s 3ms/step - loss: 0.0755 - accuracy: 0.9786 - val_loss: 0.0734 - val_accuracy: 0.9785\n",
      "Epoch 7/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0719 - accuracy: 0.9789 - val_loss: 0.0723 - val_accuracy: 0.9802\n",
      "Epoch 8/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0716 - accuracy: 0.9793 - val_loss: 0.0717 - val_accuracy: 0.9799\n",
      "Epoch 9/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0687 - accuracy: 0.9789 - val_loss: 0.0722 - val_accuracy: 0.9793\n",
      "Epoch 10/500\n",
      "448/448 [==============================] - 2s 4ms/step - loss: 0.0675 - accuracy: 0.9802 - val_loss: 0.0751 - val_accuracy: 0.9796\n",
      "Epoch 11/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0670 - accuracy: 0.9801 - val_loss: 0.0718 - val_accuracy: 0.9785\n",
      "Epoch 12/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0662 - accuracy: 0.9814 - val_loss: 0.0730 - val_accuracy: 0.9791\n",
      "Epoch 13/500\n",
      "448/448 [==============================] - 2s 3ms/step - loss: 0.0666 - accuracy: 0.9801 - val_loss: 0.0703 - val_accuracy: 0.9799\n",
      "Epoch 14/500\n",
      "448/448 [==============================] - 2s 4ms/step - loss: 0.0662 - accuracy: 0.9807 - val_loss: 0.0720 - val_accuracy: 0.9796\n",
      "Epoch 15/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0651 - accuracy: 0.9809 - val_loss: 0.0699 - val_accuracy: 0.9791\n",
      "Epoch 16/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0659 - accuracy: 0.9803 - val_loss: 0.0716 - val_accuracy: 0.9788\n",
      "Epoch 17/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0656 - accuracy: 0.9797 - val_loss: 0.0751 - val_accuracy: 0.9782\n",
      "Epoch 18/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0663 - accuracy: 0.9796 - val_loss: 0.0714 - val_accuracy: 0.9788\n",
      "Epoch 19/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0649 - accuracy: 0.9812 - val_loss: 0.0718 - val_accuracy: 0.9791\n",
      "Epoch 20/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0647 - accuracy: 0.9807 - val_loss: 0.0731 - val_accuracy: 0.9779\n",
      "Epoch 21/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0620 - accuracy: 0.9818 - val_loss: 0.0715 - val_accuracy: 0.9802\n",
      "Epoch 22/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0633 - accuracy: 0.9811 - val_loss: 0.0723 - val_accuracy: 0.9793\n",
      "Epoch 23/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0636 - accuracy: 0.9804 - val_loss: 0.0731 - val_accuracy: 0.9791\n",
      "Epoch 24/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0633 - accuracy: 0.9808 - val_loss: 0.0724 - val_accuracy: 0.9799\n",
      "Epoch 25/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0633 - accuracy: 0.9809 - val_loss: 0.0736 - val_accuracy: 0.9788\n",
      "Epoch 26/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0620 - accuracy: 0.9807 - val_loss: 0.0753 - val_accuracy: 0.9785\n",
      "Epoch 27/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0625 - accuracy: 0.9809 - val_loss: 0.0711 - val_accuracy: 0.9796\n",
      "Epoch 28/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0614 - accuracy: 0.9817 - val_loss: 0.0707 - val_accuracy: 0.9791\n",
      "Epoch 29/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0635 - accuracy: 0.9803 - val_loss: 0.0721 - val_accuracy: 0.9791\n",
      "Epoch 30/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0617 - accuracy: 0.9811 - val_loss: 0.0712 - val_accuracy: 0.9796\n",
      "Epoch 31/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0629 - accuracy: 0.9816 - val_loss: 0.0699 - val_accuracy: 0.9791\n",
      "Epoch 32/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0615 - accuracy: 0.9802 - val_loss: 0.0733 - val_accuracy: 0.9785\n",
      "Epoch 33/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0611 - accuracy: 0.9815 - val_loss: 0.0742 - val_accuracy: 0.9785\n",
      "Epoch 34/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0622 - accuracy: 0.9810 - val_loss: 0.0714 - val_accuracy: 0.9791\n",
      "Epoch 35/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0615 - accuracy: 0.9814 - val_loss: 0.0723 - val_accuracy: 0.9799\n",
      "Epoch 36/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0591 - accuracy: 0.9819 - val_loss: 0.0716 - val_accuracy: 0.9791\n",
      "Epoch 37/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0612 - accuracy: 0.9813 - val_loss: 0.0710 - val_accuracy: 0.9799\n",
      "Epoch 38/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0607 - accuracy: 0.9813 - val_loss: 0.0722 - val_accuracy: 0.9785\n",
      "Epoch 39/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0600 - accuracy: 0.9815 - val_loss: 0.0732 - val_accuracy: 0.9799\n",
      "Epoch 40/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0615 - accuracy: 0.9812 - val_loss: 0.0711 - val_accuracy: 0.9791\n",
      "Epoch 41/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0606 - accuracy: 0.9823 - val_loss: 0.0748 - val_accuracy: 0.9782\n",
      "Epoch 42/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0600 - accuracy: 0.9821 - val_loss: 0.0737 - val_accuracy: 0.9782\n",
      "Epoch 43/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0607 - accuracy: 0.9812 - val_loss: 0.0720 - val_accuracy: 0.9799\n",
      "Epoch 44/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0594 - accuracy: 0.9817 - val_loss: 0.0743 - val_accuracy: 0.9788\n",
      "Epoch 45/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0590 - accuracy: 0.9817 - val_loss: 0.0732 - val_accuracy: 0.9796\n",
      "Epoch 46/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0619 - accuracy: 0.9806 - val_loss: 0.0722 - val_accuracy: 0.9782\n",
      "Epoch 47/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0598 - accuracy: 0.9818 - val_loss: 0.0757 - val_accuracy: 0.9799\n",
      "Epoch 48/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0587 - accuracy: 0.9821 - val_loss: 0.0735 - val_accuracy: 0.9796\n",
      "Epoch 49/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0595 - accuracy: 0.9823 - val_loss: 0.0732 - val_accuracy: 0.9807\n",
      "Epoch 50/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0596 - accuracy: 0.9816 - val_loss: 0.0714 - val_accuracy: 0.9796\n",
      "Epoch 51/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0589 - accuracy: 0.9823 - val_loss: 0.0718 - val_accuracy: 0.9796\n",
      "Epoch 52/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0599 - accuracy: 0.9825 - val_loss: 0.0707 - val_accuracy: 0.9793\n",
      "Epoch 53/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0570 - accuracy: 0.9821 - val_loss: 0.0717 - val_accuracy: 0.9791\n",
      "Epoch 54/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0587 - accuracy: 0.9821 - val_loss: 0.0738 - val_accuracy: 0.9796\n",
      "Epoch 55/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0579 - accuracy: 0.9819 - val_loss: 0.0703 - val_accuracy: 0.9799\n",
      "Epoch 56/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0565 - accuracy: 0.9828 - val_loss: 0.0717 - val_accuracy: 0.9793\n",
      "Epoch 57/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0552 - accuracy: 0.9827 - val_loss: 0.0713 - val_accuracy: 0.9804\n",
      "Epoch 58/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0551 - accuracy: 0.9823 - val_loss: 0.0706 - val_accuracy: 0.9802\n",
      "Epoch 59/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0547 - accuracy: 0.9829 - val_loss: 0.0725 - val_accuracy: 0.9799\n",
      "Epoch 60/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0542 - accuracy: 0.9830 - val_loss: 0.0716 - val_accuracy: 0.9802\n",
      "Epoch 61/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0540 - accuracy: 0.9831 - val_loss: 0.0714 - val_accuracy: 0.9804\n",
      "Epoch 62/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0536 - accuracy: 0.9834 - val_loss: 0.0719 - val_accuracy: 0.9799\n",
      "Epoch 63/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0536 - accuracy: 0.9834 - val_loss: 0.0721 - val_accuracy: 0.9799\n",
      "Epoch 64/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0549 - accuracy: 0.9830 - val_loss: 0.0726 - val_accuracy: 0.9791\n",
      "Epoch 65/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0546 - accuracy: 0.9828 - val_loss: 0.0720 - val_accuracy: 0.9788\n",
      "Epoch 66/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0542 - accuracy: 0.9831 - val_loss: 0.0725 - val_accuracy: 0.9799\n",
      "Epoch 67/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0541 - accuracy: 0.9830 - val_loss: 0.0721 - val_accuracy: 0.9793\n",
      "Epoch 68/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0545 - accuracy: 0.9830 - val_loss: 0.0720 - val_accuracy: 0.9802\n",
      "Epoch 69/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0535 - accuracy: 0.9830 - val_loss: 0.0731 - val_accuracy: 0.9802\n",
      "Epoch 70/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0537 - accuracy: 0.9831 - val_loss: 0.0731 - val_accuracy: 0.9799\n",
      "Epoch 71/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0549 - accuracy: 0.9830 - val_loss: 0.0719 - val_accuracy: 0.9796\n",
      "Epoch 72/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0534 - accuracy: 0.9824 - val_loss: 0.0721 - val_accuracy: 0.9793\n",
      "Epoch 73/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0522 - accuracy: 0.9839 - val_loss: 0.0739 - val_accuracy: 0.9799\n",
      "Epoch 74/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0528 - accuracy: 0.9826 - val_loss: 0.0734 - val_accuracy: 0.9793\n",
      "Epoch 75/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0538 - accuracy: 0.9823 - val_loss: 0.0743 - val_accuracy: 0.9791\n",
      "Epoch 76/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0534 - accuracy: 0.9829 - val_loss: 0.0739 - val_accuracy: 0.9788\n",
      "Epoch 77/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0514 - accuracy: 0.9835 - val_loss: 0.0736 - val_accuracy: 0.9791\n",
      "Epoch 78/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0537 - accuracy: 0.9837 - val_loss: 0.0736 - val_accuracy: 0.9785\n",
      "Epoch 79/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0513 - accuracy: 0.9837 - val_loss: 0.0734 - val_accuracy: 0.9793\n",
      "Epoch 80/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0546 - accuracy: 0.9823 - val_loss: 0.0734 - val_accuracy: 0.9791\n",
      "Epoch 81/500\n",
      "448/448 [==============================] - 1s 3ms/step - loss: 0.0530 - accuracy: 0.9832 - val_loss: 0.0743 - val_accuracy: 0.9788\n",
      "Accuracy: 0.9787709712982178\n",
      " MSE: 0.07431156933307648\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model11 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu', input_shape=(X_trains.shape[1],)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.001),  # Dropout 추가\n",
    "    tf.keras.layers.Dense(128, activation='relu'),  # L1/L2 정규화 추가\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.001),  # Dropout 추가\n",
    "    tf.keras.layers.Dense(64, activation='relu'),  # L1/L2 정규화 추가\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),  # L1/L2 정규화 추가\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD()\n",
    "model11.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 학습 자동 중단 설정\n",
    "es = EarlyStopping(monitor='val_loss', patience=50, mode='min')\n",
    "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=40, mode='min')\n",
    "\n",
    "# 모델 학습\n",
    "model11.fit(X_trains, y_train, epochs=500, batch_size=32, verbose=1, \n",
    "          validation_data=(X_tests, y_test), callbacks=[es, rlrp])\n",
    "\n",
    "# 모델 평가\n",
    "y_pred = model11.predict(X_tests)\n",
    "    \n",
    "loss, accuracy = model11.evaluate(X_tests, y_test, verbose=0)\n",
    "print(f'Accuracy: {accuracy}\\n MSE: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2864/2864 [==============================] - 7s 2ms/step - loss: 0.1483 - accuracy: 0.9558 - val_loss: 0.0948 - val_accuracy: 0.9721\n",
      "Epoch 2/100\n",
      "2864/2864 [==============================] - 5s 2ms/step - loss: 0.0944 - accuracy: 0.9745 - val_loss: 0.0861 - val_accuracy: 0.9751\n",
      "Epoch 3/100\n",
      "2864/2864 [==============================] - 5s 2ms/step - loss: 0.0911 - accuracy: 0.9752 - val_loss: 0.0776 - val_accuracy: 0.9788\n",
      "Epoch 4/100\n",
      "2864/2864 [==============================] - 6s 2ms/step - loss: 0.0889 - accuracy: 0.9760 - val_loss: 0.0803 - val_accuracy: 0.9763\n",
      "Epoch 5/100\n",
      "2864/2864 [==============================] - 6s 2ms/step - loss: 0.0837 - accuracy: 0.9776 - val_loss: 0.0815 - val_accuracy: 0.9768\n",
      "Epoch 6/100\n",
      "2864/2864 [==============================] - 6s 2ms/step - loss: 0.0814 - accuracy: 0.9769 - val_loss: 0.0826 - val_accuracy: 0.9791\n",
      "Epoch 7/100\n",
      "2864/2864 [==============================] - 5s 2ms/step - loss: 0.0827 - accuracy: 0.9776 - val_loss: 0.0970 - val_accuracy: 0.9668\n",
      "Epoch 8/100\n",
      "2864/2864 [==============================] - 5s 2ms/step - loss: 0.0803 - accuracy: 0.9777 - val_loss: 0.1111 - val_accuracy: 0.9640\n",
      "Epoch 9/100\n",
      "2864/2864 [==============================] - 6s 2ms/step - loss: 0.0836 - accuracy: 0.9774 - val_loss: 0.1096 - val_accuracy: 0.9640\n",
      "Epoch 10/100\n",
      "2864/2864 [==============================] - 6s 2ms/step - loss: 0.0817 - accuracy: 0.9772 - val_loss: 0.1005 - val_accuracy: 0.9642\n",
      "Epoch 11/100\n",
      "2864/2864 [==============================] - 5s 2ms/step - loss: 0.0814 - accuracy: 0.9772 - val_loss: 0.1032 - val_accuracy: 0.9615\n",
      "Epoch 12/100\n",
      "2864/2864 [==============================] - 5s 2ms/step - loss: 0.0792 - accuracy: 0.9770 - val_loss: 0.1018 - val_accuracy: 0.9631\n",
      "Epoch 13/100\n",
      "2864/2864 [==============================] - 6s 2ms/step - loss: 0.0812 - accuracy: 0.9771 - val_loss: 0.2089 - val_accuracy: 0.9397\n",
      "Epoch 14/100\n",
      "2864/2864 [==============================] - 6s 2ms/step - loss: 0.0772 - accuracy: 0.9779 - val_loss: 0.0836 - val_accuracy: 0.9749\n",
      "Epoch 15/100\n",
      "2864/2864 [==============================] - 6s 2ms/step - loss: 0.0780 - accuracy: 0.9774 - val_loss: 0.0817 - val_accuracy: 0.9763\n",
      "Epoch 16/100\n",
      "2864/2864 [==============================] - 6s 2ms/step - loss: 0.0782 - accuracy: 0.9783 - val_loss: 0.1035 - val_accuracy: 0.9659\n",
      "Epoch 17/100\n",
      "2864/2864 [==============================] - 6s 2ms/step - loss: 0.0808 - accuracy: 0.9790 - val_loss: 0.0960 - val_accuracy: 0.9712\n",
      "Epoch 18/100\n",
      "2864/2864 [==============================] - 5s 2ms/step - loss: 0.0759 - accuracy: 0.9780 - val_loss: 0.0804 - val_accuracy: 0.9768\n",
      "Epoch 19/100\n",
      "2864/2864 [==============================] - 5s 2ms/step - loss: 0.0762 - accuracy: 0.9779 - val_loss: 0.0784 - val_accuracy: 0.9788\n",
      "Epoch 20/100\n",
      "2864/2864 [==============================] - 5s 2ms/step - loss: 0.0766 - accuracy: 0.9790 - val_loss: 0.0782 - val_accuracy: 0.9785\n",
      "Epoch 21/100\n",
      "2864/2864 [==============================] - 6s 2ms/step - loss: 0.0771 - accuracy: 0.9777 - val_loss: 0.0849 - val_accuracy: 0.9687\n",
      "Epoch 22/100\n",
      "2864/2864 [==============================] - 5s 2ms/step - loss: 0.0747 - accuracy: 0.9787 - val_loss: 0.0791 - val_accuracy: 0.9729\n",
      "Epoch 23/100\n",
      "2864/2864 [==============================] - 5s 2ms/step - loss: 0.0770 - accuracy: 0.9781 - val_loss: 0.0868 - val_accuracy: 0.9721\n",
      "Epoch 24/100\n",
      "2864/2864 [==============================] - 5s 2ms/step - loss: 0.0764 - accuracy: 0.9781 - val_loss: 0.0889 - val_accuracy: 0.9687\n",
      "Epoch 25/100\n",
      "2864/2864 [==============================] - 6s 2ms/step - loss: 0.0747 - accuracy: 0.9793 - val_loss: 0.0854 - val_accuracy: 0.9690\n",
      "Epoch 26/100\n",
      "2864/2864 [==============================] - 5s 2ms/step - loss: 0.0750 - accuracy: 0.9789 - val_loss: 0.0828 - val_accuracy: 0.9712\n",
      "Epoch 27/100\n",
      "2864/2864 [==============================] - 5s 2ms/step - loss: 0.0750 - accuracy: 0.9781 - val_loss: 0.0853 - val_accuracy: 0.9721\n",
      "Epoch 28/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0749 - accuracy: 0.9791 - val_loss: 0.1139 - val_accuracy: 0.9550\n",
      "Epoch 29/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0740 - accuracy: 0.9794 - val_loss: 0.1200 - val_accuracy: 0.9531\n",
      "Epoch 30/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0735 - accuracy: 0.9798 - val_loss: 0.0802 - val_accuracy: 0.9743\n",
      "Epoch 31/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0750 - accuracy: 0.9786 - val_loss: 0.0841 - val_accuracy: 0.9726\n",
      "Epoch 32/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0734 - accuracy: 0.9793 - val_loss: 0.0969 - val_accuracy: 0.9589\n",
      "Epoch 33/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0752 - accuracy: 0.9788 - val_loss: 0.0857 - val_accuracy: 0.9662\n",
      "Epoch 34/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0739 - accuracy: 0.9788 - val_loss: 0.0928 - val_accuracy: 0.9642\n",
      "Epoch 35/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0739 - accuracy: 0.9785 - val_loss: 0.0945 - val_accuracy: 0.9642\n",
      "Epoch 36/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0737 - accuracy: 0.9784 - val_loss: 0.1006 - val_accuracy: 0.9606\n",
      "Epoch 37/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0752 - accuracy: 0.9787 - val_loss: 0.0988 - val_accuracy: 0.9606\n",
      "Epoch 38/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0709 - accuracy: 0.9792 - val_loss: 0.1008 - val_accuracy: 0.9603\n",
      "Epoch 39/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0708 - accuracy: 0.9802 - val_loss: 0.0891 - val_accuracy: 0.9737\n",
      "Epoch 40/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0716 - accuracy: 0.9793 - val_loss: 0.0921 - val_accuracy: 0.9654\n",
      "Epoch 41/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0728 - accuracy: 0.9782 - val_loss: 0.0886 - val_accuracy: 0.9654\n",
      "Epoch 42/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0734 - accuracy: 0.9797 - val_loss: 0.0919 - val_accuracy: 0.9642\n",
      "Epoch 43/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0720 - accuracy: 0.9792 - val_loss: 0.0798 - val_accuracy: 0.9757\n",
      "Epoch 44/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0688 - accuracy: 0.9814 - val_loss: 0.0864 - val_accuracy: 0.9715\n",
      "Epoch 45/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0687 - accuracy: 0.9802 - val_loss: 0.0883 - val_accuracy: 0.9687\n",
      "Epoch 46/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0692 - accuracy: 0.9802 - val_loss: 0.0840 - val_accuracy: 0.9723\n",
      "Epoch 47/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0673 - accuracy: 0.9805 - val_loss: 0.0862 - val_accuracy: 0.9718\n",
      "Epoch 48/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0683 - accuracy: 0.9806 - val_loss: 0.0846 - val_accuracy: 0.9732\n",
      "Epoch 49/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0696 - accuracy: 0.9804 - val_loss: 0.0822 - val_accuracy: 0.9760\n",
      "Epoch 50/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0681 - accuracy: 0.9810 - val_loss: 0.0917 - val_accuracy: 0.9651\n",
      "Epoch 51/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0693 - accuracy: 0.9801 - val_loss: 0.0854 - val_accuracy: 0.9715\n",
      "Epoch 52/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0681 - accuracy: 0.9806 - val_loss: 0.0892 - val_accuracy: 0.9682\n",
      "Epoch 53/100\n",
      "2864/2864 [==============================] - 4s 1ms/step - loss: 0.0667 - accuracy: 0.9811 - val_loss: 0.0899 - val_accuracy: 0.9687\n",
      "Accuracy: 0.9687150716781616\n",
      " MSE: 0.08986072987318039\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model11 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu', input_shape=(X_trains.shape[1],)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.001),  # Dropout 추가\n",
    "    tf.keras.layers.Dense(128, activation='relu'),  # L1/L2 정규화 추가\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.001),  # Dropout 추가\n",
    "    tf.keras.layers.Dense(64, activation='relu'),  # L1/L2 정규화 추가\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),  # L1/L2 정규화 추가\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD()\n",
    "model11.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 학습 자동 중단 설정\n",
    "es = EarlyStopping(monitor='val_loss', patience=50, mode='min')\n",
    "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=40, mode='min')\n",
    "\n",
    "# 모델 학습\n",
    "model11.fit(X_trains, y_train, epochs=100, batch_size=5, verbose=1, \n",
    "          validation_data=(X_tests, y_test), callbacks=[es, rlrp])\n",
    "\n",
    "# 모델 평가\n",
    "y_pred = model11.predict(X_tests)\n",
    "    \n",
    "loss, accuracy = model11.evaluate(X_tests, y_test, verbose=0)\n",
    "print(f'Accuracy: {accuracy}\\n MSE: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1432/1432 [==============================] - 6s 3ms/step - loss: 1.1287 - accuracy: 0.9468 - val_loss: 0.9861 - val_accuracy: 0.9768\n",
      "Epoch 2/100\n",
      "1432/1432 [==============================] - 5s 4ms/step - loss: 0.9597 - accuracy: 0.9747 - val_loss: 0.8966 - val_accuracy: 0.9807\n",
      "Epoch 3/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.8664 - accuracy: 0.9753 - val_loss: 0.8115 - val_accuracy: 0.9796\n",
      "Epoch 4/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.7692 - accuracy: 0.9776 - val_loss: 0.7246 - val_accuracy: 0.9771\n",
      "Epoch 5/100\n",
      "1432/1432 [==============================] - 5s 4ms/step - loss: 0.6808 - accuracy: 0.9767 - val_loss: 0.6360 - val_accuracy: 0.9791\n",
      "Epoch 6/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.6014 - accuracy: 0.9765 - val_loss: 0.5704 - val_accuracy: 0.9763\n",
      "Epoch 7/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.5311 - accuracy: 0.9783 - val_loss: 0.5007 - val_accuracy: 0.9751\n",
      "Epoch 8/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.4667 - accuracy: 0.9797 - val_loss: 0.4389 - val_accuracy: 0.9788\n",
      "Epoch 9/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.4164 - accuracy: 0.9780 - val_loss: 0.3959 - val_accuracy: 0.9785\n",
      "Epoch 10/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.3737 - accuracy: 0.9779 - val_loss: 0.3564 - val_accuracy: 0.9777\n",
      "Epoch 11/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.3403 - accuracy: 0.9795 - val_loss: 0.3336 - val_accuracy: 0.9771\n",
      "Epoch 12/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.3106 - accuracy: 0.9775 - val_loss: 0.3021 - val_accuracy: 0.9779\n",
      "Epoch 13/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.2884 - accuracy: 0.9788 - val_loss: 0.2790 - val_accuracy: 0.9791\n",
      "Epoch 14/100\n",
      "1432/1432 [==============================] - 5s 4ms/step - loss: 0.2717 - accuracy: 0.9785 - val_loss: 0.2675 - val_accuracy: 0.9777\n",
      "Epoch 15/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.2523 - accuracy: 0.9782 - val_loss: 0.2499 - val_accuracy: 0.9779\n",
      "Epoch 16/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.2361 - accuracy: 0.9792 - val_loss: 0.2425 - val_accuracy: 0.9737\n",
      "Epoch 17/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.2241 - accuracy: 0.9783 - val_loss: 0.2259 - val_accuracy: 0.9785\n",
      "Epoch 18/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.2114 - accuracy: 0.9790 - val_loss: 0.2120 - val_accuracy: 0.9782\n",
      "Epoch 19/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.2001 - accuracy: 0.9789 - val_loss: 0.2002 - val_accuracy: 0.9791\n",
      "Epoch 20/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1972 - accuracy: 0.9787 - val_loss: 0.1970 - val_accuracy: 0.9779\n",
      "Epoch 21/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1871 - accuracy: 0.9788 - val_loss: 0.1835 - val_accuracy: 0.9782\n",
      "Epoch 22/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1840 - accuracy: 0.9791 - val_loss: 0.1809 - val_accuracy: 0.9791\n",
      "Epoch 23/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1746 - accuracy: 0.9785 - val_loss: 0.1921 - val_accuracy: 0.9751\n",
      "Epoch 24/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.1707 - accuracy: 0.9786 - val_loss: 0.1723 - val_accuracy: 0.9777\n",
      "Epoch 25/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1630 - accuracy: 0.9799 - val_loss: 0.1681 - val_accuracy: 0.9793\n",
      "Epoch 26/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1600 - accuracy: 0.9786 - val_loss: 0.2006 - val_accuracy: 0.9603\n",
      "Epoch 27/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1547 - accuracy: 0.9791 - val_loss: 0.1549 - val_accuracy: 0.9804\n",
      "Epoch 28/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1521 - accuracy: 0.9783 - val_loss: 0.1537 - val_accuracy: 0.9802\n",
      "Epoch 29/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.1492 - accuracy: 0.9797 - val_loss: 0.1452 - val_accuracy: 0.9799\n",
      "Epoch 30/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1419 - accuracy: 0.9792 - val_loss: 0.1529 - val_accuracy: 0.9765\n",
      "Epoch 31/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1403 - accuracy: 0.9797 - val_loss: 0.1393 - val_accuracy: 0.9788\n",
      "Epoch 32/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1361 - accuracy: 0.9790 - val_loss: 0.1409 - val_accuracy: 0.9777\n",
      "Epoch 33/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1346 - accuracy: 0.9786 - val_loss: 0.1335 - val_accuracy: 0.9802\n",
      "Epoch 34/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.1329 - accuracy: 0.9787 - val_loss: 0.1285 - val_accuracy: 0.9796\n",
      "Epoch 35/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.1271 - accuracy: 0.9788 - val_loss: 0.1360 - val_accuracy: 0.9774\n",
      "Epoch 36/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1258 - accuracy: 0.9801 - val_loss: 0.1402 - val_accuracy: 0.9774\n",
      "Epoch 37/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1248 - accuracy: 0.9793 - val_loss: 0.1247 - val_accuracy: 0.9796\n",
      "Epoch 38/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.1227 - accuracy: 0.9800 - val_loss: 0.1227 - val_accuracy: 0.9796\n",
      "Epoch 39/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1191 - accuracy: 0.9794 - val_loss: 0.1345 - val_accuracy: 0.9774\n",
      "Epoch 40/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1161 - accuracy: 0.9794 - val_loss: 0.1230 - val_accuracy: 0.9788\n",
      "Epoch 41/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1125 - accuracy: 0.9799 - val_loss: 0.1156 - val_accuracy: 0.9799\n",
      "Epoch 42/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.1110 - accuracy: 0.9799 - val_loss: 0.1248 - val_accuracy: 0.9768\n",
      "Epoch 43/100\n",
      "1432/1432 [==============================] - 5s 4ms/step - loss: 0.1111 - accuracy: 0.9800 - val_loss: 0.1087 - val_accuracy: 0.9799\n",
      "Epoch 44/100\n",
      "1432/1432 [==============================] - 5s 4ms/step - loss: 0.1090 - accuracy: 0.9797 - val_loss: 0.1146 - val_accuracy: 0.9791\n",
      "Epoch 45/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.1059 - accuracy: 0.9810 - val_loss: 0.1279 - val_accuracy: 0.9771\n",
      "Epoch 46/100\n",
      "1432/1432 [==============================] - 5s 4ms/step - loss: 0.1040 - accuracy: 0.9797 - val_loss: 0.1073 - val_accuracy: 0.9791\n",
      "Epoch 47/100\n",
      "1432/1432 [==============================] - 5s 4ms/step - loss: 0.1041 - accuracy: 0.9807 - val_loss: 0.1231 - val_accuracy: 0.9788\n",
      "Epoch 48/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.1062 - accuracy: 0.9802 - val_loss: 0.1068 - val_accuracy: 0.9793\n",
      "Epoch 49/100\n",
      "1432/1432 [==============================] - 5s 4ms/step - loss: 0.1030 - accuracy: 0.9801 - val_loss: 0.1344 - val_accuracy: 0.9763\n",
      "Epoch 50/100\n",
      "1432/1432 [==============================] - 5s 4ms/step - loss: 0.1012 - accuracy: 0.9803 - val_loss: 0.1483 - val_accuracy: 0.9679\n",
      "Epoch 51/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.1011 - accuracy: 0.9793 - val_loss: 0.1063 - val_accuracy: 0.9796\n",
      "Epoch 52/100\n",
      "1432/1432 [==============================] - 5s 4ms/step - loss: 0.0981 - accuracy: 0.9802 - val_loss: 0.1026 - val_accuracy: 0.9791\n",
      "Epoch 53/100\n",
      "1432/1432 [==============================] - 5s 4ms/step - loss: 0.0978 - accuracy: 0.9804 - val_loss: 0.1065 - val_accuracy: 0.9799\n",
      "Epoch 54/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0977 - accuracy: 0.9803 - val_loss: 0.1256 - val_accuracy: 0.9707\n",
      "Epoch 55/100\n",
      "1432/1432 [==============================] - 5s 4ms/step - loss: 0.0951 - accuracy: 0.9798 - val_loss: 0.1004 - val_accuracy: 0.9793\n",
      "Epoch 56/100\n",
      "1432/1432 [==============================] - 5s 4ms/step - loss: 0.0957 - accuracy: 0.9801 - val_loss: 0.1196 - val_accuracy: 0.9777\n",
      "Epoch 57/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0951 - accuracy: 0.9796 - val_loss: 0.1021 - val_accuracy: 0.9793\n",
      "Epoch 58/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0961 - accuracy: 0.9795 - val_loss: 0.1071 - val_accuracy: 0.9788\n",
      "Epoch 59/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0933 - accuracy: 0.9799 - val_loss: 0.1010 - val_accuracy: 0.9799\n",
      "Epoch 60/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0920 - accuracy: 0.9802 - val_loss: 0.1053 - val_accuracy: 0.9779\n",
      "Epoch 61/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0923 - accuracy: 0.9809 - val_loss: 0.1127 - val_accuracy: 0.9774\n",
      "Epoch 62/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0902 - accuracy: 0.9806 - val_loss: 0.0971 - val_accuracy: 0.9802\n",
      "Epoch 63/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0897 - accuracy: 0.9802 - val_loss: 0.0954 - val_accuracy: 0.9799\n",
      "Epoch 64/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0893 - accuracy: 0.9804 - val_loss: 0.0996 - val_accuracy: 0.9802\n",
      "Epoch 65/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0890 - accuracy: 0.9802 - val_loss: 0.0943 - val_accuracy: 0.9799\n",
      "Epoch 66/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0907 - accuracy: 0.9796 - val_loss: 0.1117 - val_accuracy: 0.9788\n",
      "Epoch 67/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0869 - accuracy: 0.9808 - val_loss: 0.1101 - val_accuracy: 0.9768\n",
      "Epoch 68/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0898 - accuracy: 0.9800 - val_loss: 0.0915 - val_accuracy: 0.9804\n",
      "Epoch 69/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0861 - accuracy: 0.9802 - val_loss: 0.1089 - val_accuracy: 0.9760\n",
      "Epoch 70/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0862 - accuracy: 0.9804 - val_loss: 0.0937 - val_accuracy: 0.9799\n",
      "Epoch 71/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0853 - accuracy: 0.9809 - val_loss: 0.0932 - val_accuracy: 0.9802\n",
      "Epoch 72/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0851 - accuracy: 0.9813 - val_loss: 0.0902 - val_accuracy: 0.9799\n",
      "Epoch 73/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0860 - accuracy: 0.9800 - val_loss: 0.0899 - val_accuracy: 0.9799\n",
      "Epoch 74/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0836 - accuracy: 0.9809 - val_loss: 0.0931 - val_accuracy: 0.9774\n",
      "Epoch 75/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0856 - accuracy: 0.9804 - val_loss: 0.0950 - val_accuracy: 0.9804\n",
      "Epoch 76/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0843 - accuracy: 0.9804 - val_loss: 0.1048 - val_accuracy: 0.9791\n",
      "Epoch 77/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0839 - accuracy: 0.9809 - val_loss: 0.0977 - val_accuracy: 0.9796\n",
      "Epoch 78/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0815 - accuracy: 0.9804 - val_loss: 0.0929 - val_accuracy: 0.9788\n",
      "Epoch 79/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0826 - accuracy: 0.9805 - val_loss: 0.1015 - val_accuracy: 0.9785\n",
      "Epoch 80/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0825 - accuracy: 0.9809 - val_loss: 0.1183 - val_accuracy: 0.9757\n",
      "Epoch 81/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0818 - accuracy: 0.9816 - val_loss: 0.0872 - val_accuracy: 0.9802\n",
      "Epoch 82/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0829 - accuracy: 0.9817 - val_loss: 0.0882 - val_accuracy: 0.9793\n",
      "Epoch 83/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0819 - accuracy: 0.9800 - val_loss: 0.0911 - val_accuracy: 0.9804\n",
      "Epoch 84/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0805 - accuracy: 0.9816 - val_loss: 0.0918 - val_accuracy: 0.9788\n",
      "Epoch 85/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0824 - accuracy: 0.9804 - val_loss: 0.0921 - val_accuracy: 0.9799\n",
      "Epoch 86/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0820 - accuracy: 0.9802 - val_loss: 0.0905 - val_accuracy: 0.9804\n",
      "Epoch 87/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0809 - accuracy: 0.9809 - val_loss: 0.0980 - val_accuracy: 0.9796\n",
      "Epoch 88/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0807 - accuracy: 0.9808 - val_loss: 0.0899 - val_accuracy: 0.9793\n",
      "Epoch 89/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0809 - accuracy: 0.9801 - val_loss: 0.0874 - val_accuracy: 0.9810\n",
      "Epoch 90/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0780 - accuracy: 0.9818 - val_loss: 0.0899 - val_accuracy: 0.9802\n",
      "Epoch 91/100\n",
      "1432/1432 [==============================] - 4s 3ms/step - loss: 0.0802 - accuracy: 0.9811 - val_loss: 0.0877 - val_accuracy: 0.9813\n",
      "Epoch 92/100\n",
      "1432/1432 [==============================] - 5s 4ms/step - loss: 0.0792 - accuracy: 0.9807 - val_loss: 0.0880 - val_accuracy: 0.9799\n",
      "Epoch 93/100\n",
      "1432/1432 [==============================] - 5s 4ms/step - loss: 0.0802 - accuracy: 0.9809 - val_loss: 0.0954 - val_accuracy: 0.9793\n",
      "Epoch 94/100\n",
      "1432/1432 [==============================] - 5s 4ms/step - loss: 0.0793 - accuracy: 0.9807 - val_loss: 0.0932 - val_accuracy: 0.9793\n",
      "Epoch 95/100\n",
      "1432/1432 [==============================] - 5s 3ms/step - loss: 0.0794 - accuracy: 0.9811 - val_loss: 0.0935 - val_accuracy: 0.9796\n",
      "Epoch 96/100\n",
      "1432/1432 [==============================] - 5s 4ms/step - loss: 0.0781 - accuracy: 0.9818 - val_loss: 0.0939 - val_accuracy: 0.9785\n",
      "Epoch 97/100\n",
      "1432/1432 [==============================] - 6s 4ms/step - loss: 0.0778 - accuracy: 0.9820 - val_loss: 0.0861 - val_accuracy: 0.9793\n",
      "Epoch 98/100\n",
      "1432/1432 [==============================] - 6s 4ms/step - loss: 0.0793 - accuracy: 0.9813 - val_loss: 0.0847 - val_accuracy: 0.9804\n",
      "Epoch 99/100\n",
      "1432/1432 [==============================] - 6s 4ms/step - loss: 0.0791 - accuracy: 0.9803 - val_loss: 0.0841 - val_accuracy: 0.9807\n",
      "Epoch 100/100\n",
      "1432/1432 [==============================] - 6s 5ms/step - loss: 0.0798 - accuracy: 0.9807 - val_loss: 0.0958 - val_accuracy: 0.9782\n",
      "Accuracy: 0.9790502786636353\n",
      " MSE: 0.15688163042068481\n"
     ]
    }
   ],
   "source": [
    "model15 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(512, activation='relu', input_shape=(X_trains.shape[1],)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.0001),\n",
    "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.0001, l2=0.0001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.0001),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.0001, l2=0.0001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.0001),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.0001, l2=0.0001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.0001),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.0001, l2=0.0001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.0001),\n",
    "    tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.0001, l2=0.0001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.0001),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adamax()\n",
    "model15.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 학습 자동 중단 설정\n",
    "rlrp = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=40, mode='min')\n",
    "\n",
    "# 모델 학습\n",
    "model15.fit(X_trains, y_train, epochs=100, batch_size=10, verbose=1, validation_data=(X_tests, y_test), callbacks=[rlrp])\n",
    "\n",
    "# 모델 평가\n",
    "y_pred = model9.predict(X_tests)\n",
    "    \n",
    "loss, accuracy = model9.evaluate(X_tests, y_test, verbose=0)\n",
    "print(f'Accuracy: {accuracy}\\n MSE: {loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
