{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1552, 27) (389, 27) (1552,) (389,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def load_dataset(csv_path, TRAIN_RATIO=0.8):\n",
    "    \n",
    "    global X, y, X_train, X_test, y_train, y_test, df\n",
    "    \n",
    "    # 데이터셋 로드\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # 인코딩 방식 변경\n",
    "    # idxmax 함수는 각 행의 최대값을 가진 열의 인덱스를 반환한다. 따라서 원핫인코딩된 피쳐를 하나의 카테고리 변수로 복원할 수 있음\n",
    "    df['Fault'] = df[['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']].idxmax(axis=1)\n",
    "    \n",
    "    # 라벨 인코딩(원핫인코딩보다 차원수가 줄어드는 효과)\n",
    "    encoder = LabelEncoder()\n",
    "    df['Fault'] = encoder.fit_transform(df['Fault'])\n",
    "    \n",
    "    # 학습 데이터 분리\n",
    "    X = df.drop(['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults', 'Fault'], axis=1)\n",
    "    y = df['Fault']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=TRAIN_RATIO, random_state = 83)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "csv_path = '/mnt/c/Users/k10dh/AppData/Local/Packages/CanonicalGroupLimited.Ubuntu_79rhkp1fndgsc/k10dh/TeamProject/TeamProject1/KDH/Dataset/mulit_classification_data.csv'\n",
    "X_train, X_test, y_train, y_test = load_dataset(csv_path)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1551, 18) (388, 18) (1551,) (388,)\n"
     ]
    }
   ],
   "source": [
    "# 특성공학\n",
    "# 이상치 제거\n",
    "df = df[~((df['Pixels_Areas'] > 35000) |\n",
    "          (df['X_Perimeter'] > 2000) |\n",
    "          (df['Y_Perimeter'] > 2500) |\n",
    "          (df['Sum_of_Luminosity'] > 0.5e7))]\n",
    "\n",
    "# 'TypeOfSteel_A300' + 'TypeOfSteel_400' = 'TypeOfSteel' (A300이면 1)\n",
    "df['TypeOfSteel'] = df['TypeOfSteel_A300']\n",
    "df.drop(['TypeOfSteel_A300', 'TypeOfSteel_A400'], axis=1, inplace=True)\n",
    "\n",
    "# 'Minimum_of_Luminosity' + 'Maximum_of_Luminosity' = 'Mean_of_Luminosity'\n",
    "df['Mean_of_Luminosity'] = (df['Minimum_of_Luminosity'] + df['Maximum_of_Luminosity']) / 2\n",
    "df.drop(['Minimum_of_Luminosity', 'Maximum_of_Luminosity'], axis=1, inplace=True)\n",
    "\n",
    "# 'X_Minimum' + 'X_Maximum' = 'X_Mean'\n",
    "df['X_Mean'] = (df['X_Minimum'] + df['X_Maximum']) / 2\n",
    "df.drop(['X_Minimum', 'X_Maximum'], axis=1, inplace=True)\n",
    "\n",
    "# 'Y_Minimum' + 'Y_Maximum' = 'Y_Mean'\n",
    "df['Y_Mean'] = (df['Y_Minimum'] + df['Y_Maximum']) / 2\n",
    "df.drop(['Y_Minimum', 'Y_Maximum'], axis=1, inplace=True)\n",
    "\n",
    "# 'Log_X_Index', 'Log_Y_Index' 제거\n",
    "df.drop(['Log_X_Index', 'Log_Y_Index'], axis=1, inplace=True)\n",
    "\n",
    "# 상관관계가 높은 feature 확인 (절대값 0.9이상)\n",
    "# 'X_Perimeter' + 'Pixels_Areas'\n",
    "# 'Y_Perimeter' + 'X_Perimeter'\n",
    "# 'Sum_of_Luminosity' + 'Pixels_Areas' + 'X_Perimeter'\n",
    "df.drop(['X_Perimeter', 'Y_Perimeter'], axis=1, inplace=True)\n",
    "\n",
    "# 로그 스케일링\n",
    "# df['Log_Pixels_Areas'] = np.log(df['Pixels_Areas']) Sum_of_Luminosity만 활용\n",
    "df['Log_Sum_of_Luminosity'] = np.log(df['Sum_of_Luminosity'])\n",
    "df['Log_Y_Mean'] = np.log(df['Y_Mean'])\n",
    "df.drop(['Pixels_Areas', 'Sum_of_Luminosity', 'Y_Mean'], axis=1, inplace=True)\n",
    "\n",
    "# 학습 데이터 분리\n",
    "X = df.drop(['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults', 'Fault'], axis=1)\n",
    "y = df['Fault']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state = 83)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length_of_Conveyer</th>\n",
       "      <th>Steel_Plate_Thickness</th>\n",
       "      <th>Edges_Index</th>\n",
       "      <th>Empty_Index</th>\n",
       "      <th>Square_Index</th>\n",
       "      <th>Outside_X_Index</th>\n",
       "      <th>Edges_X_Index</th>\n",
       "      <th>Edges_Y_Index</th>\n",
       "      <th>Outside_Global_Index</th>\n",
       "      <th>LogOfAreas</th>\n",
       "      <th>...</th>\n",
       "      <th>Stains</th>\n",
       "      <th>Dirtiness</th>\n",
       "      <th>Bumps</th>\n",
       "      <th>Other_Faults</th>\n",
       "      <th>Fault</th>\n",
       "      <th>TypeOfSteel</th>\n",
       "      <th>Mean_of_Luminosity</th>\n",
       "      <th>X_Mean</th>\n",
       "      <th>Log_Sum_of_Luminosity</th>\n",
       "      <th>Log_Y_Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1939.000000</td>\n",
       "      <td>1939.000000</td>\n",
       "      <td>1939.000000</td>\n",
       "      <td>1939.000000</td>\n",
       "      <td>1939.000000</td>\n",
       "      <td>1939.000000</td>\n",
       "      <td>1939.000000</td>\n",
       "      <td>1939.000000</td>\n",
       "      <td>1939.000000</td>\n",
       "      <td>1939.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1939.000000</td>\n",
       "      <td>1939.000000</td>\n",
       "      <td>1939.000000</td>\n",
       "      <td>1939.00000</td>\n",
       "      <td>1939.000000</td>\n",
       "      <td>1939.000000</td>\n",
       "      <td>1939.000000</td>\n",
       "      <td>1939.000000</td>\n",
       "      <td>1939.000000</td>\n",
       "      <td>1939.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1459.260959</td>\n",
       "      <td>78.777720</td>\n",
       "      <td>0.332032</td>\n",
       "      <td>0.413977</td>\n",
       "      <td>0.571184</td>\n",
       "      <td>0.033288</td>\n",
       "      <td>0.611075</td>\n",
       "      <td>0.813561</td>\n",
       "      <td>0.575297</td>\n",
       "      <td>2.489928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037133</td>\n",
       "      <td>0.028365</td>\n",
       "      <td>0.207323</td>\n",
       "      <td>0.34657</td>\n",
       "      <td>2.569881</td>\n",
       "      <td>0.400722</td>\n",
       "      <td>107.402269</td>\n",
       "      <td>595.072718</td>\n",
       "      <td>10.426846</td>\n",
       "      <td>13.743778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>144.616025</td>\n",
       "      <td>55.100383</td>\n",
       "      <td>0.299704</td>\n",
       "      <td>0.136789</td>\n",
       "      <td>0.270839</td>\n",
       "      <td>0.058947</td>\n",
       "      <td>0.242797</td>\n",
       "      <td>0.234216</td>\n",
       "      <td>0.482408</td>\n",
       "      <td>0.785543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189135</td>\n",
       "      <td>0.166057</td>\n",
       "      <td>0.405494</td>\n",
       "      <td>0.47600</td>\n",
       "      <td>1.763241</td>\n",
       "      <td>0.490171</td>\n",
       "      <td>21.772871</td>\n",
       "      <td>507.670909</td>\n",
       "      <td>1.793217</td>\n",
       "      <td>1.241757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1227.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.521461</td>\n",
       "      <td>8.812546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1358.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.315750</td>\n",
       "      <td>0.361650</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.412800</td>\n",
       "      <td>0.597100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.924300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>126.500000</td>\n",
       "      <td>9.160835</td>\n",
       "      <td>13.060777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1364.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.227800</td>\n",
       "      <td>0.412100</td>\n",
       "      <td>0.558000</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.636400</td>\n",
       "      <td>0.947400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.238000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>107.500000</td>\n",
       "      <td>451.500000</td>\n",
       "      <td>9.862092</td>\n",
       "      <td>13.997623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1650.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.575500</td>\n",
       "      <td>0.501400</td>\n",
       "      <td>0.818200</td>\n",
       "      <td>0.023350</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.907950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>119.500000</td>\n",
       "      <td>1061.000000</td>\n",
       "      <td>11.315616</td>\n",
       "      <td>14.595897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1794.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>0.995200</td>\n",
       "      <td>0.927500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.406100</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>211.500000</td>\n",
       "      <td>1709.000000</td>\n",
       "      <td>14.934447</td>\n",
       "      <td>16.379512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Length_of_Conveyer  Steel_Plate_Thickness  Edges_Index  Empty_Index   \n",
       "count         1939.000000            1939.000000  1939.000000  1939.000000  \\\n",
       "mean          1459.260959              78.777720     0.332032     0.413977   \n",
       "std            144.616025              55.100383     0.299704     0.136789   \n",
       "min           1227.000000              40.000000     0.000000     0.000000   \n",
       "25%           1358.000000              40.000000     0.060400     0.315750   \n",
       "50%           1364.000000              70.000000     0.227800     0.412100   \n",
       "75%           1650.000000              80.000000     0.575500     0.501400   \n",
       "max           1794.000000             300.000000     0.995200     0.927500   \n",
       "\n",
       "       Square_Index  Outside_X_Index  Edges_X_Index  Edges_Y_Index   \n",
       "count   1939.000000      1939.000000    1939.000000    1939.000000  \\\n",
       "mean       0.571184         0.033288       0.611075       0.813561   \n",
       "std        0.270839         0.058947       0.242797       0.234216   \n",
       "min        0.009000         0.001500       0.064500       0.048400   \n",
       "25%        0.361650         0.006600       0.412800       0.597100   \n",
       "50%        0.558000         0.010100       0.636400       0.947400   \n",
       "75%        0.818200         0.023350       0.800000       1.000000   \n",
       "max        1.000000         0.875900       1.000000       1.000000   \n",
       "\n",
       "       Outside_Global_Index   LogOfAreas  ...       Stains    Dirtiness   \n",
       "count           1939.000000  1939.000000  ...  1939.000000  1939.000000  \\\n",
       "mean               0.575297     2.489928  ...     0.037133     0.028365   \n",
       "std                0.482408     0.785543  ...     0.189135     0.166057   \n",
       "min                0.000000     0.301000  ...     0.000000     0.000000   \n",
       "25%                0.000000     1.924300  ...     0.000000     0.000000   \n",
       "50%                1.000000     2.238000  ...     0.000000     0.000000   \n",
       "75%                1.000000     2.907950  ...     0.000000     0.000000   \n",
       "max                1.000000     4.406100  ...     1.000000     1.000000   \n",
       "\n",
       "             Bumps  Other_Faults        Fault  TypeOfSteel   \n",
       "count  1939.000000    1939.00000  1939.000000  1939.000000  \\\n",
       "mean      0.207323       0.34657     2.569881     0.400722   \n",
       "std       0.405494       0.47600     1.763241     0.490171   \n",
       "min       0.000000       0.00000     0.000000     0.000000   \n",
       "25%       0.000000       0.00000     2.000000     0.000000   \n",
       "50%       0.000000       0.00000     3.000000     0.000000   \n",
       "75%       0.000000       1.00000     3.000000     1.000000   \n",
       "max       1.000000       1.00000     6.000000     1.000000   \n",
       "\n",
       "       Mean_of_Luminosity       X_Mean  Log_Sum_of_Luminosity   Log_Y_Mean  \n",
       "count         1939.000000  1939.000000            1939.000000  1939.000000  \n",
       "mean           107.402269   595.072718              10.426846    13.743778  \n",
       "std             21.772871   507.670909               1.793217     1.241757  \n",
       "min             18.500000     2.000000               5.521461     8.812546  \n",
       "25%             91.000000   126.500000               9.160835    13.060777  \n",
       "50%            107.500000   451.500000               9.862092    13.997623  \n",
       "75%            119.500000  1061.000000              11.315616    14.595897  \n",
       "max            211.500000  1709.000000              14.934447    16.379512  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 데이터 크기 X_train: (1551, 18)\n",
      "원본 데이터 크기 y_train: (1551,) \n",
      "\n",
      "원본 데이터 '1' 개수: 43\n",
      "원본 데이터 '0' 개수: 318 \n",
      "\n",
      "샘플링 데이터 크기 X_train: (3710, 18)\n",
      "샘플링 데이터 크기 y_train: (3710,) \n",
      "\n",
      "샘플링 데이터 '1' 개수: 530\n",
      "샘플링 데이터 '0' 개수: 530\n"
     ]
    }
   ],
   "source": [
    "# 오버샘플링(학습데이터만)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# 오버샘플링 전 클래스 분포 확인\n",
    "print('원본 데이터 크기 X_train: {}'.format(X_train.shape))\n",
    "print('원본 데이터 크기 y_train: {} \\n'.format(y_train.shape))\n",
    "\n",
    "print(\"원본 데이터 '1' 개수: {}\".format(sum(y_train==1)))\n",
    "print(\"원본 데이터 '0' 개수: {} \\n\".format(sum(y_train==0)))\n",
    "\n",
    "# SMOTE 적용\n",
    "sm = SMOTE(random_state=2)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel())\n",
    "\n",
    "# 오버샘플링 후 클래스 분포 확인\n",
    "print('샘플링 데이터 크기 X_train: {}'.format(X_train_res.shape))\n",
    "print('샘플링 데이터 크기 y_train: {} \\n'.format(y_train_res.shape))\n",
    "\n",
    "print(\"샘플링 데이터 '1' 개수: {}\".format(sum(y_train_res==1)))\n",
    "print(\"샘플링 데이터 '0' 개수: {}\".format(sum(y_train_res==0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orientation_Index        0.136810\n",
      "Empty_Index              0.129313\n",
      "Edges_Y_Index            0.076571\n",
      "Outside_Global_Index     0.069283\n",
      "Steel_Plate_Thickness    0.058778\n",
      "TypeOfSteel              0.040343\n",
      "Mean_of_Luminosity       0.027345\n",
      "SigmoidOfAreas          -0.022443\n",
      "Luminosity_Index        -0.033933\n",
      "Outside_X_Index         -0.096823\n",
      "Log_Y_Mean              -0.099240\n",
      "LogOfAreas              -0.107178\n",
      "Edges_X_Index           -0.110439\n",
      "Log_Sum_of_Luminosity   -0.112474\n",
      "Edges_Index             -0.136113\n",
      "X_Mean                  -0.168924\n",
      "Square_Index            -0.180232\n",
      "Length_of_Conveyer      -0.206868\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 타겟값과 각 변수들 간의 상관관계\n",
    "co = X_train.corrwith(y_train)\n",
    "\n",
    "# 상관계수를 내림차순으로 정리\n",
    "print(co.sort_values(ascending=False))\n",
    "\n",
    "# 절대값\n",
    "co_abs = abs(co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 스케일링할 피처 선택\n",
    "scaling_features = X_train.columns\n",
    "\n",
    "# 스케일링\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = X_train.copy()  # 원본 데이터 복사\n",
    "X_test_scaled = X_test.copy()    # 원본 데이터 복사\n",
    "X_train_scaled[scaling_features] = scaler.fit_transform(X_train[scaling_features])\n",
    "X_test_scaled[scaling_features] = scaler.transform(X_test[scaling_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 하이퍼파라미터 랜덤\n",
    "import random\n",
    "\n",
    "# 유닛 랜덤\n",
    "def units_random():\n",
    "    # 2의 7제곱 (128)부터 2의 10제곱 (1024)까지\n",
    "    possible_values = [2**i for i in range(7, 11)]\n",
    "\n",
    "    # 랜덤하게 8개 선택, 중복 허용\n",
    "    units = [random.choice(possible_values) for _ in range(8)]\n",
    "\n",
    "    print(f\"units : {units}\")\n",
    "    \n",
    "    return units\n",
    "\n",
    "# L2 랜덤\n",
    "def L2_random():\n",
    "    # 0.001부터 0.015까지 0.001의 간격으로\n",
    "    possible_values = np.arange(0.001, 0.016, 0.001)\n",
    "\n",
    "    # 랜덤하게 8개 선택, 중복 허용\n",
    "    L2 = [random.choice(possible_values) for _ in range(8)]\n",
    "    L2 = [round(num, 3) for num in L2]\n",
    "\n",
    "    print(f\"L2 : {L2}\")\n",
    "    \n",
    "    return L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스모델\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def Classifier_Model(units, l2):\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=units[0], activation='relu', input_shape=(len(X_train.keys()),), kernel_regularizer=regularizers.l2(l2[0])),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=units[1], activation='relu', kernel_regularizer=regularizers.l2(l2[1])),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=units[2], activation='relu', kernel_regularizer=regularizers.l2(l2[2])),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=units[3], activation='relu', kernel_regularizer=regularizers.l2(l2[3])),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=units[4], activation='relu', kernel_regularizer=regularizers.l2(l2[4])),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=units[5], activation='relu', kernel_regularizer=regularizers.l2(l2[5])),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=units[6], activation='relu', kernel_regularizer=regularizers.l2(l2[6])),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=units[7], activation='relu', kernel_regularizer=regularizers.l2(l2[7])),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=7, activation='softmax') # 출력 유닛의 수를 클래스 수에 맞추고, softmax 활성화 함수를 사용\n",
    "    ])\n",
    "    \n",
    "    # 옵티마이저와 손실 함수 설정\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "                                        learning_rate=0.001,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.999,\n",
    "                                        epsilon=1e-08\n",
    "                                        )\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',  # 손실함수를 다중 클래스 분류에 적합한 형태로 변경\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스킵 연결 모델(Skip Connection)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Add\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def Classifier_Model_SC(units, l2):\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    inputs = Input(shape=(len(X_train.keys()),))\n",
    "    x = Dense(units=units[0], activation='relu', kernel_regularizer=regularizers.l2(l2[0]))(inputs)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    for i in range(1, 8):   \n",
    "        dense = Dense(units=units[i], activation='relu', kernel_regularizer=regularizers.l2(l2[i]))\n",
    "        y = dense(x)\n",
    "        y = Dropout(0.2)(y)\n",
    "        x = Add()([x, y])\n",
    "\n",
    "    outputs = Dense(units=7, activation='softmax')(x) # 출력 유닛의 수를 클래스 수에 맞추고, softmax 활성화 함수를 사용\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # 옵티마이저와 손실 함수 설정\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "                                        learning_rate=0.001,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.999,\n",
    "                                        epsilon=1e-08\n",
    "                                        )\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',  # 손실함수를 다중 클래스 분류에 적합한 형태로 변경\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 랜덤 서치\\n# 하이퍼파라미터 세팅\\nnp.random.seed(42)\\ntf.random.set_seed(42)\\n\\nLEARNING_RATE = 0.001\\nEPOCHS = 1024\\nMB_SIZE = 1024\\nREPORT = 1\\nTRAIN_RATIO = 0.8\\n\\n# 학습 자동 중단 설정\\nes = EarlyStopping(monitor=\\'loss\\', patience=128, mode=\\'min\\')\\nrlrp = ReduceLROnPlateau(monitor=\\'loss\\', factor=0.2, patience=128, mode=\\'min\\')\\n\\naccuracy_list = []\\n\\nwhile True:  \\n    model = Classifier_Model(units_random(), L2_random())\\n    \\n    # 모델 학습\\n    history = model.fit(\\n    X_train_scaled, y_train,\\n    batch_size=MB_SIZE,\\n    validation_split = 0.2,\\n    verbose=1,\\n    epochs=EPOCHS,\\n    callbacks=[es, rlrp]\\n    )\\n    \\n    # 테스트 세트 평가\\n    loss, accuracy = model.evaluate(X_test_scaled, y_test)\\n    print(\"Test Loss:\", round(loss, 3))\\n    print(\"Test Accuracy:\", round(accuracy, 3))\\n    \\n    accuracy_list.append(accuracy)\\n    \\n    if len(accuracy_list) >= 10:\\n        max_accuracy = max(accuracy_list)\\n        units, L2 = units_random(), L2_random()\\n        print(\"Maximum Accuracy:\", max_accuracy)\\n        print(\"units:\", units)\\n        print(\"L2\", L2)\\n        \\n        break\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# 랜덤 서치\n",
    "# 하이퍼파라미터 세팅\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 1024\n",
    "MB_SIZE = 1024\n",
    "REPORT = 1\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "# 학습 자동 중단 설정\n",
    "es = EarlyStopping(monitor='loss', patience=128, mode='min')\n",
    "rlrp = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=128, mode='min')\n",
    "\n",
    "accuracy_list = []\n",
    "\n",
    "while True:  \n",
    "    model = Classifier_Model(units_random(), L2_random())\n",
    "    \n",
    "    # 모델 학습\n",
    "    history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    batch_size=MB_SIZE,\n",
    "    validation_split = 0.2,\n",
    "    verbose=1,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[es, rlrp]\n",
    "    )\n",
    "    \n",
    "    # 테스트 세트 평가\n",
    "    loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "    print(\"Test Loss:\", round(loss, 3))\n",
    "    print(\"Test Accuracy:\", round(accuracy, 3))\n",
    "    \n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "    if len(accuracy_list) >= 10:\n",
    "        max_accuracy = max(accuracy_list)\n",
    "        units, L2 = units_random(), L2_random()\n",
    "        print(\"Maximum Accuracy:\", max_accuracy)\n",
    "        print(\"units:\", units)\n",
    "        print(\"L2\", L2)\n",
    "        \n",
    "        break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 앙상블 모델\\n# 앙상블 모델\\ndef Ensemble(num_models = 5):\\n    np.random.seed(42)\\n    tf.random.set_seed(42)\\n    \\n    models = []\\n    \\n    # 하이퍼파라미터 세팅\\n    LEARNING_RATE = 0.001\\n    EPOCHS = 1024\\n    MB_SIZE = 2000\\n    REPORT = 1\\n    TRAIN_RATIO = 0.8\\n    \\n    # 학습 자동 중단 설정\\n    es = EarlyStopping(monitor=\\'loss\\', patience=64, mode=\\'min\\')\\n    rlrp = ReduceLROnPlateau(monitor=\\'loss\\', factor=0.15, patience=64, mode=\\'min\\')\\n    \\n    optimizer = tf.keras.optimizers.Adam(\\n                                        learning_rate=0.001,\\n                                        beta_1=0.9,\\n                                        beta_2=0.999,\\n                                        epsilon=1e-08\\n                                        )\\n\\n    for _ in range(num_models):\\n        model = Classifier_Model()\\n        models.append(model)\\n\\n    histories = []\\n    for i in range(num_models):\\n        history = models[i].fit(\\n                                X_train_scaled, y_train,\\n                                batch_size=MB_SIZE,\\n                                validation_split = 0.2,\\n                                verbose=1,\\n                                epochs=EPOCHS,\\n                                callbacks=[es, rlrp]\\n                                )\\n        \\n        histories.append(history)\\n\\n    # 각 모델의 예측값을 얻고 평균을 냅니다\\n    predictions = []\\n    for model in models:\\n        loss, accuracy = model.evaluate(X_test_scaled, y_test)\\n        predictions.append(loss, accuracy)\\n        print(\"Test Loss:\", round(loss, 3))\\n        print(\"Test Accuracy:\", round(accuracy, 3))\\n\\n    # 최종 예측값은 여러 모델의 예측값을 평균낸 것입니다\\n    final_predictions = np.mean(predictions, axis=0)\\n    \\n    return final_predictions\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 앙상블 모델\n",
    "# 앙상블 모델\n",
    "def Ensemble(num_models = 5):\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    # 하이퍼파라미터 세팅\n",
    "    LEARNING_RATE = 0.001\n",
    "    EPOCHS = 1024\n",
    "    MB_SIZE = 2000\n",
    "    REPORT = 1\n",
    "    TRAIN_RATIO = 0.8\n",
    "    \n",
    "    # 학습 자동 중단 설정\n",
    "    es = EarlyStopping(monitor='loss', patience=64, mode='min')\n",
    "    rlrp = ReduceLROnPlateau(monitor='loss', factor=0.15, patience=64, mode='min')\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "                                        learning_rate=0.001,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.999,\n",
    "                                        epsilon=1e-08\n",
    "                                        )\n",
    "\n",
    "    for _ in range(num_models):\n",
    "        model = Classifier_Model()\n",
    "        models.append(model)\n",
    "\n",
    "    histories = []\n",
    "    for i in range(num_models):\n",
    "        history = models[i].fit(\n",
    "                                X_train_scaled, y_train,\n",
    "                                batch_size=MB_SIZE,\n",
    "                                validation_split = 0.2,\n",
    "                                verbose=1,\n",
    "                                epochs=EPOCHS,\n",
    "                                callbacks=[es, rlrp]\n",
    "                                )\n",
    "        \n",
    "        histories.append(history)\n",
    "\n",
    "    # 각 모델의 예측값을 얻고 평균을 냅니다\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "        predictions.append(loss, accuracy)\n",
    "        print(\"Test Loss:\", round(loss, 3))\n",
    "        print(\"Test Accuracy:\", round(accuracy, 3))\n",
    "\n",
    "    # 최종 예측값은 여러 모델의 예측값을 평균낸 것입니다\n",
    "    final_predictions = np.mean(predictions, axis=0)\n",
    "    \n",
    "    return final_predictions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 객체 생성\n",
    "\"\"\"\n",
    "units = [33, 64, 128, 256, 256, 128, 64, 33]\n",
    "L2 = [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000]\n",
    "model = Classifier_Model(units, L2)\n",
    "\"\"\"\n",
    "units = [1024 for _ in range(8)]\n",
    "L2 = [0.001 + i * 0.0025 for i in range(8)]\n",
    "model = Classifier_Model_SC(units, L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 자동 중단 설정\n",
    "es = EarlyStopping(monitor='loss', patience=50, mode='auto')\n",
    "rlrp = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=50, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 00:07:28.251466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-05-24 00:07:28.300527: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x25a4d60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-24 00:07:28.300576: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2023-05-24 00:07:28.353406: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-24 00:07:31.611672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8901\n",
      "2023-05-24 00:07:31.983445: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 10s 10s/step - loss: 81.9226 - accuracy: 0.0984 - val_loss: 98.9736 - val_accuracy: 0.3280 - lr: 0.0010\n",
      "Epoch 2/1024\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 100.4526 - accuracy: 0.3452 - val_loss: 79.5710 - val_accuracy: 0.3280 - lr: 0.0010\n",
      "Epoch 3/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 79.8698 - accuracy: 0.3452 - val_loss: 77.0263 - val_accuracy: 0.1897 - lr: 0.0010\n",
      "Epoch 4/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 77.3363 - accuracy: 0.2089 - val_loss: 74.3682 - val_accuracy: 0.3183 - lr: 0.0010\n",
      "Epoch 5/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 74.5595 - accuracy: 0.3081 - val_loss: 72.3092 - val_accuracy: 0.2186 - lr: 0.0010\n",
      "Epoch 6/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 72.4650 - accuracy: 0.2065 - val_loss: 70.2785 - val_accuracy: 0.2219 - lr: 0.0010\n",
      "Epoch 7/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 70.3858 - accuracy: 0.2226 - val_loss: 68.3131 - val_accuracy: 0.2894 - lr: 0.0010\n",
      "Epoch 8/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 68.3905 - accuracy: 0.2637 - val_loss: 66.3992 - val_accuracy: 0.3280 - lr: 0.0010\n",
      "Epoch 9/1024\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 66.4435 - accuracy: 0.2935 - val_loss: 64.5198 - val_accuracy: 0.3151 - lr: 0.0010\n",
      "Epoch 10/1024\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 64.5396 - accuracy: 0.2911 - val_loss: 62.6700 - val_accuracy: 0.3087 - lr: 0.0010\n",
      "Epoch 11/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 62.6809 - accuracy: 0.3016 - val_loss: 60.8472 - val_accuracy: 0.4084 - lr: 0.0010\n",
      "Epoch 12/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 60.8482 - accuracy: 0.3661 - val_loss: 59.0510 - val_accuracy: 0.5402 - lr: 0.0010\n",
      "Epoch 13/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 59.0472 - accuracy: 0.5056 - val_loss: 57.2839 - val_accuracy: 0.4855 - lr: 0.0010\n",
      "Epoch 14/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 57.2807 - accuracy: 0.5016 - val_loss: 55.5503 - val_accuracy: 0.5113 - lr: 0.0010\n",
      "Epoch 15/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 55.5458 - accuracy: 0.5024 - val_loss: 53.8560 - val_accuracy: 0.5113 - lr: 0.0010\n",
      "Epoch 16/1024\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 53.8501 - accuracy: 0.4927 - val_loss: 52.2068 - val_accuracy: 0.5080 - lr: 0.0010\n",
      "Epoch 17/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 52.2087 - accuracy: 0.4903 - val_loss: 50.6060 - val_accuracy: 0.5016 - lr: 0.0010\n",
      "Epoch 18/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 50.6022 - accuracy: 0.4944 - val_loss: 49.0524 - val_accuracy: 0.4984 - lr: 0.0010\n",
      "Epoch 19/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 49.0544 - accuracy: 0.4992 - val_loss: 47.5417 - val_accuracy: 0.4920 - lr: 0.0010\n",
      "Epoch 20/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 47.5410 - accuracy: 0.5097 - val_loss: 46.0714 - val_accuracy: 0.5113 - lr: 0.0010\n",
      "Epoch 21/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 46.0724 - accuracy: 0.5274 - val_loss: 44.6388 - val_accuracy: 0.4920 - lr: 0.0010\n",
      "Epoch 22/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 44.6416 - accuracy: 0.5282 - val_loss: 43.2428 - val_accuracy: 0.4984 - lr: 0.0010\n",
      "Epoch 23/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 43.2517 - accuracy: 0.5218 - val_loss: 41.8873 - val_accuracy: 0.5080 - lr: 0.0010\n",
      "Epoch 24/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 41.8972 - accuracy: 0.5169 - val_loss: 40.5783 - val_accuracy: 0.5177 - lr: 0.0010\n",
      "Epoch 25/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 40.5903 - accuracy: 0.5435 - val_loss: 39.3167 - val_accuracy: 0.5273 - lr: 0.0010\n",
      "Epoch 26/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 39.3380 - accuracy: 0.5331 - val_loss: 38.0984 - val_accuracy: 0.5241 - lr: 0.0010\n",
      "Epoch 27/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 38.1158 - accuracy: 0.5371 - val_loss: 36.9182 - val_accuracy: 0.5145 - lr: 0.0010\n",
      "Epoch 28/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 36.9393 - accuracy: 0.5500 - val_loss: 35.7747 - val_accuracy: 0.5113 - lr: 0.0010\n",
      "Epoch 29/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 35.7959 - accuracy: 0.5573 - val_loss: 34.6694 - val_accuracy: 0.5145 - lr: 0.0010\n",
      "Epoch 30/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 34.6844 - accuracy: 0.5548 - val_loss: 33.6052 - val_accuracy: 0.5177 - lr: 0.0010\n",
      "Epoch 31/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 33.6146 - accuracy: 0.5476 - val_loss: 32.5803 - val_accuracy: 0.5241 - lr: 0.0010\n",
      "Epoch 32/1024\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 32.5861 - accuracy: 0.5605 - val_loss: 31.5902 - val_accuracy: 0.5209 - lr: 0.0010\n",
      "Epoch 33/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 31.5902 - accuracy: 0.5573 - val_loss: 30.6302 - val_accuracy: 0.5305 - lr: 0.0010\n",
      "Epoch 34/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 30.6323 - accuracy: 0.5653 - val_loss: 29.7000 - val_accuracy: 0.5659 - lr: 0.0010\n",
      "Epoch 35/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 29.7101 - accuracy: 0.5766 - val_loss: 28.8027 - val_accuracy: 0.5884 - lr: 0.0010\n",
      "Epoch 36/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 28.8105 - accuracy: 0.6048 - val_loss: 27.9405 - val_accuracy: 0.5981 - lr: 0.0010\n",
      "Epoch 37/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 27.9462 - accuracy: 0.6016 - val_loss: 27.1101 - val_accuracy: 0.6045 - lr: 0.0010\n",
      "Epoch 38/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 27.1244 - accuracy: 0.6089 - val_loss: 26.3092 - val_accuracy: 0.6077 - lr: 0.0010\n",
      "Epoch 39/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 26.3259 - accuracy: 0.6161 - val_loss: 25.5380 - val_accuracy: 0.6077 - lr: 0.0010\n",
      "Epoch 40/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 25.5524 - accuracy: 0.6411 - val_loss: 24.7975 - val_accuracy: 0.6367 - lr: 0.0010\n",
      "Epoch 41/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 24.8171 - accuracy: 0.6242 - val_loss: 24.0888 - val_accuracy: 0.6334 - lr: 0.0010\n",
      "Epoch 42/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 24.1009 - accuracy: 0.6331 - val_loss: 23.4032 - val_accuracy: 0.6334 - lr: 0.0010\n",
      "Epoch 43/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 23.4101 - accuracy: 0.6500 - val_loss: 22.7395 - val_accuracy: 0.6431 - lr: 0.0010\n",
      "Epoch 44/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 22.7511 - accuracy: 0.6621 - val_loss: 22.1039 - val_accuracy: 0.6592 - lr: 0.0010\n",
      "Epoch 45/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 22.1157 - accuracy: 0.6710 - val_loss: 21.4970 - val_accuracy: 0.6559 - lr: 0.0010\n",
      "Epoch 46/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 21.5050 - accuracy: 0.6718 - val_loss: 20.9189 - val_accuracy: 0.6463 - lr: 0.0010\n",
      "Epoch 47/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 20.9175 - accuracy: 0.6726 - val_loss: 20.3535 - val_accuracy: 0.6559 - lr: 0.0010\n",
      "Epoch 48/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 20.3488 - accuracy: 0.6855 - val_loss: 19.8111 - val_accuracy: 0.6592 - lr: 0.0010\n",
      "Epoch 49/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 19.8144 - accuracy: 0.6815 - val_loss: 19.3018 - val_accuracy: 0.6624 - lr: 0.0010\n",
      "Epoch 50/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 19.2893 - accuracy: 0.6798 - val_loss: 18.8078 - val_accuracy: 0.6463 - lr: 0.0010\n",
      "Epoch 51/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 18.8034 - accuracy: 0.6669 - val_loss: 18.3440 - val_accuracy: 0.6785 - lr: 0.0010\n",
      "Epoch 52/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 18.3404 - accuracy: 0.6540 - val_loss: 17.8884 - val_accuracy: 0.6399 - lr: 0.0010\n",
      "Epoch 53/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 17.8725 - accuracy: 0.6411 - val_loss: 17.4099 - val_accuracy: 0.6688 - lr: 0.0010\n",
      "Epoch 54/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 17.3937 - accuracy: 0.6790 - val_loss: 17.0069 - val_accuracy: 0.6752 - lr: 0.0010\n",
      "Epoch 55/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 16.9843 - accuracy: 0.6637 - val_loss: 16.5690 - val_accuracy: 0.6302 - lr: 0.0010\n",
      "Epoch 56/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 16.5336 - accuracy: 0.6887 - val_loss: 16.1622 - val_accuracy: 0.6367 - lr: 0.0010\n",
      "Epoch 57/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 16.1379 - accuracy: 0.6766 - val_loss: 15.7792 - val_accuracy: 0.6817 - lr: 0.0010\n",
      "Epoch 58/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 15.7520 - accuracy: 0.6919 - val_loss: 15.3980 - val_accuracy: 0.6785 - lr: 0.0010\n",
      "Epoch 59/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 15.3767 - accuracy: 0.7048 - val_loss: 15.0413 - val_accuracy: 0.6624 - lr: 0.0010\n",
      "Epoch 60/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 15.0151 - accuracy: 0.6935 - val_loss: 14.6889 - val_accuracy: 0.6624 - lr: 0.0010\n",
      "Epoch 61/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 14.6518 - accuracy: 0.7008 - val_loss: 14.3564 - val_accuracy: 0.6752 - lr: 0.0010\n",
      "Epoch 62/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 14.3274 - accuracy: 0.6968 - val_loss: 14.0183 - val_accuracy: 0.6785 - lr: 0.0010\n",
      "Epoch 63/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 13.9893 - accuracy: 0.6984 - val_loss: 13.7058 - val_accuracy: 0.6720 - lr: 0.0010\n",
      "Epoch 64/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 13.6695 - accuracy: 0.7016 - val_loss: 13.4015 - val_accuracy: 0.6720 - lr: 0.0010\n",
      "Epoch 65/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 13.3503 - accuracy: 0.7040 - val_loss: 13.1053 - val_accuracy: 0.6752 - lr: 0.0010\n",
      "Epoch 66/1024\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 13.0712 - accuracy: 0.7129 - val_loss: 12.8175 - val_accuracy: 0.6817 - lr: 0.0010\n",
      "Epoch 67/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 12.7755 - accuracy: 0.7185 - val_loss: 12.5410 - val_accuracy: 0.6720 - lr: 0.0010\n",
      "Epoch 68/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 12.5004 - accuracy: 0.7097 - val_loss: 12.2796 - val_accuracy: 0.6817 - lr: 0.0010\n",
      "Epoch 69/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 12.2319 - accuracy: 0.7129 - val_loss: 12.0093 - val_accuracy: 0.6785 - lr: 0.0010\n",
      "Epoch 70/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 11.9671 - accuracy: 0.7177 - val_loss: 11.7596 - val_accuracy: 0.6785 - lr: 0.0010\n",
      "Epoch 71/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 11.7151 - accuracy: 0.7153 - val_loss: 11.5154 - val_accuracy: 0.6817 - lr: 0.0010\n",
      "Epoch 72/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 11.4602 - accuracy: 0.7153 - val_loss: 11.2802 - val_accuracy: 0.6881 - lr: 0.0010\n",
      "Epoch 73/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 11.2425 - accuracy: 0.7121 - val_loss: 11.0461 - val_accuracy: 0.6817 - lr: 0.0010\n",
      "Epoch 74/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 11.0027 - accuracy: 0.7065 - val_loss: 10.8287 - val_accuracy: 0.6785 - lr: 0.0010\n",
      "Epoch 75/1024\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 10.7760 - accuracy: 0.7153 - val_loss: 10.6093 - val_accuracy: 0.6881 - lr: 0.0010\n",
      "Epoch 76/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 10.5604 - accuracy: 0.7177 - val_loss: 10.3957 - val_accuracy: 0.6977 - lr: 0.0010\n",
      "Epoch 77/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 10.3458 - accuracy: 0.7194 - val_loss: 10.1965 - val_accuracy: 0.6881 - lr: 0.0010\n",
      "Epoch 78/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 10.1396 - accuracy: 0.7363 - val_loss: 9.9941 - val_accuracy: 0.7010 - lr: 0.0010\n",
      "Epoch 79/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 9.9412 - accuracy: 0.7331 - val_loss: 9.7976 - val_accuracy: 0.6913 - lr: 0.0010\n",
      "Epoch 80/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 9.7475 - accuracy: 0.7177 - val_loss: 9.6170 - val_accuracy: 0.6945 - lr: 0.0010\n",
      "Epoch 81/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 9.5619 - accuracy: 0.7250 - val_loss: 9.4334 - val_accuracy: 0.6945 - lr: 0.0010\n",
      "Epoch 82/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 9.3744 - accuracy: 0.7371 - val_loss: 9.2490 - val_accuracy: 0.6913 - lr: 0.0010\n",
      "Epoch 83/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 9.2033 - accuracy: 0.7121 - val_loss: 9.0795 - val_accuracy: 0.6913 - lr: 0.0010\n",
      "Epoch 84/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 9.0092 - accuracy: 0.7371 - val_loss: 8.9101 - val_accuracy: 0.7010 - lr: 0.0010\n",
      "Epoch 85/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 8.8538 - accuracy: 0.7355 - val_loss: 8.7511 - val_accuracy: 0.6945 - lr: 0.0010\n",
      "Epoch 86/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 8.6975 - accuracy: 0.7234 - val_loss: 8.5879 - val_accuracy: 0.6945 - lr: 0.0010\n",
      "Epoch 87/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 8.5257 - accuracy: 0.7379 - val_loss: 8.4326 - val_accuracy: 0.7074 - lr: 0.0010\n",
      "Epoch 88/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 8.3777 - accuracy: 0.7258 - val_loss: 8.2816 - val_accuracy: 0.6849 - lr: 0.0010\n",
      "Epoch 89/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.2187 - accuracy: 0.7242 - val_loss: 8.1466 - val_accuracy: 0.6913 - lr: 0.0010\n",
      "Epoch 90/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.0715 - accuracy: 0.7476 - val_loss: 7.9825 - val_accuracy: 0.6977 - lr: 0.0010\n",
      "Epoch 91/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 7.9235 - accuracy: 0.7347 - val_loss: 7.8539 - val_accuracy: 0.6881 - lr: 0.0010\n",
      "Epoch 92/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 7.7800 - accuracy: 0.7323 - val_loss: 7.7174 - val_accuracy: 0.7042 - lr: 0.0010\n",
      "Epoch 93/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.6468 - accuracy: 0.7427 - val_loss: 7.5786 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 94/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7.5252 - accuracy: 0.7435 - val_loss: 7.4524 - val_accuracy: 0.6913 - lr: 0.0010\n",
      "Epoch 95/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.3868 - accuracy: 0.7137 - val_loss: 7.3253 - val_accuracy: 0.6977 - lr: 0.0010\n",
      "Epoch 96/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.2589 - accuracy: 0.7347 - val_loss: 7.1995 - val_accuracy: 0.7042 - lr: 0.0010\n",
      "Epoch 97/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7.1297 - accuracy: 0.7460 - val_loss: 7.0835 - val_accuracy: 0.7106 - lr: 0.0010\n",
      "Epoch 98/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7.0032 - accuracy: 0.7460 - val_loss: 6.9583 - val_accuracy: 0.7042 - lr: 0.0010\n",
      "Epoch 99/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6.8779 - accuracy: 0.7452 - val_loss: 6.8400 - val_accuracy: 0.7074 - lr: 0.0010\n",
      "Epoch 100/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6.7740 - accuracy: 0.7435 - val_loss: 6.7397 - val_accuracy: 0.6913 - lr: 0.0010\n",
      "Epoch 101/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6.6585 - accuracy: 0.7500 - val_loss: 6.6183 - val_accuracy: 0.7010 - lr: 0.0010\n",
      "Epoch 102/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6.5390 - accuracy: 0.7508 - val_loss: 6.4994 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 103/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6.4187 - accuracy: 0.7605 - val_loss: 6.4061 - val_accuracy: 0.7203 - lr: 0.0010\n",
      "Epoch 104/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 6.3235 - accuracy: 0.7460 - val_loss: 6.2972 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 105/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6.2225 - accuracy: 0.7565 - val_loss: 6.2001 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 106/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 6.1309 - accuracy: 0.7492 - val_loss: 6.1105 - val_accuracy: 0.6977 - lr: 0.0010\n",
      "Epoch 107/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 6.0181 - accuracy: 0.7484 - val_loss: 6.0090 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 108/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 5.9251 - accuracy: 0.7581 - val_loss: 5.9087 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 109/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 5.8411 - accuracy: 0.7452 - val_loss: 5.8394 - val_accuracy: 0.7106 - lr: 0.0010\n",
      "Epoch 110/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 5.7553 - accuracy: 0.7427 - val_loss: 5.7261 - val_accuracy: 0.7106 - lr: 0.0010\n",
      "Epoch 111/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 5.6383 - accuracy: 0.7573 - val_loss: 5.6395 - val_accuracy: 0.7074 - lr: 0.0010\n",
      "Epoch 112/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 5.5712 - accuracy: 0.7460 - val_loss: 5.5804 - val_accuracy: 0.7042 - lr: 0.0010\n",
      "Epoch 113/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5.4873 - accuracy: 0.7516 - val_loss: 5.5004 - val_accuracy: 0.6817 - lr: 0.0010\n",
      "Epoch 114/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 5.4138 - accuracy: 0.7250 - val_loss: 5.4020 - val_accuracy: 0.7106 - lr: 0.0010\n",
      "Epoch 115/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5.3324 - accuracy: 0.7444 - val_loss: 5.3050 - val_accuracy: 0.7235 - lr: 0.0010\n",
      "Epoch 116/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 5.2239 - accuracy: 0.7685 - val_loss: 5.2491 - val_accuracy: 0.7074 - lr: 0.0010\n",
      "Epoch 117/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5.1614 - accuracy: 0.7355 - val_loss: 5.1571 - val_accuracy: 0.7042 - lr: 0.0010\n",
      "Epoch 118/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 5.0688 - accuracy: 0.7637 - val_loss: 5.0779 - val_accuracy: 0.7299 - lr: 0.0010\n",
      "Epoch 119/1024\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 4.9986 - accuracy: 0.7613 - val_loss: 5.0217 - val_accuracy: 0.7106 - lr: 0.0010\n",
      "Epoch 120/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 4.9258 - accuracy: 0.7508 - val_loss: 4.9408 - val_accuracy: 0.7010 - lr: 0.0010\n",
      "Epoch 121/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 4.8568 - accuracy: 0.7524 - val_loss: 4.8585 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 122/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 4.7794 - accuracy: 0.7621 - val_loss: 4.7926 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 123/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 4.6966 - accuracy: 0.7581 - val_loss: 4.7411 - val_accuracy: 0.7203 - lr: 0.0010\n",
      "Epoch 124/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 4.6529 - accuracy: 0.7573 - val_loss: 4.6566 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 125/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 4.5744 - accuracy: 0.7573 - val_loss: 4.5957 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 126/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 4.5046 - accuracy: 0.7645 - val_loss: 4.5429 - val_accuracy: 0.7106 - lr: 0.0010\n",
      "Epoch 127/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 4.4457 - accuracy: 0.7540 - val_loss: 4.4734 - val_accuracy: 0.7042 - lr: 0.0010\n",
      "Epoch 128/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 4.3778 - accuracy: 0.7597 - val_loss: 4.4057 - val_accuracy: 0.7235 - lr: 0.0010\n",
      "Epoch 129/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 4.3272 - accuracy: 0.7540 - val_loss: 4.3458 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 130/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 4.2588 - accuracy: 0.7645 - val_loss: 4.3160 - val_accuracy: 0.6977 - lr: 0.0010\n",
      "Epoch 131/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 4.2298 - accuracy: 0.7540 - val_loss: 4.2317 - val_accuracy: 0.7235 - lr: 0.0010\n",
      "Epoch 132/1024\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 4.1422 - accuracy: 0.7548 - val_loss: 4.1690 - val_accuracy: 0.7267 - lr: 0.0010\n",
      "Epoch 133/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 4.0869 - accuracy: 0.7613 - val_loss: 4.1300 - val_accuracy: 0.7010 - lr: 0.0010\n",
      "Epoch 134/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 4.0200 - accuracy: 0.7645 - val_loss: 4.0796 - val_accuracy: 0.6913 - lr: 0.0010\n",
      "Epoch 135/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.9716 - accuracy: 0.7565 - val_loss: 4.0066 - val_accuracy: 0.7235 - lr: 0.0010\n",
      "Epoch 136/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.9077 - accuracy: 0.7685 - val_loss: 3.9529 - val_accuracy: 0.7235 - lr: 0.0010\n",
      "Epoch 137/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.8510 - accuracy: 0.7661 - val_loss: 3.9218 - val_accuracy: 0.7203 - lr: 0.0010\n",
      "Epoch 138/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.8122 - accuracy: 0.7653 - val_loss: 3.8543 - val_accuracy: 0.7203 - lr: 0.0010\n",
      "Epoch 139/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.7540 - accuracy: 0.7782 - val_loss: 3.8054 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 140/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.7148 - accuracy: 0.7605 - val_loss: 3.7702 - val_accuracy: 0.7203 - lr: 0.0010\n",
      "Epoch 141/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.6585 - accuracy: 0.7766 - val_loss: 3.7178 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 142/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.6120 - accuracy: 0.7718 - val_loss: 3.6622 - val_accuracy: 0.7235 - lr: 0.0010\n",
      "Epoch 143/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.5642 - accuracy: 0.7798 - val_loss: 3.6224 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 144/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.5140 - accuracy: 0.7806 - val_loss: 3.5826 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 145/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.4715 - accuracy: 0.7694 - val_loss: 3.5306 - val_accuracy: 0.7203 - lr: 0.0010\n",
      "Epoch 146/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.4317 - accuracy: 0.7702 - val_loss: 3.4922 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 147/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.3815 - accuracy: 0.7766 - val_loss: 3.4886 - val_accuracy: 0.7042 - lr: 0.0010\n",
      "Epoch 148/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.3835 - accuracy: 0.7403 - val_loss: 3.4854 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 149/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.3968 - accuracy: 0.7218 - val_loss: 3.4806 - val_accuracy: 0.6817 - lr: 0.0010\n",
      "Epoch 150/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.3875 - accuracy: 0.6960 - val_loss: 3.3361 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 151/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.2410 - accuracy: 0.7669 - val_loss: 3.3866 - val_accuracy: 0.7106 - lr: 0.0010\n",
      "Epoch 152/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.2925 - accuracy: 0.7169 - val_loss: 3.2522 - val_accuracy: 0.7203 - lr: 0.0010\n",
      "Epoch 153/1024\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.1651 - accuracy: 0.7645 - val_loss: 3.2628 - val_accuracy: 0.6945 - lr: 0.0010\n",
      "Epoch 154/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.1829 - accuracy: 0.7234 - val_loss: 3.1992 - val_accuracy: 0.6817 - lr: 0.0010\n",
      "Epoch 155/1024\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.1110 - accuracy: 0.7508 - val_loss: 3.1583 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 156/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.0648 - accuracy: 0.7621 - val_loss: 3.1417 - val_accuracy: 0.7203 - lr: 0.0010\n",
      "Epoch 157/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.0532 - accuracy: 0.7427 - val_loss: 3.0698 - val_accuracy: 0.7203 - lr: 0.0010\n",
      "Epoch 158/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9774 - accuracy: 0.7766 - val_loss: 3.0633 - val_accuracy: 0.7042 - lr: 0.0010\n",
      "Epoch 159/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9655 - accuracy: 0.7435 - val_loss: 3.0253 - val_accuracy: 0.6881 - lr: 0.0010\n",
      "Epoch 160/1024\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9183 - accuracy: 0.7532 - val_loss: 2.9777 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 161/1024\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8729 - accuracy: 0.7782 - val_loss: 2.9632 - val_accuracy: 0.7203 - lr: 0.0010\n",
      "Epoch 162/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8636 - accuracy: 0.7589 - val_loss: 2.9218 - val_accuracy: 0.7203 - lr: 0.0010\n",
      "Epoch 163/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.7998 - accuracy: 0.7839 - val_loss: 2.9022 - val_accuracy: 0.6945 - lr: 0.0010\n",
      "Epoch 164/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.7887 - accuracy: 0.7565 - val_loss: 2.8459 - val_accuracy: 0.7267 - lr: 0.0010\n",
      "Epoch 165/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7360 - accuracy: 0.7734 - val_loss: 2.8186 - val_accuracy: 0.7074 - lr: 0.0010\n",
      "Epoch 166/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7126 - accuracy: 0.7726 - val_loss: 2.7951 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 167/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6778 - accuracy: 0.7758 - val_loss: 2.7629 - val_accuracy: 0.7106 - lr: 0.0010\n",
      "Epoch 168/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.6450 - accuracy: 0.7782 - val_loss: 2.7307 - val_accuracy: 0.6945 - lr: 0.0010\n",
      "Epoch 169/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6199 - accuracy: 0.7718 - val_loss: 2.7019 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 170/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5822 - accuracy: 0.7831 - val_loss: 2.6807 - val_accuracy: 0.7042 - lr: 0.0010\n",
      "Epoch 171/1024\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.5547 - accuracy: 0.7847 - val_loss: 2.6474 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 172/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.5293 - accuracy: 0.7734 - val_loss: 2.6237 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 173/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.5004 - accuracy: 0.7847 - val_loss: 2.5973 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 174/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.4771 - accuracy: 0.7823 - val_loss: 2.5717 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 175/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.4438 - accuracy: 0.7855 - val_loss: 2.5311 - val_accuracy: 0.7235 - lr: 0.0010\n",
      "Epoch 176/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.4134 - accuracy: 0.7823 - val_loss: 2.5076 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 177/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.3792 - accuracy: 0.7895 - val_loss: 2.4808 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 178/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.3596 - accuracy: 0.7871 - val_loss: 2.4564 - val_accuracy: 0.7106 - lr: 0.0010\n",
      "Epoch 179/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.3379 - accuracy: 0.7863 - val_loss: 2.4409 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 180/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.3147 - accuracy: 0.7766 - val_loss: 2.4157 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 181/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.2851 - accuracy: 0.7863 - val_loss: 2.3909 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 182/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.2668 - accuracy: 0.7919 - val_loss: 2.3748 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 183/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.2466 - accuracy: 0.7750 - val_loss: 2.3496 - val_accuracy: 0.7267 - lr: 0.0010\n",
      "Epoch 184/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.2274 - accuracy: 0.7750 - val_loss: 2.3216 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 185/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.1980 - accuracy: 0.7839 - val_loss: 2.2936 - val_accuracy: 0.7235 - lr: 0.0010\n",
      "Epoch 186/1024\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.1773 - accuracy: 0.7774 - val_loss: 2.2853 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 187/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.1590 - accuracy: 0.7847 - val_loss: 2.2504 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 188/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.1347 - accuracy: 0.7911 - val_loss: 2.2301 - val_accuracy: 0.7235 - lr: 0.0010\n",
      "Epoch 189/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.1096 - accuracy: 0.7855 - val_loss: 2.2311 - val_accuracy: 0.7235 - lr: 0.0010\n",
      "Epoch 190/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.0876 - accuracy: 0.7952 - val_loss: 2.2050 - val_accuracy: 0.7203 - lr: 0.0010\n",
      "Epoch 191/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.0652 - accuracy: 0.7911 - val_loss: 2.1772 - val_accuracy: 0.7299 - lr: 0.0010\n",
      "Epoch 192/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.0456 - accuracy: 0.7992 - val_loss: 2.1594 - val_accuracy: 0.7299 - lr: 0.0010\n",
      "Epoch 193/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.0324 - accuracy: 0.7815 - val_loss: 2.1407 - val_accuracy: 0.7203 - lr: 0.0010\n",
      "Epoch 194/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.0036 - accuracy: 0.7935 - val_loss: 2.1249 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 195/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.9884 - accuracy: 0.7855 - val_loss: 2.0972 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 196/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.9643 - accuracy: 0.7887 - val_loss: 2.0824 - val_accuracy: 0.7299 - lr: 0.0010\n",
      "Epoch 197/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.9551 - accuracy: 0.7871 - val_loss: 2.0700 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 198/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.9410 - accuracy: 0.7855 - val_loss: 2.0430 - val_accuracy: 0.7203 - lr: 0.0010\n",
      "Epoch 199/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.9106 - accuracy: 0.7992 - val_loss: 2.0431 - val_accuracy: 0.7267 - lr: 0.0010\n",
      "Epoch 200/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.9013 - accuracy: 0.7879 - val_loss: 2.0330 - val_accuracy: 0.7235 - lr: 0.0010\n",
      "Epoch 201/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.9030 - accuracy: 0.7742 - val_loss: 2.0254 - val_accuracy: 0.7267 - lr: 0.0010\n",
      "Epoch 202/1024\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.9080 - accuracy: 0.7669 - val_loss: 2.0119 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 203/1024\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 1.8923 - accuracy: 0.7702 - val_loss: 1.9864 - val_accuracy: 0.7106 - lr: 0.0010\n",
      "Epoch 204/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.8422 - accuracy: 0.7879 - val_loss: 1.9489 - val_accuracy: 0.7267 - lr: 0.0010\n",
      "Epoch 205/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.8140 - accuracy: 0.8000 - val_loss: 1.9613 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 206/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.8249 - accuracy: 0.7758 - val_loss: 1.9242 - val_accuracy: 0.7235 - lr: 0.0010\n",
      "Epoch 207/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.7935 - accuracy: 0.7847 - val_loss: 1.9084 - val_accuracy: 0.7235 - lr: 0.0010\n",
      "Epoch 208/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.7771 - accuracy: 0.8016 - val_loss: 1.9024 - val_accuracy: 0.7299 - lr: 0.0010\n",
      "Epoch 209/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.7746 - accuracy: 0.7839 - val_loss: 1.8668 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 210/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.7359 - accuracy: 0.8024 - val_loss: 1.8650 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 211/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.7343 - accuracy: 0.7919 - val_loss: 1.8469 - val_accuracy: 0.7203 - lr: 0.0010\n",
      "Epoch 212/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.7108 - accuracy: 0.7935 - val_loss: 1.8349 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 213/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.6844 - accuracy: 0.7911 - val_loss: 1.8299 - val_accuracy: 0.7299 - lr: 0.0010\n",
      "Epoch 214/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.6945 - accuracy: 0.7831 - val_loss: 1.8032 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 215/1024\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.6583 - accuracy: 0.7976 - val_loss: 1.7947 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 216/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 1.6436 - accuracy: 0.8073 - val_loss: 1.7828 - val_accuracy: 0.7267 - lr: 0.0010\n",
      "Epoch 217/1024\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.6451 - accuracy: 0.7927 - val_loss: 1.7548 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 218/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.6036 - accuracy: 0.8161 - val_loss: 1.7596 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 219/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.6057 - accuracy: 0.7984 - val_loss: 1.7594 - val_accuracy: 0.7235 - lr: 0.0010\n",
      "Epoch 220/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.6080 - accuracy: 0.7935 - val_loss: 1.7357 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 221/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.5859 - accuracy: 0.7935 - val_loss: 1.7172 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 222/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.5855 - accuracy: 0.7927 - val_loss: 1.7781 - val_accuracy: 0.7074 - lr: 0.0010\n",
      "Epoch 223/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.6345 - accuracy: 0.7548 - val_loss: 1.7396 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 224/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.6214 - accuracy: 0.7726 - val_loss: 1.7247 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 225/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.5827 - accuracy: 0.7661 - val_loss: 1.6691 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 226/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.5345 - accuracy: 0.8016 - val_loss: 1.6689 - val_accuracy: 0.7267 - lr: 0.0010\n",
      "Epoch 227/1024\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.5335 - accuracy: 0.7895 - val_loss: 1.6623 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 228/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.5268 - accuracy: 0.7927 - val_loss: 1.6383 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 229/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.5034 - accuracy: 0.8008 - val_loss: 1.6296 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 230/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.4884 - accuracy: 0.7960 - val_loss: 1.6221 - val_accuracy: 0.7203 - lr: 0.0010\n",
      "Epoch 231/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.4763 - accuracy: 0.8008 - val_loss: 1.6108 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 232/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.4558 - accuracy: 0.8016 - val_loss: 1.5975 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 233/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.4473 - accuracy: 0.8129 - val_loss: 1.5935 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 234/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.4473 - accuracy: 0.7984 - val_loss: 1.5834 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 235/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.4327 - accuracy: 0.7952 - val_loss: 1.5749 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 236/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.4193 - accuracy: 0.7944 - val_loss: 1.5539 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 237/1024\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.3976 - accuracy: 0.8121 - val_loss: 1.5403 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 238/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.3872 - accuracy: 0.8032 - val_loss: 1.5569 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 239/1024\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.3912 - accuracy: 0.7960 - val_loss: 1.5387 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 240/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.3916 - accuracy: 0.7992 - val_loss: 1.5247 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 241/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.3713 - accuracy: 0.7952 - val_loss: 1.5157 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 242/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.3508 - accuracy: 0.8008 - val_loss: 1.4951 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 243/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.3508 - accuracy: 0.8121 - val_loss: 1.5007 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 244/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.3386 - accuracy: 0.8145 - val_loss: 1.4879 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 245/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.3361 - accuracy: 0.8000 - val_loss: 1.4688 - val_accuracy: 0.7235 - lr: 0.0010\n",
      "Epoch 246/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.3017 - accuracy: 0.8210 - val_loss: 1.4803 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 247/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.2981 - accuracy: 0.8210 - val_loss: 1.4680 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 248/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.3060 - accuracy: 0.8121 - val_loss: 1.4540 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 249/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.2922 - accuracy: 0.8161 - val_loss: 1.4521 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 250/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.2682 - accuracy: 0.8339 - val_loss: 1.4344 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 251/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.2676 - accuracy: 0.8145 - val_loss: 1.4223 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 252/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.2670 - accuracy: 0.8137 - val_loss: 1.4327 - val_accuracy: 0.7299 - lr: 0.0010\n",
      "Epoch 253/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.2724 - accuracy: 0.8048 - val_loss: 1.4306 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 254/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.2491 - accuracy: 0.8048 - val_loss: 1.4184 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 255/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.2646 - accuracy: 0.8056 - val_loss: 1.4682 - val_accuracy: 0.7267 - lr: 0.0010\n",
      "Epoch 256/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.2898 - accuracy: 0.7750 - val_loss: 1.4003 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 257/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.2494 - accuracy: 0.8073 - val_loss: 1.3812 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 258/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.2186 - accuracy: 0.8065 - val_loss: 1.3856 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 259/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 1.2114 - accuracy: 0.8065 - val_loss: 1.3657 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 260/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.2109 - accuracy: 0.8137 - val_loss: 1.3478 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 261/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.1912 - accuracy: 0.8121 - val_loss: 1.3685 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 262/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.1929 - accuracy: 0.8105 - val_loss: 1.3504 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 263/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.1891 - accuracy: 0.8161 - val_loss: 1.3294 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 264/1024\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.1813 - accuracy: 0.8024 - val_loss: 1.3577 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 265/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.1771 - accuracy: 0.7992 - val_loss: 1.3430 - val_accuracy: 0.7267 - lr: 0.0010\n",
      "Epoch 266/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1750 - accuracy: 0.8065 - val_loss: 1.3162 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 267/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.1547 - accuracy: 0.8145 - val_loss: 1.3284 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 268/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.1547 - accuracy: 0.8048 - val_loss: 1.3094 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 269/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1382 - accuracy: 0.8145 - val_loss: 1.3146 - val_accuracy: 0.7299 - lr: 0.0010\n",
      "Epoch 270/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1283 - accuracy: 0.8234 - val_loss: 1.3154 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 271/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.1327 - accuracy: 0.8024 - val_loss: 1.2814 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 272/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1142 - accuracy: 0.8218 - val_loss: 1.2789 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 273/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1129 - accuracy: 0.8185 - val_loss: 1.2848 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 274/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1169 - accuracy: 0.8113 - val_loss: 1.2601 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 275/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.0963 - accuracy: 0.8218 - val_loss: 1.2591 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 276/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.0914 - accuracy: 0.8266 - val_loss: 1.2651 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 277/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.0768 - accuracy: 0.8274 - val_loss: 1.2814 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 278/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.0820 - accuracy: 0.8202 - val_loss: 1.2533 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 279/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.0891 - accuracy: 0.8161 - val_loss: 1.2867 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 280/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.1135 - accuracy: 0.7935 - val_loss: 1.2793 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 281/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.1279 - accuracy: 0.7879 - val_loss: 1.2800 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 282/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.0962 - accuracy: 0.7944 - val_loss: 1.2198 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 283/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.0557 - accuracy: 0.8266 - val_loss: 1.2199 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 284/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.0565 - accuracy: 0.8274 - val_loss: 1.2505 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 285/1024\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 1.0747 - accuracy: 0.7952 - val_loss: 1.2132 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 286/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.0440 - accuracy: 0.8298 - val_loss: 1.2006 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 287/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.0385 - accuracy: 0.8169 - val_loss: 1.2184 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 288/1024\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 1.0372 - accuracy: 0.8218 - val_loss: 1.1956 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 289/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.0199 - accuracy: 0.8258 - val_loss: 1.1895 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 290/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.0239 - accuracy: 0.8161 - val_loss: 1.2055 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 291/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.0166 - accuracy: 0.8234 - val_loss: 1.1808 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 292/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.9942 - accuracy: 0.8323 - val_loss: 1.1825 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 293/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.0204 - accuracy: 0.8113 - val_loss: 1.2294 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 294/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.0218 - accuracy: 0.8040 - val_loss: 1.1927 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 295/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.0276 - accuracy: 0.8048 - val_loss: 1.1746 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 296/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.0025 - accuracy: 0.8202 - val_loss: 1.1666 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 297/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.9745 - accuracy: 0.8298 - val_loss: 1.1659 - val_accuracy: 0.7267 - lr: 0.0010\n",
      "Epoch 298/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.9889 - accuracy: 0.8226 - val_loss: 1.1798 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 299/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.9899 - accuracy: 0.8081 - val_loss: 1.1502 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 300/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.9675 - accuracy: 0.8242 - val_loss: 1.1659 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 301/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.9676 - accuracy: 0.8258 - val_loss: 1.1499 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 302/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.9616 - accuracy: 0.8218 - val_loss: 1.1264 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 303/1024\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.9512 - accuracy: 0.8363 - val_loss: 1.1333 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 304/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.9480 - accuracy: 0.8355 - val_loss: 1.1289 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 305/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.9445 - accuracy: 0.8315 - val_loss: 1.1268 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 306/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.9334 - accuracy: 0.8435 - val_loss: 1.1284 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 307/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.9319 - accuracy: 0.8403 - val_loss: 1.1505 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 308/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.9303 - accuracy: 0.8250 - val_loss: 1.1330 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 309/1024\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.9492 - accuracy: 0.8242 - val_loss: 1.2101 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 310/1024\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.9810 - accuracy: 0.7919 - val_loss: 1.2622 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 311/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.1122 - accuracy: 0.7452 - val_loss: 1.3068 - val_accuracy: 0.6881 - lr: 0.0010\n",
      "Epoch 312/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1069 - accuracy: 0.7484 - val_loss: 1.1590 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 313/1024\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.9633 - accuracy: 0.7976 - val_loss: 1.1784 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 314/1024\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.0439 - accuracy: 0.7750 - val_loss: 1.1475 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 315/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.0159 - accuracy: 0.7782 - val_loss: 1.1305 - val_accuracy: 0.7299 - lr: 0.0010\n",
      "Epoch 316/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.9759 - accuracy: 0.8000 - val_loss: 1.1924 - val_accuracy: 0.7042 - lr: 0.0010\n",
      "Epoch 317/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.0312 - accuracy: 0.7347 - val_loss: 1.1253 - val_accuracy: 0.7203 - lr: 0.0010\n",
      "Epoch 318/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.9794 - accuracy: 0.7839 - val_loss: 1.1009 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 319/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.9681 - accuracy: 0.8121 - val_loss: 1.1084 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 320/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.9784 - accuracy: 0.7935 - val_loss: 1.1105 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 321/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.9645 - accuracy: 0.7976 - val_loss: 1.0993 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 322/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.9252 - accuracy: 0.8105 - val_loss: 1.1361 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 323/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.9498 - accuracy: 0.7927 - val_loss: 1.1312 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 324/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.9317 - accuracy: 0.7960 - val_loss: 1.1050 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 325/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.9170 - accuracy: 0.8161 - val_loss: 1.0996 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 326/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.9314 - accuracy: 0.8056 - val_loss: 1.0941 - val_accuracy: 0.7203 - lr: 0.0010\n",
      "Epoch 327/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.9165 - accuracy: 0.8129 - val_loss: 1.1076 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 328/1024\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.9061 - accuracy: 0.8145 - val_loss: 1.1030 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 329/1024\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.9101 - accuracy: 0.8024 - val_loss: 1.0833 - val_accuracy: 0.7299 - lr: 0.0010\n",
      "Epoch 330/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.8975 - accuracy: 0.8121 - val_loss: 1.0879 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 331/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.9010 - accuracy: 0.7960 - val_loss: 1.0590 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 332/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.8701 - accuracy: 0.8282 - val_loss: 1.0738 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 333/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.8864 - accuracy: 0.8113 - val_loss: 1.0679 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 334/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.8770 - accuracy: 0.8169 - val_loss: 1.0402 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 335/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.8632 - accuracy: 0.8234 - val_loss: 1.0472 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 336/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.8617 - accuracy: 0.8185 - val_loss: 1.0433 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 337/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.8572 - accuracy: 0.8177 - val_loss: 1.0480 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 338/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.8530 - accuracy: 0.8218 - val_loss: 1.0302 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 339/1024\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.8367 - accuracy: 0.8298 - val_loss: 1.0336 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 340/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.8370 - accuracy: 0.8355 - val_loss: 1.0404 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 341/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.8337 - accuracy: 0.8298 - val_loss: 1.0417 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 342/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.8305 - accuracy: 0.8306 - val_loss: 1.0222 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 343/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.8213 - accuracy: 0.8395 - val_loss: 1.0216 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 344/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.8189 - accuracy: 0.8379 - val_loss: 1.0244 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 345/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.8215 - accuracy: 0.8323 - val_loss: 1.0166 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 346/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.8054 - accuracy: 0.8323 - val_loss: 1.0149 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 347/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.8036 - accuracy: 0.8452 - val_loss: 1.0238 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 348/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.8149 - accuracy: 0.8315 - val_loss: 1.0198 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 349/1024\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.8079 - accuracy: 0.8274 - val_loss: 1.0064 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 350/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.8012 - accuracy: 0.8452 - val_loss: 1.0153 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 351/1024\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.8053 - accuracy: 0.8315 - val_loss: 1.0157 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 352/1024\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.8034 - accuracy: 0.8290 - val_loss: 1.0129 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 353/1024\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.7900 - accuracy: 0.8460 - val_loss: 1.0040 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 354/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.7881 - accuracy: 0.8355 - val_loss: 0.9926 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 355/1024\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.7884 - accuracy: 0.8395 - val_loss: 1.0125 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 356/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.7921 - accuracy: 0.8427 - val_loss: 1.0045 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 357/1024\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.7885 - accuracy: 0.8452 - val_loss: 1.0060 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 358/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.7867 - accuracy: 0.8379 - val_loss: 0.9938 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 359/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.7781 - accuracy: 0.8419 - val_loss: 1.0283 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 360/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.7970 - accuracy: 0.8274 - val_loss: 0.9831 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 361/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.7986 - accuracy: 0.8331 - val_loss: 0.9950 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 362/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.7739 - accuracy: 0.8411 - val_loss: 1.0015 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 363/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.7661 - accuracy: 0.8419 - val_loss: 0.9882 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 364/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.7735 - accuracy: 0.8395 - val_loss: 0.9774 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 365/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.7627 - accuracy: 0.8476 - val_loss: 0.9980 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 366/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.7669 - accuracy: 0.8387 - val_loss: 0.9825 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 367/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.7638 - accuracy: 0.8363 - val_loss: 0.9825 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 368/1024\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7456 - accuracy: 0.8476 - val_loss: 0.9997 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 369/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.7561 - accuracy: 0.8484 - val_loss: 0.9736 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 370/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.7628 - accuracy: 0.8500 - val_loss: 0.9776 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 371/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.7399 - accuracy: 0.8573 - val_loss: 0.9960 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 372/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.7414 - accuracy: 0.8452 - val_loss: 0.9784 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 373/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.7520 - accuracy: 0.8452 - val_loss: 0.9805 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 374/1024\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.7423 - accuracy: 0.8468 - val_loss: 0.9802 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 375/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.7513 - accuracy: 0.8419 - val_loss: 1.0138 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 376/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.7724 - accuracy: 0.8387 - val_loss: 1.0919 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 377/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.8981 - accuracy: 0.7629 - val_loss: 1.3809 - val_accuracy: 0.6752 - lr: 0.0010\n",
      "Epoch 378/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1584 - accuracy: 0.7145 - val_loss: 0.9551 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 379/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.7550 - accuracy: 0.8331 - val_loss: 1.1347 - val_accuracy: 0.7106 - lr: 0.0010\n",
      "Epoch 380/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.9899 - accuracy: 0.7185 - val_loss: 0.9752 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 381/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.8005 - accuracy: 0.8218 - val_loss: 1.0571 - val_accuracy: 0.6977 - lr: 0.0010\n",
      "Epoch 382/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.8958 - accuracy: 0.7685 - val_loss: 1.0471 - val_accuracy: 0.7106 - lr: 0.0010\n",
      "Epoch 383/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.9027 - accuracy: 0.7347 - val_loss: 0.9779 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 384/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.8407 - accuracy: 0.7984 - val_loss: 0.9728 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 385/1024\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8405 - accuracy: 0.8040 - val_loss: 1.0065 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 386/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.8714 - accuracy: 0.7806 - val_loss: 0.9923 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 387/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.8390 - accuracy: 0.8000 - val_loss: 0.9686 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 388/1024\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.8008 - accuracy: 0.8242 - val_loss: 0.9910 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 389/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.8119 - accuracy: 0.8081 - val_loss: 1.0068 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 390/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.8261 - accuracy: 0.7871 - val_loss: 0.9856 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 391/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.8017 - accuracy: 0.8081 - val_loss: 0.9641 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 392/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.7755 - accuracy: 0.8315 - val_loss: 0.9742 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 393/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.7948 - accuracy: 0.8113 - val_loss: 0.9668 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 394/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.7860 - accuracy: 0.8161 - val_loss: 0.9628 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 395/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.7581 - accuracy: 0.8274 - val_loss: 0.9934 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 396/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.7712 - accuracy: 0.8089 - val_loss: 0.9863 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 397/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.7635 - accuracy: 0.8153 - val_loss: 0.9508 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 398/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.7537 - accuracy: 0.8274 - val_loss: 0.9495 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 399/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.7603 - accuracy: 0.8298 - val_loss: 0.9507 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 400/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.7440 - accuracy: 0.8347 - val_loss: 0.9568 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 401/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.7441 - accuracy: 0.8274 - val_loss: 0.9541 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 402/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.7488 - accuracy: 0.8218 - val_loss: 0.9320 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 403/1024\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7233 - accuracy: 0.8403 - val_loss: 0.9441 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 404/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.7501 - accuracy: 0.8290 - val_loss: 0.9254 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 405/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.7280 - accuracy: 0.8395 - val_loss: 0.9463 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 406/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.7162 - accuracy: 0.8258 - val_loss: 0.9527 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 407/1024\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.7314 - accuracy: 0.8250 - val_loss: 0.9267 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 408/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.7063 - accuracy: 0.8403 - val_loss: 0.9363 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 409/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.7273 - accuracy: 0.8282 - val_loss: 0.9302 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 410/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.7063 - accuracy: 0.8395 - val_loss: 0.9402 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 411/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.7258 - accuracy: 0.8250 - val_loss: 0.9286 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 412/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.6971 - accuracy: 0.8403 - val_loss: 0.9252 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 413/1024\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6979 - accuracy: 0.8435 - val_loss: 0.9263 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 414/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.7012 - accuracy: 0.8476 - val_loss: 0.9427 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 415/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.7095 - accuracy: 0.8274 - val_loss: 0.9086 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 416/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6838 - accuracy: 0.8613 - val_loss: 0.9047 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 417/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6918 - accuracy: 0.8476 - val_loss: 0.9238 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 418/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6811 - accuracy: 0.8516 - val_loss: 0.9232 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 419/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6844 - accuracy: 0.8476 - val_loss: 0.9197 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 420/1024\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.6830 - accuracy: 0.8581 - val_loss: 0.9158 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 421/1024\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6820 - accuracy: 0.8500 - val_loss: 0.9215 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 422/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6720 - accuracy: 0.8548 - val_loss: 0.9244 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 423/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6873 - accuracy: 0.8452 - val_loss: 0.9075 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 424/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6722 - accuracy: 0.8540 - val_loss: 0.9067 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 425/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6784 - accuracy: 0.8468 - val_loss: 0.9128 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 426/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6715 - accuracy: 0.8492 - val_loss: 0.8966 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 427/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6671 - accuracy: 0.8565 - val_loss: 0.9000 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 428/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6622 - accuracy: 0.8621 - val_loss: 0.9080 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 429/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6670 - accuracy: 0.8508 - val_loss: 0.9031 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 430/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6600 - accuracy: 0.8589 - val_loss: 0.9310 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 431/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6639 - accuracy: 0.8476 - val_loss: 0.9091 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 432/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6683 - accuracy: 0.8532 - val_loss: 0.9088 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 433/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6411 - accuracy: 0.8694 - val_loss: 0.8990 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 434/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6658 - accuracy: 0.8484 - val_loss: 0.9156 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 435/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6560 - accuracy: 0.8629 - val_loss: 0.9274 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 436/1024\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6573 - accuracy: 0.8573 - val_loss: 0.9001 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 437/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6519 - accuracy: 0.8597 - val_loss: 0.9274 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 438/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.6611 - accuracy: 0.8581 - val_loss: 0.9087 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 439/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6550 - accuracy: 0.8556 - val_loss: 0.9207 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 440/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6486 - accuracy: 0.8476 - val_loss: 0.9111 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 441/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6400 - accuracy: 0.8702 - val_loss: 0.9145 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 442/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6446 - accuracy: 0.8516 - val_loss: 0.9327 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 443/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6458 - accuracy: 0.8581 - val_loss: 0.9052 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 444/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.6453 - accuracy: 0.8613 - val_loss: 0.9418 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 445/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6557 - accuracy: 0.8484 - val_loss: 0.9436 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 446/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.7156 - accuracy: 0.8266 - val_loss: 1.0148 - val_accuracy: 0.7267 - lr: 0.0010\n",
      "Epoch 447/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.7272 - accuracy: 0.8008 - val_loss: 0.9345 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 448/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6986 - accuracy: 0.8274 - val_loss: 0.8926 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 449/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6341 - accuracy: 0.8629 - val_loss: 0.9536 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 450/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6832 - accuracy: 0.8169 - val_loss: 0.9081 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 451/1024\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6695 - accuracy: 0.8468 - val_loss: 0.9057 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 452/1024\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.6599 - accuracy: 0.8476 - val_loss: 0.9542 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 453/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6929 - accuracy: 0.8169 - val_loss: 0.8983 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 454/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6482 - accuracy: 0.8597 - val_loss: 0.9090 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 455/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6684 - accuracy: 0.8516 - val_loss: 0.8969 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 456/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6445 - accuracy: 0.8589 - val_loss: 0.9319 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 457/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6625 - accuracy: 0.8387 - val_loss: 0.8940 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 458/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6384 - accuracy: 0.8629 - val_loss: 0.8910 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 459/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6566 - accuracy: 0.8548 - val_loss: 0.8942 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 460/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6308 - accuracy: 0.8677 - val_loss: 0.9264 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 461/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6435 - accuracy: 0.8508 - val_loss: 0.8888 - val_accuracy: 0.7846 - lr: 0.0010\n",
      "Epoch 462/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6350 - accuracy: 0.8548 - val_loss: 0.9100 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 463/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6404 - accuracy: 0.8613 - val_loss: 0.9079 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 464/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6173 - accuracy: 0.8726 - val_loss: 0.9039 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 465/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6322 - accuracy: 0.8548 - val_loss: 0.9066 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 466/1024\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6436 - accuracy: 0.8597 - val_loss: 0.9038 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 467/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6434 - accuracy: 0.8573 - val_loss: 0.9340 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 468/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6586 - accuracy: 0.8363 - val_loss: 0.9187 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 469/1024\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6549 - accuracy: 0.8452 - val_loss: 0.9156 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 470/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6318 - accuracy: 0.8661 - val_loss: 0.9224 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 471/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6366 - accuracy: 0.8581 - val_loss: 0.8932 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 472/1024\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.6217 - accuracy: 0.8710 - val_loss: 0.9079 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 473/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6255 - accuracy: 0.8653 - val_loss: 0.8932 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 474/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6204 - accuracy: 0.8589 - val_loss: 0.9048 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 475/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6248 - accuracy: 0.8581 - val_loss: 0.8882 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 476/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6134 - accuracy: 0.8758 - val_loss: 0.8943 - val_accuracy: 0.7846 - lr: 0.0010\n",
      "Epoch 477/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6185 - accuracy: 0.8694 - val_loss: 0.9144 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 478/1024\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5991 - accuracy: 0.8702 - val_loss: 0.8902 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 479/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6264 - accuracy: 0.8645 - val_loss: 0.9230 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 480/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6213 - accuracy: 0.8621 - val_loss: 0.8899 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 481/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6132 - accuracy: 0.8613 - val_loss: 0.8838 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 482/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6087 - accuracy: 0.8694 - val_loss: 0.9482 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 483/1024\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6411 - accuracy: 0.8532 - val_loss: 0.8995 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 484/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6482 - accuracy: 0.8468 - val_loss: 0.9474 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 485/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6399 - accuracy: 0.8452 - val_loss: 0.9061 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 486/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6133 - accuracy: 0.8645 - val_loss: 0.8803 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 487/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6145 - accuracy: 0.8742 - val_loss: 0.9312 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 488/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6149 - accuracy: 0.8540 - val_loss: 0.8931 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 489/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6157 - accuracy: 0.8589 - val_loss: 0.8806 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 490/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6082 - accuracy: 0.8702 - val_loss: 0.8993 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 491/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5997 - accuracy: 0.8694 - val_loss: 0.9059 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 492/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6258 - accuracy: 0.8589 - val_loss: 0.9084 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 493/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6145 - accuracy: 0.8621 - val_loss: 0.8785 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 494/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6008 - accuracy: 0.8758 - val_loss: 0.9038 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 495/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6008 - accuracy: 0.8677 - val_loss: 0.8823 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 496/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5976 - accuracy: 0.8766 - val_loss: 0.8958 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 497/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6046 - accuracy: 0.8621 - val_loss: 0.9221 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 498/1024\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6170 - accuracy: 0.8581 - val_loss: 0.8954 - val_accuracy: 0.7846 - lr: 0.0010\n",
      "Epoch 499/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5961 - accuracy: 0.8613 - val_loss: 0.9302 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 500/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5991 - accuracy: 0.8669 - val_loss: 0.9046 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 501/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6276 - accuracy: 0.8540 - val_loss: 0.9386 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 502/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6242 - accuracy: 0.8556 - val_loss: 0.8717 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 503/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6036 - accuracy: 0.8645 - val_loss: 0.8822 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 504/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5997 - accuracy: 0.8694 - val_loss: 0.9116 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 505/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5922 - accuracy: 0.8750 - val_loss: 0.8697 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 506/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5949 - accuracy: 0.8726 - val_loss: 0.9000 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 507/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5814 - accuracy: 0.8718 - val_loss: 0.9075 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 508/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5737 - accuracy: 0.8823 - val_loss: 0.8793 - val_accuracy: 0.7846 - lr: 0.0010\n",
      "Epoch 509/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6010 - accuracy: 0.8758 - val_loss: 0.9416 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 510/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6010 - accuracy: 0.8694 - val_loss: 0.9335 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 511/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6243 - accuracy: 0.8484 - val_loss: 0.9551 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 512/1024\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.6211 - accuracy: 0.8492 - val_loss: 0.9200 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 513/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6571 - accuracy: 0.8339 - val_loss: 0.9773 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 514/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6211 - accuracy: 0.8532 - val_loss: 0.8805 - val_accuracy: 0.7846 - lr: 0.0010\n",
      "Epoch 515/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5908 - accuracy: 0.8710 - val_loss: 0.8743 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 516/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6002 - accuracy: 0.8669 - val_loss: 0.9320 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 517/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.6170 - accuracy: 0.8597 - val_loss: 0.8715 - val_accuracy: 0.7846 - lr: 0.0010\n",
      "Epoch 518/1024\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5840 - accuracy: 0.8774 - val_loss: 0.8648 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 519/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6015 - accuracy: 0.8613 - val_loss: 0.9171 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 520/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5949 - accuracy: 0.8726 - val_loss: 0.8861 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 521/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5901 - accuracy: 0.8702 - val_loss: 0.8688 - val_accuracy: 0.7878 - lr: 0.0010\n",
      "Epoch 522/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5891 - accuracy: 0.8637 - val_loss: 0.9055 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 523/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5855 - accuracy: 0.8750 - val_loss: 0.9085 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 524/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5782 - accuracy: 0.8774 - val_loss: 0.8801 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 525/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5834 - accuracy: 0.8710 - val_loss: 0.9047 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 526/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5620 - accuracy: 0.8734 - val_loss: 0.9066 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 527/1024\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5746 - accuracy: 0.8798 - val_loss: 0.8932 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 528/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5692 - accuracy: 0.8798 - val_loss: 0.8906 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 529/1024\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5698 - accuracy: 0.8774 - val_loss: 0.9117 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 530/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5671 - accuracy: 0.8871 - val_loss: 0.8934 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 531/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5535 - accuracy: 0.8863 - val_loss: 0.8914 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 532/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5702 - accuracy: 0.8734 - val_loss: 0.9162 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 533/1024\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5627 - accuracy: 0.8855 - val_loss: 0.8954 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 534/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5603 - accuracy: 0.8879 - val_loss: 0.9286 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 535/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5865 - accuracy: 0.8702 - val_loss: 0.9605 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 536/1024\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6470 - accuracy: 0.8403 - val_loss: 1.1980 - val_accuracy: 0.6849 - lr: 0.0010\n",
      "Epoch 537/1024\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.8382 - accuracy: 0.7589 - val_loss: 1.0911 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 538/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.8473 - accuracy: 0.7548 - val_loss: 0.9233 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 539/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5935 - accuracy: 0.8637 - val_loss: 1.0523 - val_accuracy: 0.7203 - lr: 0.0010\n",
      "Epoch 540/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.7465 - accuracy: 0.7782 - val_loss: 0.8718 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 541/1024\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.6267 - accuracy: 0.8621 - val_loss: 0.9353 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 542/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.7135 - accuracy: 0.8121 - val_loss: 0.8981 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 543/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6677 - accuracy: 0.8452 - val_loss: 0.8822 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 544/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6412 - accuracy: 0.8581 - val_loss: 0.9376 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 545/1024\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6919 - accuracy: 0.7976 - val_loss: 0.9195 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 546/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6607 - accuracy: 0.8298 - val_loss: 0.8742 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 547/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6363 - accuracy: 0.8677 - val_loss: 0.8718 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 548/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6568 - accuracy: 0.8524 - val_loss: 0.8740 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 549/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6510 - accuracy: 0.8444 - val_loss: 0.8804 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 550/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6182 - accuracy: 0.8694 - val_loss: 0.9059 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 551/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6263 - accuracy: 0.8435 - val_loss: 0.9035 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 552/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6332 - accuracy: 0.8460 - val_loss: 0.8699 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 553/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5930 - accuracy: 0.8702 - val_loss: 0.8897 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 554/1024\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6240 - accuracy: 0.8556 - val_loss: 0.8708 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 555/1024\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6022 - accuracy: 0.8702 - val_loss: 0.8824 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 556/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5857 - accuracy: 0.8718 - val_loss: 0.9392 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 557/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6053 - accuracy: 0.8524 - val_loss: 0.8861 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 558/1024\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.5928 - accuracy: 0.8613 - val_loss: 0.8771 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 559/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6001 - accuracy: 0.8694 - val_loss: 0.8771 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 560/1024\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5801 - accuracy: 0.8734 - val_loss: 0.9037 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 561/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5936 - accuracy: 0.8597 - val_loss: 0.8905 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 562/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5750 - accuracy: 0.8734 - val_loss: 0.8871 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 563/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5815 - accuracy: 0.8742 - val_loss: 0.8711 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 564/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5635 - accuracy: 0.8798 - val_loss: 0.8867 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 565/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5735 - accuracy: 0.8694 - val_loss: 0.8963 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 566/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5613 - accuracy: 0.8750 - val_loss: 0.8775 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 567/1024\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.5683 - accuracy: 0.8831 - val_loss: 0.8715 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 568/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5589 - accuracy: 0.8839 - val_loss: 0.9106 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 569/1024\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5673 - accuracy: 0.8782 - val_loss: 0.8915 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 570/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5519 - accuracy: 0.8847 - val_loss: 0.8745 - val_accuracy: 0.7910 - lr: 0.0010\n",
      "Epoch 571/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5603 - accuracy: 0.8798 - val_loss: 0.8922 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 572/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5654 - accuracy: 0.8726 - val_loss: 0.9215 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 573/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5592 - accuracy: 0.8750 - val_loss: 0.8773 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 574/1024\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5816 - accuracy: 0.8677 - val_loss: 0.8879 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 575/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5439 - accuracy: 0.8855 - val_loss: 0.9123 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 576/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5499 - accuracy: 0.8935 - val_loss: 0.8739 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 577/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5534 - accuracy: 0.8831 - val_loss: 0.8794 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 578/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5463 - accuracy: 0.8782 - val_loss: 0.9292 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 579/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5637 - accuracy: 0.8750 - val_loss: 0.8736 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 580/1024\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.5547 - accuracy: 0.8815 - val_loss: 0.8847 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 581/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5434 - accuracy: 0.8790 - val_loss: 0.9399 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 582/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5780 - accuracy: 0.8548 - val_loss: 0.8835 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 583/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5723 - accuracy: 0.8766 - val_loss: 0.9109 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 584/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5602 - accuracy: 0.8815 - val_loss: 0.9158 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 585/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5508 - accuracy: 0.8790 - val_loss: 0.8681 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 586/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5542 - accuracy: 0.8831 - val_loss: 0.8938 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 587/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5378 - accuracy: 0.8927 - val_loss: 0.9427 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 588/1024\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5691 - accuracy: 0.8710 - val_loss: 0.8769 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 589/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5531 - accuracy: 0.8798 - val_loss: 0.8853 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 590/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5490 - accuracy: 0.8831 - val_loss: 0.9501 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 591/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5490 - accuracy: 0.8871 - val_loss: 0.8874 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 592/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5438 - accuracy: 0.8968 - val_loss: 0.9077 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 593/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5752 - accuracy: 0.8653 - val_loss: 0.9876 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 594/1024\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.6159 - accuracy: 0.8516 - val_loss: 0.9737 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 595/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6018 - accuracy: 0.8597 - val_loss: 0.8734 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 596/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5472 - accuracy: 0.8863 - val_loss: 0.9152 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 597/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5709 - accuracy: 0.8734 - val_loss: 0.9745 - val_accuracy: 0.7299 - lr: 0.0010\n",
      "Epoch 598/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6050 - accuracy: 0.8573 - val_loss: 0.8891 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 599/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5429 - accuracy: 0.8992 - val_loss: 0.9311 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 600/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6222 - accuracy: 0.8589 - val_loss: 0.9134 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 601/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5645 - accuracy: 0.8734 - val_loss: 0.9708 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 602/1024\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5990 - accuracy: 0.8621 - val_loss: 0.8755 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 603/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5538 - accuracy: 0.8774 - val_loss: 0.8641 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 604/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5779 - accuracy: 0.8750 - val_loss: 0.8957 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 605/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5487 - accuracy: 0.8847 - val_loss: 0.9489 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 606/1024\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5709 - accuracy: 0.8750 - val_loss: 0.8673 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 607/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5464 - accuracy: 0.8815 - val_loss: 0.8997 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 608/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5948 - accuracy: 0.8621 - val_loss: 0.8810 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 609/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5467 - accuracy: 0.8911 - val_loss: 0.9761 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 610/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5784 - accuracy: 0.8556 - val_loss: 0.8795 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 611/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5285 - accuracy: 0.8944 - val_loss: 0.8849 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 612/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5666 - accuracy: 0.8718 - val_loss: 0.9018 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 613/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5285 - accuracy: 0.8944 - val_loss: 0.9424 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 614/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5477 - accuracy: 0.8879 - val_loss: 0.8939 - val_accuracy: 0.7846 - lr: 0.0010\n",
      "Epoch 615/1024\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5315 - accuracy: 0.8944 - val_loss: 0.9075 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 616/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5729 - accuracy: 0.8774 - val_loss: 0.8892 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 617/1024\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5573 - accuracy: 0.8806 - val_loss: 1.0284 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 618/1024\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6130 - accuracy: 0.8379 - val_loss: 0.9665 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 619/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6440 - accuracy: 0.8315 - val_loss: 0.9549 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 620/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6009 - accuracy: 0.8621 - val_loss: 0.9119 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 621/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5495 - accuracy: 0.8806 - val_loss: 0.9262 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 622/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5799 - accuracy: 0.8702 - val_loss: 0.9148 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 623/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5429 - accuracy: 0.8871 - val_loss: 0.8999 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 624/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5652 - accuracy: 0.8702 - val_loss: 0.8756 - val_accuracy: 0.7910 - lr: 0.0010\n",
      "Epoch 625/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5564 - accuracy: 0.8823 - val_loss: 0.9141 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 626/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5555 - accuracy: 0.8887 - val_loss: 0.9098 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 627/1024\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5471 - accuracy: 0.8935 - val_loss: 0.8781 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 628/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5561 - accuracy: 0.8798 - val_loss: 0.8731 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 629/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5269 - accuracy: 0.9032 - val_loss: 0.9143 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 630/1024\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.5404 - accuracy: 0.8831 - val_loss: 0.8995 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 631/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5383 - accuracy: 0.8952 - val_loss: 0.8653 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 632/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5275 - accuracy: 0.8927 - val_loss: 0.9027 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 633/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5310 - accuracy: 0.8919 - val_loss: 0.9016 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 634/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5221 - accuracy: 0.8919 - val_loss: 0.9051 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 635/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5212 - accuracy: 0.9000 - val_loss: 0.8967 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 636/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5299 - accuracy: 0.8903 - val_loss: 0.8842 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 637/1024\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5211 - accuracy: 0.9000 - val_loss: 0.9423 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 638/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5311 - accuracy: 0.9000 - val_loss: 0.8984 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 639/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5384 - accuracy: 0.8871 - val_loss: 0.9420 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 640/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5361 - accuracy: 0.8903 - val_loss: 0.9014 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 641/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5453 - accuracy: 0.8855 - val_loss: 0.9623 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 642/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5403 - accuracy: 0.8895 - val_loss: 0.8920 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 643/1024\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.5148 - accuracy: 0.9008 - val_loss: 0.8821 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 644/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5167 - accuracy: 0.9000 - val_loss: 0.9227 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 645/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5220 - accuracy: 0.9016 - val_loss: 0.9065 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 646/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5247 - accuracy: 0.8944 - val_loss: 0.9038 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 647/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5096 - accuracy: 0.9097 - val_loss: 0.8869 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 648/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5203 - accuracy: 0.9056 - val_loss: 0.9013 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 649/1024\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5143 - accuracy: 0.8968 - val_loss: 0.9321 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 650/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5223 - accuracy: 0.8952 - val_loss: 0.8957 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 651/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5162 - accuracy: 0.9016 - val_loss: 0.9058 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 652/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5103 - accuracy: 0.9073 - val_loss: 0.9391 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 653/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5189 - accuracy: 0.8968 - val_loss: 0.8950 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 654/1024\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5263 - accuracy: 0.8960 - val_loss: 0.9138 - val_accuracy: 0.7846 - lr: 0.0010\n",
      "Epoch 655/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5043 - accuracy: 0.9121 - val_loss: 0.9136 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 656/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5047 - accuracy: 0.9073 - val_loss: 0.9021 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 657/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5101 - accuracy: 0.9056 - val_loss: 0.9083 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 658/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5038 - accuracy: 0.8944 - val_loss: 0.9052 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 659/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5195 - accuracy: 0.9008 - val_loss: 0.9674 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 660/1024\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5325 - accuracy: 0.8855 - val_loss: 0.9152 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 661/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5658 - accuracy: 0.8758 - val_loss: 1.0462 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 662/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6060 - accuracy: 0.8484 - val_loss: 0.9653 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 663/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.6018 - accuracy: 0.8677 - val_loss: 0.9221 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 664/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5116 - accuracy: 0.8992 - val_loss: 0.9915 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 665/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5731 - accuracy: 0.8629 - val_loss: 0.9231 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 666/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5685 - accuracy: 0.8710 - val_loss: 0.8940 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 667/1024\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.5215 - accuracy: 0.9048 - val_loss: 0.9792 - val_accuracy: 0.7363 - lr: 0.0010\n",
      "Epoch 668/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5790 - accuracy: 0.8685 - val_loss: 0.8804 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 669/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5141 - accuracy: 0.9048 - val_loss: 0.8856 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 670/1024\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.5497 - accuracy: 0.8815 - val_loss: 0.8914 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 671/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5197 - accuracy: 0.9032 - val_loss: 0.9213 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 672/1024\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5520 - accuracy: 0.8758 - val_loss: 0.8685 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 673/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5143 - accuracy: 0.9000 - val_loss: 0.8860 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 674/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5295 - accuracy: 0.8984 - val_loss: 0.9049 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 675/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5145 - accuracy: 0.9000 - val_loss: 0.9067 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 676/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5254 - accuracy: 0.8887 - val_loss: 0.8812 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 677/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5043 - accuracy: 0.9048 - val_loss: 0.9174 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 678/1024\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5246 - accuracy: 0.8960 - val_loss: 0.9263 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 679/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5093 - accuracy: 0.8992 - val_loss: 0.9041 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 680/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4986 - accuracy: 0.9137 - val_loss: 0.8994 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 681/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5057 - accuracy: 0.9048 - val_loss: 0.9415 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 682/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5116 - accuracy: 0.8960 - val_loss: 0.9060 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 683/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5020 - accuracy: 0.9105 - val_loss: 0.8922 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 684/1024\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5069 - accuracy: 0.9113 - val_loss: 0.9151 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 685/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4994 - accuracy: 0.9089 - val_loss: 0.9126 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 686/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4954 - accuracy: 0.9065 - val_loss: 0.9125 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 687/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4925 - accuracy: 0.9129 - val_loss: 0.9295 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 688/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5244 - accuracy: 0.8895 - val_loss: 0.9706 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 689/1024\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5451 - accuracy: 0.8919 - val_loss: 1.0037 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 690/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5818 - accuracy: 0.8621 - val_loss: 0.9910 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 691/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5579 - accuracy: 0.8750 - val_loss: 0.9133 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 692/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5342 - accuracy: 0.8855 - val_loss: 0.9837 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 693/1024\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5526 - accuracy: 0.8710 - val_loss: 0.9173 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 694/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5536 - accuracy: 0.8815 - val_loss: 0.9020 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 695/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4990 - accuracy: 0.9153 - val_loss: 0.9557 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 696/1024\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5491 - accuracy: 0.8766 - val_loss: 0.8895 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 697/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5367 - accuracy: 0.8839 - val_loss: 0.9162 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 698/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5278 - accuracy: 0.8968 - val_loss: 0.9500 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 699/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5384 - accuracy: 0.8879 - val_loss: 0.8962 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 700/1024\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5320 - accuracy: 0.8935 - val_loss: 0.9522 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 701/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5360 - accuracy: 0.8927 - val_loss: 0.9311 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 702/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5061 - accuracy: 0.9073 - val_loss: 0.9167 - val_accuracy: 0.7846 - lr: 0.0010\n",
      "Epoch 703/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5517 - accuracy: 0.8806 - val_loss: 0.9215 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 704/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5151 - accuracy: 0.9000 - val_loss: 0.9175 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 705/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5109 - accuracy: 0.9056 - val_loss: 0.8932 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 706/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5025 - accuracy: 0.9008 - val_loss: 0.9363 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 707/1024\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5182 - accuracy: 0.8839 - val_loss: 0.8921 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 708/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4974 - accuracy: 0.9177 - val_loss: 0.8951 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 709/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4863 - accuracy: 0.9161 - val_loss: 0.9122 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 710/1024\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5078 - accuracy: 0.9000 - val_loss: 0.8896 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 711/1024\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4901 - accuracy: 0.9185 - val_loss: 0.9423 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 712/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5054 - accuracy: 0.9169 - val_loss: 0.9074 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 713/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4902 - accuracy: 0.9129 - val_loss: 0.9065 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 714/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.4898 - accuracy: 0.9113 - val_loss: 0.9326 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 715/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4959 - accuracy: 0.9121 - val_loss: 0.9331 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 716/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5007 - accuracy: 0.9089 - val_loss: 0.9143 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 717/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4945 - accuracy: 0.9081 - val_loss: 0.9083 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 718/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.4909 - accuracy: 0.9089 - val_loss: 0.9846 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 719/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5019 - accuracy: 0.9081 - val_loss: 0.9139 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 720/1024\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5002 - accuracy: 0.9048 - val_loss: 0.9924 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 721/1024\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5251 - accuracy: 0.8919 - val_loss: 0.9644 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 722/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5655 - accuracy: 0.8774 - val_loss: 1.0385 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 723/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5505 - accuracy: 0.8742 - val_loss: 0.9264 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 724/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5362 - accuracy: 0.8911 - val_loss: 0.9500 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 725/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5028 - accuracy: 0.9081 - val_loss: 0.9830 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 726/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5006 - accuracy: 0.9121 - val_loss: 0.9203 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 727/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5348 - accuracy: 0.8758 - val_loss: 0.9625 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 728/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5359 - accuracy: 0.8895 - val_loss: 0.9286 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 729/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4883 - accuracy: 0.9210 - val_loss: 0.9379 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 730/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5346 - accuracy: 0.8895 - val_loss: 0.9254 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 731/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5013 - accuracy: 0.9073 - val_loss: 0.8991 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 732/1024\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.4804 - accuracy: 0.9218 - val_loss: 0.9086 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 733/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4961 - accuracy: 0.9097 - val_loss: 0.9227 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 734/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4979 - accuracy: 0.9065 - val_loss: 0.9160 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 735/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4994 - accuracy: 0.9113 - val_loss: 0.9063 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 736/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5077 - accuracy: 0.9081 - val_loss: 0.9353 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 737/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4910 - accuracy: 0.9177 - val_loss: 0.9069 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 738/1024\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4884 - accuracy: 0.9089 - val_loss: 0.9143 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 739/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4751 - accuracy: 0.9161 - val_loss: 0.9024 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 740/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4801 - accuracy: 0.9177 - val_loss: 0.9396 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 741/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4918 - accuracy: 0.9202 - val_loss: 0.9268 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 742/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.4965 - accuracy: 0.9097 - val_loss: 0.9016 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 743/1024\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4891 - accuracy: 0.9113 - val_loss: 0.9768 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 744/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5040 - accuracy: 0.9081 - val_loss: 0.9349 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 745/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5303 - accuracy: 0.8927 - val_loss: 0.9926 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 746/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5274 - accuracy: 0.8927 - val_loss: 0.9543 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 747/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5558 - accuracy: 0.8677 - val_loss: 1.0136 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 748/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5374 - accuracy: 0.8734 - val_loss: 0.9203 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 749/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.4833 - accuracy: 0.9105 - val_loss: 0.9700 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 750/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5359 - accuracy: 0.9032 - val_loss: 0.9542 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 751/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4892 - accuracy: 0.9161 - val_loss: 0.9449 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 752/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5054 - accuracy: 0.8927 - val_loss: 0.9434 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 753/1024\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5093 - accuracy: 0.9056 - val_loss: 0.9091 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 754/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4924 - accuracy: 0.9145 - val_loss: 0.9252 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 755/1024\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5023 - accuracy: 0.9016 - val_loss: 0.9325 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 756/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4881 - accuracy: 0.9137 - val_loss: 0.9028 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 757/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4916 - accuracy: 0.9113 - val_loss: 0.8947 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 758/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4959 - accuracy: 0.9177 - val_loss: 0.9579 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 759/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4877 - accuracy: 0.9097 - val_loss: 0.9400 - val_accuracy: 0.7460 - lr: 0.0010\n",
      "Epoch 760/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4862 - accuracy: 0.9073 - val_loss: 0.9380 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 761/1024\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4860 - accuracy: 0.9210 - val_loss: 0.9044 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 762/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4769 - accuracy: 0.9169 - val_loss: 0.9363 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 763/1024\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.4646 - accuracy: 0.9226 - val_loss: 0.9457 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 764/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4703 - accuracy: 0.9161 - val_loss: 0.9093 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 765/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4795 - accuracy: 0.9202 - val_loss: 0.9548 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 766/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4964 - accuracy: 0.9000 - val_loss: 0.9506 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 767/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5019 - accuracy: 0.9000 - val_loss: 1.0621 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 768/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5458 - accuracy: 0.8702 - val_loss: 1.0254 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 769/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6725 - accuracy: 0.8177 - val_loss: 1.1420 - val_accuracy: 0.7235 - lr: 0.0010\n",
      "Epoch 770/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.6359 - accuracy: 0.8435 - val_loss: 0.9392 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 771/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5066 - accuracy: 0.9016 - val_loss: 0.9361 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 772/1024\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5547 - accuracy: 0.8726 - val_loss: 0.9706 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 773/1024\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5368 - accuracy: 0.8839 - val_loss: 0.9717 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 774/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5324 - accuracy: 0.8839 - val_loss: 0.9141 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 775/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5354 - accuracy: 0.8895 - val_loss: 0.8802 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 776/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5311 - accuracy: 0.8895 - val_loss: 0.9130 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 777/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5164 - accuracy: 0.9073 - val_loss: 0.9513 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 778/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5301 - accuracy: 0.8952 - val_loss: 0.9005 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 779/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5099 - accuracy: 0.9097 - val_loss: 0.8769 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 780/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5250 - accuracy: 0.8984 - val_loss: 0.8925 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 781/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.4991 - accuracy: 0.9089 - val_loss: 0.9461 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 782/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5179 - accuracy: 0.8984 - val_loss: 0.9159 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 783/1024\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4963 - accuracy: 0.9089 - val_loss: 0.8862 - val_accuracy: 0.7846 - lr: 0.0010\n",
      "Epoch 784/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4981 - accuracy: 0.9048 - val_loss: 0.8958 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 785/1024\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4911 - accuracy: 0.9161 - val_loss: 0.9566 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 786/1024\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.4990 - accuracy: 0.9056 - val_loss: 0.9401 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 787/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.4850 - accuracy: 0.9234 - val_loss: 0.8969 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 788/1024\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.4763 - accuracy: 0.9210 - val_loss: 0.9130 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 789/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4799 - accuracy: 0.9234 - val_loss: 0.9519 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 790/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4653 - accuracy: 0.9266 - val_loss: 0.9296 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 791/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4647 - accuracy: 0.9258 - val_loss: 0.9163 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 792/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4768 - accuracy: 0.9210 - val_loss: 0.9254 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 793/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4754 - accuracy: 0.9161 - val_loss: 0.9515 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 794/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4734 - accuracy: 0.9145 - val_loss: 0.9496 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 795/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.4669 - accuracy: 0.9250 - val_loss: 0.9386 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 796/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4944 - accuracy: 0.9048 - val_loss: 0.9802 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 797/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4797 - accuracy: 0.9089 - val_loss: 0.9658 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 798/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4842 - accuracy: 0.9218 - val_loss: 0.9765 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 799/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4820 - accuracy: 0.9121 - val_loss: 0.9465 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 800/1024\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4560 - accuracy: 0.9282 - val_loss: 0.9827 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 801/1024\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4664 - accuracy: 0.9177 - val_loss: 0.9730 - val_accuracy: 0.7878 - lr: 0.0010\n",
      "Epoch 802/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4666 - accuracy: 0.9177 - val_loss: 0.9495 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 803/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4781 - accuracy: 0.9194 - val_loss: 0.9945 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 804/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4807 - accuracy: 0.9169 - val_loss: 0.9743 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 805/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4784 - accuracy: 0.9177 - val_loss: 0.9541 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 806/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4593 - accuracy: 0.9306 - val_loss: 0.9315 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 807/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4565 - accuracy: 0.9242 - val_loss: 0.9448 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 808/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4692 - accuracy: 0.9218 - val_loss: 0.9913 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 809/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4761 - accuracy: 0.9129 - val_loss: 0.9388 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 810/1024\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4992 - accuracy: 0.8952 - val_loss: 0.9944 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 811/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4766 - accuracy: 0.9218 - val_loss: 0.9723 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 812/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4629 - accuracy: 0.9202 - val_loss: 0.9526 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 813/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.4467 - accuracy: 0.9274 - val_loss: 0.9588 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 814/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4638 - accuracy: 0.9290 - val_loss: 0.9269 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 815/1024\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4586 - accuracy: 0.9290 - val_loss: 0.9652 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 816/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4493 - accuracy: 0.9218 - val_loss: 0.9231 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 817/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4575 - accuracy: 0.9274 - val_loss: 0.9467 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 818/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4617 - accuracy: 0.9274 - val_loss: 0.9611 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 819/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4602 - accuracy: 0.9226 - val_loss: 0.9595 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 820/1024\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4440 - accuracy: 0.9363 - val_loss: 0.9598 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 821/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4464 - accuracy: 0.9282 - val_loss: 0.9393 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 822/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.4477 - accuracy: 0.9339 - val_loss: 0.9574 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 823/1024\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4577 - accuracy: 0.9242 - val_loss: 0.9597 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 824/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.4500 - accuracy: 0.9331 - val_loss: 0.9450 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 825/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4592 - accuracy: 0.9210 - val_loss: 1.0301 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 826/1024\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4781 - accuracy: 0.9113 - val_loss: 0.9921 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 827/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5245 - accuracy: 0.8935 - val_loss: 1.1392 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 828/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5827 - accuracy: 0.8516 - val_loss: 1.1268 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 829/1024\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.7324 - accuracy: 0.8137 - val_loss: 1.0159 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 830/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4867 - accuracy: 0.9153 - val_loss: 1.1225 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 831/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6228 - accuracy: 0.8242 - val_loss: 0.9991 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 832/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6510 - accuracy: 0.8331 - val_loss: 0.9693 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 833/1024\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.5996 - accuracy: 0.8637 - val_loss: 0.9733 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 834/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5317 - accuracy: 0.8863 - val_loss: 1.0207 - val_accuracy: 0.7299 - lr: 0.0010\n",
      "Epoch 835/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6235 - accuracy: 0.8306 - val_loss: 0.8661 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 836/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5125 - accuracy: 0.9032 - val_loss: 0.9089 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 837/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5903 - accuracy: 0.8702 - val_loss: 0.9056 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 838/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5603 - accuracy: 0.8871 - val_loss: 0.8871 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 839/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5163 - accuracy: 0.9194 - val_loss: 0.9495 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 840/1024\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.5909 - accuracy: 0.8484 - val_loss: 0.8992 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 841/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5248 - accuracy: 0.9065 - val_loss: 0.8779 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 842/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5168 - accuracy: 0.9169 - val_loss: 0.9042 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 843/1024\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5569 - accuracy: 0.8895 - val_loss: 0.8873 - val_accuracy: 0.7846 - lr: 0.0010\n",
      "Epoch 844/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5129 - accuracy: 0.9040 - val_loss: 0.9119 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 845/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5081 - accuracy: 0.9081 - val_loss: 0.9734 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 846/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5384 - accuracy: 0.8790 - val_loss: 0.9167 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 847/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4868 - accuracy: 0.9194 - val_loss: 0.9100 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 848/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5044 - accuracy: 0.9121 - val_loss: 0.9060 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 849/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4966 - accuracy: 0.9105 - val_loss: 0.9404 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 850/1024\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.4765 - accuracy: 0.9194 - val_loss: 0.9561 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 851/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4875 - accuracy: 0.9097 - val_loss: 0.9111 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 852/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4682 - accuracy: 0.9282 - val_loss: 0.9450 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 853/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4975 - accuracy: 0.9145 - val_loss: 0.9434 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 854/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4884 - accuracy: 0.9137 - val_loss: 0.9685 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 855/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4825 - accuracy: 0.9177 - val_loss: 0.9380 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 856/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4738 - accuracy: 0.9258 - val_loss: 0.9456 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 857/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.4877 - accuracy: 0.9056 - val_loss: 0.9951 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 858/1024\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4910 - accuracy: 0.9177 - val_loss: 0.9365 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 859/1024\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.4851 - accuracy: 0.9145 - val_loss: 0.9497 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 860/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4613 - accuracy: 0.9242 - val_loss: 0.9677 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 861/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4593 - accuracy: 0.9266 - val_loss: 0.9306 - val_accuracy: 0.7846 - lr: 0.0010\n",
      "Epoch 862/1024\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.4642 - accuracy: 0.9177 - val_loss: 0.9411 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 863/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4504 - accuracy: 0.9306 - val_loss: 0.9669 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 864/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4601 - accuracy: 0.9177 - val_loss: 0.9448 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 865/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4699 - accuracy: 0.9161 - val_loss: 0.9869 - val_accuracy: 0.7846 - lr: 0.0010\n",
      "Epoch 866/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4612 - accuracy: 0.9266 - val_loss: 0.9741 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 867/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4711 - accuracy: 0.9137 - val_loss: 0.9534 - val_accuracy: 0.7846 - lr: 0.0010\n",
      "Epoch 868/1024\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4487 - accuracy: 0.9379 - val_loss: 0.9542 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 869/1024\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4572 - accuracy: 0.9282 - val_loss: 0.9653 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 870/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4392 - accuracy: 0.9403 - val_loss: 0.9563 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 871/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4480 - accuracy: 0.9298 - val_loss: 0.9368 - val_accuracy: 0.7846 - lr: 0.0010\n",
      "Epoch 872/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4565 - accuracy: 0.9298 - val_loss: 0.9955 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 873/1024\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4607 - accuracy: 0.9258 - val_loss: 0.9566 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 874/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4494 - accuracy: 0.9306 - val_loss: 0.9639 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 875/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4418 - accuracy: 0.9347 - val_loss: 1.0051 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 876/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4505 - accuracy: 0.9250 - val_loss: 0.9747 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 877/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4539 - accuracy: 0.9266 - val_loss: 0.9640 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 878/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4405 - accuracy: 0.9403 - val_loss: 0.9694 - val_accuracy: 0.7910 - lr: 0.0010\n",
      "Epoch 879/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4455 - accuracy: 0.9331 - val_loss: 0.9457 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 880/1024\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4384 - accuracy: 0.9339 - val_loss: 0.9895 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 881/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4422 - accuracy: 0.9427 - val_loss: 0.9805 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 882/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4464 - accuracy: 0.9339 - val_loss: 0.9624 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 883/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4391 - accuracy: 0.9379 - val_loss: 0.9647 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 884/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4415 - accuracy: 0.9411 - val_loss: 1.0002 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 885/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4468 - accuracy: 0.9282 - val_loss: 0.9538 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 886/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4528 - accuracy: 0.9274 - val_loss: 1.0183 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 887/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4635 - accuracy: 0.9266 - val_loss: 0.9804 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 888/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4712 - accuracy: 0.9121 - val_loss: 1.0577 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 889/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4905 - accuracy: 0.9024 - val_loss: 0.9831 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 890/1024\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4832 - accuracy: 0.9137 - val_loss: 1.0014 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 891/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4595 - accuracy: 0.9234 - val_loss: 0.9780 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 892/1024\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4376 - accuracy: 0.9379 - val_loss: 0.9599 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 893/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4624 - accuracy: 0.9177 - val_loss: 1.0273 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 894/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4695 - accuracy: 0.9210 - val_loss: 0.9553 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 895/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4355 - accuracy: 0.9371 - val_loss: 0.9520 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 896/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4469 - accuracy: 0.9323 - val_loss: 0.9984 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 897/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4592 - accuracy: 0.9250 - val_loss: 0.9499 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 898/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4369 - accuracy: 0.9331 - val_loss: 0.9698 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 899/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4474 - accuracy: 0.9290 - val_loss: 0.9880 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 900/1024\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4507 - accuracy: 0.9347 - val_loss: 0.9492 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 901/1024\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4709 - accuracy: 0.9129 - val_loss: 1.0231 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 902/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4764 - accuracy: 0.9226 - val_loss: 0.9615 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 903/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4438 - accuracy: 0.9379 - val_loss: 0.9514 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 904/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4372 - accuracy: 0.9427 - val_loss: 0.9893 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 905/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4657 - accuracy: 0.9226 - val_loss: 0.9759 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 906/1024\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4541 - accuracy: 0.9306 - val_loss: 0.9894 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 907/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4245 - accuracy: 0.9500 - val_loss: 0.9940 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 908/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4547 - accuracy: 0.9202 - val_loss: 0.9682 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 909/1024\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.4274 - accuracy: 0.9395 - val_loss: 1.0045 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 910/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4463 - accuracy: 0.9347 - val_loss: 0.9654 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 911/1024\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4194 - accuracy: 0.9500 - val_loss: 0.9492 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 912/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4370 - accuracy: 0.9363 - val_loss: 1.0286 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 913/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4636 - accuracy: 0.9258 - val_loss: 0.9988 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 914/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4609 - accuracy: 0.9226 - val_loss: 1.0300 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 915/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4389 - accuracy: 0.9290 - val_loss: 0.9713 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 916/1024\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4389 - accuracy: 0.9339 - val_loss: 0.9806 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 917/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4336 - accuracy: 0.9395 - val_loss: 1.0027 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 918/1024\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4361 - accuracy: 0.9403 - val_loss: 0.9640 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 919/1024\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4489 - accuracy: 0.9266 - val_loss: 1.0103 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 920/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4394 - accuracy: 0.9355 - val_loss: 0.9736 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 921/1024\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4409 - accuracy: 0.9315 - val_loss: 1.0157 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 922/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4425 - accuracy: 0.9363 - val_loss: 0.9583 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 923/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4354 - accuracy: 0.9347 - val_loss: 1.0121 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 924/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4344 - accuracy: 0.9355 - val_loss: 0.9647 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 925/1024\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4380 - accuracy: 0.9290 - val_loss: 0.9796 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 926/1024\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4321 - accuracy: 0.9419 - val_loss: 0.9524 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 927/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4316 - accuracy: 0.9395 - val_loss: 0.9776 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 928/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4141 - accuracy: 0.9492 - val_loss: 0.9924 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 929/1024\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4350 - accuracy: 0.9371 - val_loss: 0.9638 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 930/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4492 - accuracy: 0.9226 - val_loss: 1.0753 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 931/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4954 - accuracy: 0.9065 - val_loss: 1.0538 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 932/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5871 - accuracy: 0.8613 - val_loss: 1.1664 - val_accuracy: 0.7267 - lr: 0.0010\n",
      "Epoch 933/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5652 - accuracy: 0.8685 - val_loss: 0.9730 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 934/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4846 - accuracy: 0.9073 - val_loss: 0.9507 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 935/1024\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4551 - accuracy: 0.9218 - val_loss: 1.0489 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 936/1024\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4864 - accuracy: 0.9250 - val_loss: 0.9648 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 937/1024\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4516 - accuracy: 0.9274 - val_loss: 0.9156 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 938/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4737 - accuracy: 0.9153 - val_loss: 0.9257 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 939/1024\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4586 - accuracy: 0.9298 - val_loss: 0.9970 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 940/1024\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4702 - accuracy: 0.9250 - val_loss: 0.9291 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 941/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4598 - accuracy: 0.9242 - val_loss: 0.9094 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 942/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4610 - accuracy: 0.9169 - val_loss: 0.9367 - val_accuracy: 0.7846 - lr: 0.0010\n",
      "Epoch 943/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4536 - accuracy: 0.9315 - val_loss: 0.9878 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 944/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4542 - accuracy: 0.9379 - val_loss: 0.9516 - val_accuracy: 0.7878 - lr: 0.0010\n",
      "Epoch 945/1024\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4497 - accuracy: 0.9339 - val_loss: 0.9578 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 946/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4416 - accuracy: 0.9403 - val_loss: 0.9753 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 947/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4401 - accuracy: 0.9339 - val_loss: 0.9852 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 948/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4536 - accuracy: 0.9371 - val_loss: 0.9806 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 949/1024\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4388 - accuracy: 0.9387 - val_loss: 0.9881 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 950/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4446 - accuracy: 0.9323 - val_loss: 0.9540 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 951/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4314 - accuracy: 0.9444 - val_loss: 0.9911 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 952/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4311 - accuracy: 0.9323 - val_loss: 1.0215 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 953/1024\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4380 - accuracy: 0.9315 - val_loss: 0.9623 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 954/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4406 - accuracy: 0.9266 - val_loss: 0.9849 - val_accuracy: 0.7781 - lr: 0.0010\n",
      "Epoch 955/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4355 - accuracy: 0.9331 - val_loss: 1.0075 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 956/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4339 - accuracy: 0.9379 - val_loss: 1.0037 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 957/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4257 - accuracy: 0.9484 - val_loss: 0.9788 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 958/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4395 - accuracy: 0.9323 - val_loss: 0.9936 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 959/1024\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4289 - accuracy: 0.9387 - val_loss: 1.0122 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 960/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4281 - accuracy: 0.9468 - val_loss: 1.0056 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 961/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4262 - accuracy: 0.9419 - val_loss: 0.9953 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 962/1024\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4410 - accuracy: 0.9347 - val_loss: 0.9987 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 963/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4305 - accuracy: 0.9347 - val_loss: 1.0184 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 964/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4315 - accuracy: 0.9444 - val_loss: 1.0007 - val_accuracy: 0.7846 - lr: 0.0010\n",
      "Epoch 965/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4225 - accuracy: 0.9435 - val_loss: 0.9930 - val_accuracy: 0.7717 - lr: 0.0010\n",
      "Epoch 966/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4383 - accuracy: 0.9339 - val_loss: 1.0257 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 967/1024\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4329 - accuracy: 0.9444 - val_loss: 1.0189 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 968/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4422 - accuracy: 0.9315 - val_loss: 1.0737 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 969/1024\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4491 - accuracy: 0.9250 - val_loss: 1.0169 - val_accuracy: 0.7653 - lr: 0.0010\n",
      "Epoch 970/1024\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4939 - accuracy: 0.9024 - val_loss: 1.0941 - val_accuracy: 0.7621 - lr: 0.0010\n",
      "Epoch 971/1024\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4876 - accuracy: 0.9056 - val_loss: 1.0135 - val_accuracy: 0.7749 - lr: 0.0010\n",
      "Epoch 972/1024\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.4959 - accuracy: 0.9008 - val_loss: 1.0475 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 973/1024\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4446 - accuracy: 0.9306 - val_loss: 0.9672 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 974/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4280 - accuracy: 0.9363 - val_loss: 0.9598 - val_accuracy: 0.7814 - lr: 0.0010\n",
      "Epoch 975/1024\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4599 - accuracy: 0.9258 - val_loss: 1.0667 - val_accuracy: 0.7588 - lr: 0.0010\n",
      "Epoch 976/1024\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4747 - accuracy: 0.9129 - val_loss: 0.9842 - val_accuracy: 0.7524 - lr: 0.0010\n",
      "Epoch 977/1024\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4232 - accuracy: 0.9476 - val_loss: 0.9644 - val_accuracy: 0.7942 - lr: 0.0010\n",
      "Epoch 978/1024\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4565 - accuracy: 0.9210 - val_loss: 1.0578 - val_accuracy: 0.7556 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 세팅\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 1024\n",
    "MB_SIZE = 4000\n",
    "REPORT = 1\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(\n",
    "  X_train_scaled, y_train,\n",
    "  batch_size=MB_SIZE,\n",
    "  validation_split = 0.2,\n",
    "  verbose=1,\n",
    "  epochs=EPOCHS,\n",
    "  callbacks=[es, rlrp]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 8ms/step - loss: 1.1196 - accuracy: 0.7603\n",
      "Test Loss: 1.12\n",
      "Test Accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Figure 설정\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, index 1\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 훈련 및 검증 평가 지표 추출\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# 평가 지표 그래프 그리기\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, index 2\n",
    "plt.plot(train_accuracy, label='Train Accuracy')\n",
    "plt.plot(val_accuracy, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 그래프 출력\n",
    "plt.tight_layout()  # 그래프 간 간격 조정\n",
    "plt.show()\n",
    "\n",
    "# 테스트 세트 평가\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Test Loss:\", round(loss, 3))\n",
    "print(\"Test Accuracy:\", round(accuracy, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 가중치 및 편향 저장 (save_weights 메서드를 사용하면 가중치와 편향 모두를 저장)\n",
    "model.save_weights('classify_model_weights.h5')\n",
    "\n",
    "# 다음과 같이 저장한 가중치 호출 가능\n",
    "new_model = Classifier_Model_SC(units, L2)\n",
    "new_model.load_weights('classify_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best 모델 가중치 및 편향 저장\n",
    "model.save_weights('best_classify_model.h5') # Test Accuracy: 0.771"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 8ms/step - loss: 1.1196 - accuracy: 0.7603\n",
      "Test Loss: 1.12\n",
      "Test Accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "# 테스트 세트 평가\n",
    "loss, accuracy = new_model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Test Loss:\", round(loss, 3))\n",
    "print(\"Test Accuracy:\", round(accuracy, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p310_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
