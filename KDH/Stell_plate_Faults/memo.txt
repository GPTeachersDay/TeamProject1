# 불균형 타겟 데이터 처리 방법

불균형한 타겟 데이터를 처리하는 데에는 여러 가지 방법이 있습니다:

1. 오버샘플링 (Oversampling):

소수 클래스의 데이터를 늘리는 방법입니다.
가장 간단한 방법은 소수 클래스 데이터를 복제하는 것이지만, 이런 방식은 모델이 과적합(Overfitting) 될 수 있습니다.
이를 해결하기 위해 Synthetic Minority Over-sampling Technique(SMOTE)와 같은 방법을 사용하여 소수 클래스의 새로운 샘플을 합성 생성하는 방법도 있습니다.

2. 언더샘플링 (Undersampling):

다수 클래스의 데이터를 줄이는 방법입니다.
가장 간단한 방법은 다수 클래스의 일부 샘플을 무작위로 제거하는 것입니다.
이 방법은 정보 손실이 발생할 수 있습니다.

3. 클래스 가중치 조정:

다수 클래스의 샘플에 낮은 가중치를, 소수 클래스의 샘플에 높은 가중치를 부여하여, 모델이 소수 클래스에 더 집중하게 만드는 방법입니다.

이 중에서 SMOTE (Synthetic Minority Over-sampling Technique)를 추천합니다.
이 방법은 소수 클래스의 샘플을 합성하여 생성하기 때문에, 과적합을 방지하면서도 클래스 불균형을 해결할 수 있습니다.



# L1규제와 L2규제 (과적합 방지)

1. L1 규제 (L1 Regularization 또는 Lasso Regularization):

L1 규제는 가중치의 절댓값에 비례하는 페널티를 부과합니다.
L1 규제를 적용하면 일부 가중치가 정확히 0이 되는 희소(sparse)한 모델을 유도합니다.
따라서 L1 규제는 특성 선택(feature selection)의 효과를 가지며, 모델에서 중요한 특성들을 식별할 수 있습니다.
이러한 특성 때문에 모델이 해석 가능성(interpretability)이 중요한 경우에 유용하게 사용될 수 있습니다.

2. L2 규제 (L2 Regularization 또는 Ridge Regularization):

L2 규제는 가중치의 제곱에 비례하는 페널티를 부과합니다.
L2 규제를 적용하면 가중치의 크기를 작게 만들어 가중치가 균등하게 분산되도록 합니다.
L2 규제는 특성들이 서로 상호작용할 수 있는 유연성을 제공하며, 일반화 성능을 향상시키는 데 효과적입니다.
L2 규제는 가중치의 크기를 줄이지만, 0으로 정확하게 만들지는 않으며, 희소성(sparcity)을 갖지 않습니다.
따라서 L1 규제는 희소성과 특성 선택에 중점을 두고, L2 규제는 가중치의 균등성과 일반화 성능 향상에 중점을 둡니다.
어떤 규제를 선택할지는 데이터와 모델의 특성, 목표에 따라 다르며, 경우에 따라 두 규제를 함께 사용하는 ElasticNet 등의 조합도 사용될 수 있습니다.