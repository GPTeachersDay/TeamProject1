# 불균형 타겟 데이터 처리 방법

불균형한 타겟 데이터를 처리하는 데에는 여러 가지 방법이 있습니다:

1. 오버샘플링 (Oversampling):

소수 클래스의 데이터를 늘리는 방법입니다.
가장 간단한 방법은 소수 클래스 데이터를 복제하는 것이지만, 이런 방식은 모델이 과적합(Overfitting) 될 수 있습니다.
이를 해결하기 위해 Synthetic Minority Over-sampling Technique(SMOTE)와 같은 방법을 사용하여 소수 클래스의 새로운 샘플을 합성 생성하는 방법도 있습니다.

2. 언더샘플링 (Undersampling):

다수 클래스의 데이터를 줄이는 방법입니다.
가장 간단한 방법은 다수 클래스의 일부 샘플을 무작위로 제거하는 것입니다.
이 방법은 정보 손실이 발생할 수 있습니다.

3. 클래스 가중치 조정:

다수 클래스의 샘플에 낮은 가중치를, 소수 클래스의 샘플에 높은 가중치를 부여하여, 모델이 소수 클래스에 더 집중하게 만드는 방법입니다.

이 중에서 SMOTE (Synthetic Minority Over-sampling Technique)를 추천합니다.
이 방법은 소수 클래스의 샘플을 합성하여 생성하기 때문에, 과적합을 방지하면서도 클래스 불균형을 해결할 수 있습니다.



# L1규제와 L2규제 (과적합 방지)

1. L1 규제 (L1 Regularization 또는 Lasso Regularization):

L1 규제는 가중치의 절댓값에 비례하는 페널티를 부과합니다.
L1 규제를 적용하면 일부 가중치가 정확히 0이 되는 희소(sparse)한 모델을 유도합니다.
따라서 L1 규제는 특성 선택(feature selection)의 효과를 가지며, 모델에서 중요한 특성들을 식별할 수 있습니다.
이러한 특성 때문에 모델이 해석 가능성(interpretability)이 중요한 경우에 유용하게 사용될 수 있습니다.

2. L2 규제 (L2 Regularization 또는 Ridge Regularization):

L2 규제는 가중치의 제곱에 비례하는 페널티를 부과합니다.
L2 규제를 적용하면 가중치의 크기를 작게 만들어 가중치가 균등하게 분산되도록 합니다.
L2 규제는 특성들이 서로 상호작용할 수 있는 유연성을 제공하며, 일반화 성능을 향상시키는 데 효과적입니다.
L2 규제는 가중치의 크기를 줄이지만, 0으로 정확하게 만들지는 않으며, 희소성(sparcity)을 갖지 않습니다.
따라서 L1 규제는 희소성과 특성 선택에 중점을 두고, L2 규제는 가중치의 균등성과 일반화 성능 향상에 중점을 둡니다.
어떤 규제를 선택할지는 데이터와 모델의 특성, 목표에 따라 다르며, 경우에 따라 두 규제를 함께 사용하는 ElasticNet 등의 조합도 사용될 수 있습니다.



# 데이터 손실 문제

신경망에서는 각 계층을 통과할 때마다 일부 정보가 손실되는 현상이 일어날 수 있습니다.
이는 특히나 깊은 신경망에서 문제가 될 수 있습니다.
이 문제를 해결하는 방법 중 하나는 "잔차 연결" 혹은 "스킵 연결"이라고 하는 기법을 사용하는 것입니다.
이 기법은 이전 계층의 출력을 몇 계층 뒤에 있는 계층의 입력과 결합하는 방식으로 동작합니다.
이로 인해 신경망의 앞부분에서 학습된 정보가 뒷부분으로 직접 전달될 수 있게 됩니다.
TensorFlow에서는 tf.keras.layers.Add()를 사용하여 잔차 연결을 구현할 수 있습니다.
이를 위해서는 tf.keras.models.Model을 사용하여 직접 모델을 구성해야 합니다.
함수형 API를 사용하여 입력과 출력 사이의 계층 흐름을 정의합니다.
여기서는 입력 계층과 첫 번째 Dense 계층 사이, 그리고 각 Dropout 계층과 다음 Dense 계층 사이에 잔차 연결을 추가합니다.