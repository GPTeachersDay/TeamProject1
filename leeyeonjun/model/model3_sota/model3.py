import numpy as np
import pandas as pd
import os

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler

import tensorflow as tf
from keras import regularizers
from keras.layers import Input, Dense, Dropout, Add
from keras.models import Model

from imblearn.over_sampling import SMOTE

from tensorflow import keras
import tensorflow as tf

class Model_3(keras.Model):
    def __init__(self
                , csv_path='mulit_classification_data.csv'
                , model_path='model3.h5'
                , TRAIN_RATIO=0.8):
        super().__init__()
        
        # print(f'âœ… ì¸ìŠ¤í„´ìŠ¤ ìƒì„±1')
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        self.df = pd.read_csv(csv_path)
        
        # ì¸ì½”ë”© ë°©ì‹ ë³€ê²½
        # idxmax í•¨ìˆ˜ëŠ” ê° í–‰ì˜ ìµœëŒ€ê°’ì„ ê°€ì§„ ì—´ì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•œë‹¤. ë”°ë¼ì„œ ì›í•«ì¸ì½”ë”©ëœ í”¼ì³ë¥¼ í•˜ë‚˜ì˜ ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ë¡œ ë³µì›í•  ìˆ˜ ìˆìŒ
        self.df['Fault'] = self.df[['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']].idxmax(axis=1)
        
        # ë¼ë²¨ ì¸ì½”ë”©
        self.encoder = LabelEncoder()
        self.df['Fault'] = self.encoder.fit_transform(self.df['Fault'])
        
        # ì´ìƒì¹˜ ì œê±°
        self.df = self.df[~((self.df['Pixels_Areas'] > 35000) |
                (self.df['X_Perimeter'] > 2000) |
                (self.df['Y_Perimeter'] > 2500) |
                (self.df['Sum_of_Luminosity'] > 0.5e7))]

        # 'TypeOfSteel_A300'ê³¼ 'TypeOfSteel_400'ë¡œ ë‚˜ëˆ„ì–´ì§„ íŠ¹ì„±ì„ í•˜ë‚˜ë¡œ í†µí•©
        self.df['TypeOfSteel'] = self.df['TypeOfSteel_A300']
        self.df.drop(['TypeOfSteel_A300', 'TypeOfSteel_A400'], axis=1, inplace=True)

        # ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ìƒê´€ê´€ê³„ê°€ ë†’ì€ feature í™•ì¸ í›„ ì œê±° (ìƒê´€ê³„ìˆ˜ ì ˆëŒ€ê°’ 0.9ì´ìƒ)
        # 'Pixels_Areas'ëŠ” íŠ¹ì„±ì¤‘ìš”ë„ê°€ ë†’ìœ¼ë¯€ë¡œ í™œìš©í•˜ê³  ìƒê´€ê´€ê³„ê°€ ë†’ì€ ë‚˜ë¨¸ì§€ íŠ¹ì„±ë“¤ ì œê±°
        self.df.drop(['Sum_of_Luminosity', 'X_Perimeter', 'Y_Perimeter'], axis=1, inplace=True)

        # ë¡œê·¸ ìŠ¤ì¼€ì¼ë§(ê´€ì¸¡ì¹˜ ì¸¡ì • ë²”ìœ„ê°€ ë„“ìŒ)
        self.df['Log_Pixels_Areas'] = np.log(self.df['Pixels_Areas'])
        self.df.drop(['Pixels_Areas'], axis=1, inplace=True)

        # ì¤‘ìš”ë„ ë‚®ì€ íŠ¹ì„± ì œê±°(ë¨¸ì‹ ëŸ¬ë‹ ë¶„ë¥˜ëª¨ë¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê¸°ë²•ì¸ë° ë”¥ëŸ¬ë‹ì—ë„ ì ìš©ì´ ê°€ëŠ¥í•œê°€? -> ìš°ì„  ëª¨ë¸ ì„±ëŠ¥ ê°œì„ ì€ ë˜ì—ˆìŒ, ë” ì•Œì•„ë³´ë©´ ì¢‹ì„ ê²ƒ ê°™ìŒ)
        self.df.drop(['Outside_Global_Index', 'SigmoidOfAreas', 'Log_Y_Index'], axis=1, inplace=True)

        # í•™ìŠµ ë°ì´í„° ë¶„ë¦¬
        self.X = self.df.drop(['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults', 'Fault'], axis=1)
        self.y = self.df['Fault']
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, train_size=TRAIN_RATIO, random_state = 83)
        
        # ì˜¤ë²„ìƒ˜í”Œë§(í•™ìŠµë°ì´í„°ë§Œ)
        # SMOTE ì ìš©
        self.sm = SMOTE(random_state=2)
        self.X_train_res, self.y_train_res = self.sm.fit_resample(self.X_train, self.y_train.ravel())
        
        # í‘œì¤€í™”(MinMaxScaling)
        # ìŠ¤ì¼€ì¼ë§í•  í”¼ì²˜ ì„ íƒ
        self.scaling_features = self.X_train.columns

        # ìŠ¤ì¼€ì¼ë§
        self.scaler = MinMaxScaler()
        self.X_train_scaled = self.X_train.copy()  # ì›ë³¸ ë°ì´í„° ë³µì‚¬
        self.X_test_scaled = self.X_test.copy()    # ì›ë³¸ ë°ì´í„° ë³µì‚¬
        self.X_train_scaled[self.scaling_features] = self.scaler.fit_transform(self.X_train[self.scaling_features])
        self.X_test_scaled[self.scaling_features] = self.scaler.transform(self.X_test[self.scaling_features])
        
        # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
        self.model = self.get_model()
        self.model.load_weights(model_path)
    
    def get_model(self, dout=0.1, output_bias=None):
        
        if output_bias is not None:
            output_bias = keras.initializers.Constant(output_bias)
        
        # ìŠ¤í‚µ ì—°ê²° ëª¨ë¸(Skip Connection)
        np.random.seed(42)
        tf.random.set_seed(42)
        
        # ë…¸ë“œ ê°œìˆ˜, L2 ì„¤ì •
        units = [256 for _ in range(8)]
        l2 = [0.001 + i * 0.0025 for i in range(8)]
        
        # ì…ë ¥ì¸µì„ ì •ì˜í•˜ë©°, ì…ë ¥ì˜ í˜•íƒœëŠ” í›ˆë ¨ ë°ì´í„°ì˜ íŠ¹ì„± ìˆ˜ì— ë”°ë¼ ê²°ì •
        inputs = Input(shape=(len(self.X_train.keys()),))
        
        # ì²« ë²ˆì§¸ Dense ì¸µì„ ë§Œë“¤ê³  ReLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©, L2 ì •ê·œí™” ì ìš©
        x = Dense(units=units[0], activation='relu', kernel_regularizer=regularizers.l2(l2[0]))(inputs)
        x = Dropout(0.2)(x)

        # ì´í›„ 7ê°œì˜ Dense ì¸µì„ ìŠ¤í‚µ ì—°ê²°ì„ ê°€ì§€ë„ë¡ ìƒì„±
        for i in range(1, 8):   
            dense = Dense(units=units[i], activation='relu', kernel_regularizer=regularizers.l2(l2[i]))
            y = dense(x)
            y = Dropout(0.2)(y)
            
            # í˜„ì¬ ì¸µì˜ ì¶œë ¥(y)ê³¼ ì´ì „ ì¸µì˜ ì¶œë ¥(x)ì„ ë”í•˜ì—¬ ìŠ¤í‚µ ì—°ê²° êµ¬í˜„
            x = Add()([x, y])

        # ì¶œë ¥ì¸µì„ ì •ì˜, ìœ ë‹›ì˜ ìˆ˜ëŠ” í´ë˜ìŠ¤ì˜ ìˆ˜ì™€ ë™ì¼í•˜ë©°, softmax í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©
        outputs = Dense(units=7, activation='softmax')(x)

        # ëª¨ë¸ì„ ìƒì„±, ì…ë ¥ê³¼ ì¶œë ¥ì„ ì§€ì •
        model = Model(inputs=inputs, outputs=outputs)
        
        # ì˜µí‹°ë§ˆì´ì €ì™€ ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
        optimizer = tf.keras.optimizers.Adam(
                                            learning_rate=0.001,
                                            beta_1=0.9,
                                            beta_2=0.999,
                                            epsilon=1e-08
                                            )

        model.compile(loss='sparse_categorical_crossentropy',  # ì†ì‹¤í•¨ìˆ˜ë¥¼ ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì— ì í•©í•œ í˜•íƒœë¡œ ë³€ê²½
                    optimizer=optimizer,
                    metrics=['accuracy'])
        
        return model
    
    def input_predict(self, input_list):

        # íƒ€ê²Ÿ ì˜ˆì¸¡
        result_list = self.model.predict([input_list], verbose=0)
        # print(f"âœ…result_list : {result_list}")
        
        result_tup = [(i,v) for i,v in enumerate(result_list[0])]
        result_tup.sort(key=lambda x:-x[1])
        
        y_label_1 = result_tup[0][0]
        y_label_2 = result_tup[1][0]
        y_RESULT_1 = self.encoder.inverse_transform([y_label_1])[0]
        y_RESULT_2 = self.encoder.inverse_transform([y_label_2])[0]
        # print(f"y_RESULT : {y_RESULT_1}")
        # print(f"y_RESULT : {y_RESULT_2}")
        
        y_rate_1 = round(result_list[0][y_label_1]*100, 2)
        y_rate_2 = round(result_list[0][y_label_2]*100, 2)
        
        template = f"""
        <div class="container text-center border border-3 rounded">
            <h5 class="p-2 mb-1">
                ğŸ” ê°•íŒì—ëŠ”<br>
                {y_rate_1}% í™•ë¥ ë¡œ '<b>{y_RESULT_1}</b>' ê²°í•©<br>
                {y_rate_2}% í™•ë¥ ë¡œ '<b>{y_RESULT_2}</b>' ê²°í•©<br>
                ì´ ìˆìŠµë‹ˆë‹¤. ğŸ˜ƒ
            </h5>
        </div>
        """

        return template
        
